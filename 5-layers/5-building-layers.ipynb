{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last lesson, we learned about initializing a layer to our neural network.  We did so by building a weight matrix where we specified the dimensions of the attributes of the layer - a column for each neuron, and a row for each feature that the neuron accepts.  For example, we constructed a linear layer with two neurons, each of which accepts five features as the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.34360319,  1.16152149],\n",
       "       [ 0.38607805, -1.13313327],\n",
       "       [ 0.43309255, -0.30408644],\n",
       "       [ 2.58529487,  1.83533272],\n",
       "       [ 0.44068987, -0.71925384]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "W = np.random.randn(5, 2)\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.58341459, -0.32504963])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = np.random.randn(2)\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson we'll move beyond constructing single layers and learn how to build a network with multiple layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by taking another look at our single layer matrix:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://storage.googleapis.com/curriculum-assets/nn-from-scratch/first-layer.png\" width=\"20%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's imagine that each observation in our training data has 10 features.  For example let's say, our feature vector $x$, representing our first observation looks like the following: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([.9, .4, .5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build a layer that takes in a feature vector of this size, we need a matrix where each row has 3 rows.  And let's have a layer with five neurons, giving us five columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(2)\n",
    "W_1 = np.random.randn(3, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our bias vector has five entries, one for each neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.5961597 , -0.0191305 ,  1.17500122, -0.74787095,  0.00902525])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_1 = np.random.randn(5)\n",
    "b_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's pass our first observation through the linear layer, and then the sigmoid function for the activation layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.26265516, 0.78202152, 0.22707164, 0.43689559, 0.15457753])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sigmoid(value): return 1/(1 + np.exp(-value))\n",
    "\n",
    "l1 = x.dot(W_1) + b_1\n",
    "a1 = sigmoid(l1)\n",
    "a1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that we get five different outputs ranging from 0 to 1, one for each of our neurons. \n",
    "\n",
    "Now if we want to add a second layer to our neural network, we take the five outputs from the neurons in our first layer and feed these ouputs to each neurons in our second layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This is called a **fully connected** neural network.  In a fully connected neural network, every neuron in one layer connects to every neuron in another layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the number of features accepted by a neuron in one layer must equal the number of inputs it receives from the previous layer.  \n",
    "\n",
    "In contrast to the features, the number of *neurons* that composes each layer is not constrained by previous layers.  \n",
    "\n",
    "* Or to put this in terms of our weight matrix, the number of columns in a weight matrix is equal to the number of the neurons in the previous layer, as each neuron outputs a single number. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Walking through an example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see this with our example above.  We start with an observation's feature vector that has three entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9, 0.4, 0.5])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means our weight matrix must have three columns (one for each corresponding feature of the neuron).  And we previously specified that there be three columns, to represent the weights of three neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.41675785, -0.05626683, -2.1361961 ,  1.64027081, -1.79343559],\n",
       "       [-0.84174737,  0.50288142, -1.24528809, -1.05795222, -0.90900761],\n",
       "       [ 0.55145404,  2.29220801,  0.04153939, -1.11792545,  0.53905832]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the outputs of the neurons' linear components are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.03221369,  1.27748593, -1.2249208 , -0.25377083, -1.69914066])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.dot(W_1) + b_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.26265516, 0.78202152, 0.22707164, 0.43689559, 0.15457753])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(x.dot(W_1) + b_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we get an output from each of the five neurons, our next weight matrix, $l2$ must have five rows, one row for the weights of each neuron.\n",
    "\n",
    "> We'll initialize it to have two neurons.  Each column in the weight matrix represents a neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.87810789, -0.15643417],\n",
       "       [ 0.25657045, -0.98877905],\n",
       "       [-0.33882197, -0.23618403],\n",
       "       [-0.63765501, -1.18761229],\n",
       "       [-1.42121723, -0.1534952 ]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_2 = np.random.randn(5, 2)\n",
    "W_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> And there is a bias term for each neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.26905696,  2.23136679])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_2 = np.random.randn(2)\n",
    "b_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapping Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can initialize our weights and biases so that our layers are fully connected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 5), (5, 2))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([0.9, 0.4, 0.5])\n",
    "W_1 = np.random.randn(3, 5)\n",
    "b_1 = np.random.randn(5)\n",
    "# outputs a prediction from each of five neurons\n",
    "\n",
    "W_2 = np.random.randn(5, 2)\n",
    "b_2 = np.random.randn(2)\n",
    "# outputs a prediction from each of two neurons\n",
    "\n",
    "W_1.shape, W_2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we feed forward our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.39744071, 0.42585218])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1 = sigmoid(x.dot(W_1) + b_1)\n",
    "l2 = sigmoid(l1.dot(W_2) + b_2)\n",
    "l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> So we can see that we make a different prediction for each layer in the last network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we call the network *feedforward* because information ﬂows through the function being evaluated from $x$, through the intermediate computations, and ﬁnally to the output of a prediction from each neuron in our last network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize\n",
    "\n",
    "So that is how our neural network makes a prediction.  \n",
    "\n",
    "1. Initialize random weights and biases for each layer where the number of weights of each neuron are the rows, and there is a separate vector for each neuron.\n",
    "\n",
    "2. Feed forward each layer with the formula $\\sigma(x \\cdot W + b)$, where $x$ is the vector of inputs for the first layer, and afterwards $x$ is the vector of the previous layer's outputs\n",
    "\n",
    "3. For each weight matrix W, the columns represent the number of weights of each neuron and must equal the number of neurons in the previous layer (or for the first layer, the number of features)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"https://www.jigsawlabs.io/free\" style=\"position: center\"><img src=\"https://storage.cloud.google.com/curriculum-assets/curriculum-assets.nosync/mom-files/jigsaw-labs.png\" width=\"15%\" style=\"text-align: center\"></a>\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
