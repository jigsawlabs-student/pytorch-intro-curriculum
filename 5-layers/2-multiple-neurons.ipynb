{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last lesson, we saw that we can rewrite the hypothesis function for the sigmoid neuron by using the dot product.  Specifically, we rewrote our linear function: \n",
    "\n",
    "$z(x) = w_1x_1 + w_2x_2 + b$ as: \n",
    "\n",
    "$z(x) =  \\begin{bmatrix}\n",
    "x_1 & x_2 \\\\\n",
    "\\end{bmatrix}\\cdot \\begin{bmatrix}\n",
    "w_1 \\\\\n",
    "w_2 \n",
    "\\end{bmatrix} + b = (x\\cdot w + b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we wrapped our linear function $z(x)$ in our activation function, the sigmoid function, to get $h(x) = \\sigma(x \\cdot w + b)$.  We diagram this hypothesis function as the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"sigmoid-neuron.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we'll learn what it means to make predictions with an multiple neurons simultaneously.  Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More features, more neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now let's return to our neuron that determines whether food contains sugar or not.  Let's say that the first food item contains the following attributes.\n",
    "\n",
    "* sweet taste 2, sweet smell 4\n",
    "* salty taste 3, salty smell 1\n",
    "\n",
    "And we capture this data in the vector $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 4, 3, 1])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.array([2, 4, 3, 1])\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now even though our neuron is only in charge of determining if something contains sugar, it still observes *all* of the features of the food."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "w_sugar = np.array([1, 3, -.5, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the corresponding weights above, in our sugar neuron there are positive weights associated with the sweetness features, and 0 or negative weights associated with saltiness.  In particular, the more the neuron detects a salty taste, the less it thinks there is sugar.  \n",
    "\n",
    "Ok, now let's set the neuron's bias to $-12$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_sugar = -12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see the calculate the output from the linear component of the neuron as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.dot(w_sugar) + b_sugar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the hypothesis of the sugar neuon as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(value): return 1/(1 + np.exp(-value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6224593312018546"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(x.dot(w_sweet) + b_sweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that the neuron assigns a probability of .62 that the first observation contains sugar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A second neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's feed this data into a second neuron, in charge of a different prediction - whether the food contains salt.  Our food observation is the same, and so has the same four features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# sweet taste 2, sweet smell 4, salty taste 3, salty smell 1\n",
    "x = np.array([2, 4, 3, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_salt = np.array([0, -.5, 3, 1.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this time around, the sweetness weights are either 0 or negative - as this neuron determines if something is salty.  The neuron also has it's own bias for saltiness, which is $-7$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_salt = -7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.5"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.dot(w_salt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8175744761936437"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(x.dot(w_salt) + b_salt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this second neuron predicts whether or not the food contains salt, and here assigns a $.817$ likelihood to the probability of there being salt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thinking with multiple neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple points to take from the above discussion. \n",
    "\n",
    "#### 1. Different weights same input\n",
    "\n",
    "The first point, is that we use the **same vector** $x$ as an input to each neuron.  The two neurons simply weigh these inputs differently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$z_{sugar}(x) = \\begin{bmatrix}\n",
    "x_1 & x_2 & x_3 & x_4 \\\\\n",
    "\\end{bmatrix}\\cdot \\begin{bmatrix}\n",
    "1 \\\\\n",
    "3 \\\\\n",
    "-.5 \\\\\n",
    "0 \\\\\n",
    "\\end{bmatrix} - 12 = .5$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$z_{salt}(x) = \\begin{bmatrix}\n",
    "x_1 & x_2 & x_3 & x_4 \\\\\n",
    "\\end{bmatrix}\\cdot \\begin{bmatrix}\n",
    "0 \\\\\n",
    "-.5 \\\\\n",
    "3 \\\\\n",
    "1.5 \\\\\n",
    "\\end{bmatrix} - 8 = 1.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6224593312018546"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(x.dot(w_sugar) + b_sugar)\n",
    "\n",
    "sigmoid(x.dot(w_salt) + b_salt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the diagram below, representing the layer of a neural network, it's illustrating this point: the attributes of an observation $x_1$ through $x_4$, are fed to each of the four neurons in the layer.  That's why each feature has a line drawn to each neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://storage.googleapis.com/curriculum-assets/nn-from-scratch/first-layer.png\" width=\"20%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So just to recap, every neuron in a layerr receives the same inputs.  Each neuron just has separate weights and biases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making it brief"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we understand that each neuron receives the same inputs, let's see if we can condense our representation of our neurons.  To do this, let's start by removing the biases of our two neurons.  This gives us:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$g_{sugar}(x) = \\begin{bmatrix}\n",
    "2 & 4 & 3 & 1 \\\\\n",
    "\\end{bmatrix}\\cdot \\begin{bmatrix}\n",
    "1 \\\\\n",
    "3 \\\\\n",
    "-.5  \\\\\n",
    "0 \\\\\n",
    "\\end{bmatrix} = 2*1 + 4*3 + 3*-.5+ 1*0 = 12.5$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$g_{salt}(x) = \\begin{bmatrix}\n",
    "2 & 4 & 3 & 1 \\\\\n",
    "\\end{bmatrix}\\cdot \\begin{bmatrix}\n",
    "0 \\\\\n",
    "-.5 \\\\\n",
    "3 \\\\\n",
    "1.5 \\\\\n",
    "\\end{bmatrix} = 2*0 + 4*-.5 + 3*3 + 1*1.5 = 8.5$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, observe that if we combine these two weight vectors into a matrix, then we can get the following: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{bmatrix}\n",
    "2 & 4 & 3 & 1 \\end{bmatrix} \\cdot\n",
    "\\begin{bmatrix}\n",
    "1 & 0\\\\\n",
    "3 & -.5 \\\\\n",
    "-.5 & 3 \\\\\n",
    "0 & 1.5\\end{bmatrix} = \\begin{bmatrix}\n",
    "12.5 & 8.5 \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or more generally: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{bmatrix}\n",
    "- & x &  -  \n",
    "\\end{bmatrix} \\cdot \\begin{bmatrix}\n",
    "|  & |  \\\\\n",
    "w_{1}  & w_{2} \\\\\n",
    "|   & |\n",
    "\\end{bmatrix}  = \\begin{bmatrix}\n",
    "x \\cdot w_{1} & x \\cdot w_{2} \\end{bmatrix}  = \\begin{bmatrix} z_{1}(x) & z_{2}(x) \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prove this in code.  We'll start by placing our feature vectors in a matrix W. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1. ,  0. ],\n",
       "       [ 3. , -0.5],\n",
       "       [-0.5,  3. ],\n",
       "       [ 0. ,  1.5]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_salt = np.array([0, -.5, 3, 1.5])\n",
    "w_sugar = np.array([1, 3, -.5, 0])\n",
    "\n",
    "W = np.vstack([w_sugar, w_salt]).T\n",
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we'll multiply the attributes of our observation $x$ by the weights of these neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12.5,  8.5])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = x.dot(W)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have just seen that we can use matrix multiplication to calculate the weighted sum of multiple neurons.  \n",
    "\n",
    "However, we still have not included the biases.  To complete our linear function, we need to add the bias of $-12$ for $z_{sugar}$ and our bias of $-7$ for $z_{salt}$.  We do so by placing the two biases into a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.array([-12, -7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5, 1.5])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.dot(W) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or summarizing the above, we can calculate the outputs of both linear functions with:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{bmatrix}\n",
    "- & x &  -  \n",
    "\\end{bmatrix} \\cdot \\begin{bmatrix}\n",
    "|  & |  \\\\\n",
    "w_1  & w_2 \\\\\n",
    "|   & |\n",
    "\\end{bmatrix} + \\begin{bmatrix} b_1 & b_2 \\end{bmatrix} = \\begin{bmatrix}\n",
    "x \\cdot w_1 & x \\cdot w_2 \\end{bmatrix} + \\begin{bmatrix} b_1 & b_2 \\end{bmatrix} = \\begin{bmatrix} z_1(x) & z_2(x) \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which we can summarize as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$z = x\\cdot W  + b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where z is a vector consisting of the output of each neuron's linear function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5, 1.5])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.dot(W) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Your turn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now consider that we have two different neurons that detect savory and bitter tastes.  They do so with the following weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 1],\n",
       "       [0, 1],\n",
       "       [1, 2],\n",
       "       [2, 0]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([2, 4, 3, 1])\n",
    "\n",
    "w_savory = np.array([2, 0, 1, 2])\n",
    "w_bitter = np.array([1, 1, 2, 0])\n",
    "\n",
    "W_new = np.vstack([w_savory, w_bitter]).T\n",
    "W_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to calculate the output of $x \\cdot W$, where $x$ is the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your answer here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check your answer with the dot product below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Including the activation layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can calculate hypothesis made by each neuron in a layer with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.62245933, 0.81757448])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(x.dot(W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or mathematically, we can write our layer as the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\sigma(x \\cdot W + b) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where sigma is applied to each entry of the vector resulting from $W\\cdot x + b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\sigma (x \\cdot W + b) = \\begin{bmatrix} \\sigma(z_1) & \\sigma(z_2) \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or expressing the above formula as two layers of a neural network, our linear layer and an activation layer.  We can express this as the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$z = (x \\cdot W + b)$\n",
    "\n",
    "$a = \\sigma(z)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $z$ is our linear layer and $a$ is our activation layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson we saw the components to build a layer of a neural network.  A single layer is a combination of a weighted input and a sigmoid activation function.  \n",
    "\n",
    "The weighted input can be represented by $x \\cdot W + b$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{bmatrix}\n",
    "- & x &  -  \n",
    "\\end{bmatrix} \\cdot \\begin{bmatrix}\n",
    "|  & |  \\\\\n",
    "w_1  & w_2 \\\\\n",
    "|   & |\n",
    "\\end{bmatrix} + \\begin{bmatrix} b_1 & b_2 \\end{bmatrix} = \\begin{bmatrix}\n",
    "x \\cdot w_1 & x \\cdot w_2 \\end{bmatrix} + \\begin{bmatrix} b_1 & b_2 \\end{bmatrix} = \\begin{bmatrix} l_1(x) & l_2(x) \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The row vector $x$ represents the features of a single observation.\n",
    "* Each column of the matrix W, contains the weights of a separate neuron, with the entries of $b$ as the corresponding biases.\n",
    "\n",
    "The output of the weighted input is fed into the activation function, which applies an entrywise operation.  Here, we use the sigmoid function.  So we can summarize the operations of our entire layer as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\sigma (W\\cdot x + b) = \\begin{bmatrix} \\sigma(z_1) \\\\ \\sigma(z_2) \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can break up the above as a linear layer $z$ and an activation layer $a$ where:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$z = (x \\cdot W + b)$\n",
    "\n",
    "$a = \\sigma(z)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"https://www.jigsawlabs.io/free\" style=\"position: center\"><img src=\"https://storage.cloud.google.com/curriculum-assets/curriculum-assets.nosync/mom-files/jigsaw-labs.png\" width=\"15%\" style=\"text-align: center\"></a>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9, 12])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.dot(W_new)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
