{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Completing the feedforward: the last layer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now so far we have learned how to write almost the entire hypothesis function of a neural network.  There is only step remaining, and that is adding the last layer to a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call the last layer of a neural network the output layer.  How many outputs should there be in the output layer?  With a classification problem -- which we have been using thus far -- it's generally one prediction per each potential outcome.  So in building a network to detect cancer the practice would be to have one neuron that makes a prediction for positive and one prediction for negative.\n",
    "\n",
    "This makes more sense in a problem that has *more than two* potential outcomes.  For example, below is observations from the famous MNIST dataset.  The MNIST dataset feeds images (in the form of pixels) into a neural network, and the network is tasked with predicting which of the ten digits the handwritten digit represents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://storage.googleapis.com/curriculum-assets/nn-from-scratch/mnist.png\" width=\"30%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that the neural network should output a vector with ten entries, with each entry indicating the probability of a different outcome.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.05, 0.62, 0.03, 0.03, 0.05, 0.03, 0.15, 0.01, 0.02, 0.01]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[.05, .62, .03, .03, .05, .03, .15, .01, .02, .01]\n",
    "# 0    1    2   3    4     5   6   7     8   9  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For example, above, the neural network is saying there is a probability of $.62$ that an observation represents the number 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But let's build up to this.  We can start by defining the input layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The input layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each observation in our dataset, that is, each image of hand written digits has 28 pixels per row  of the image and 28 per column.  And so each observation of is represented by a feature vector of 784, one pixel per each column.  \n",
    "\n",
    "So this means that the first layer should have neurons that accept feature vector that has $28\\times28 = 784$ entries.  Let's initialize a weight matrix that has 784 rows and 20 neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> First we create a dummy observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(4)\n",
    "x = np.random.randn(784)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we initialize our first layer of 20 neurons -- our first layer of neurons which accepts each of these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2)\n",
    "W_1 = np.random.randn(784, 20)\n",
    "b_1 = np.random.randn(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(value): return 1/(1 + np.exp(-value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = sigmoid(x.dot(W_1) + b_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20,)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have twenty outputs from our first layer.  Now, we don't need more than two layers in our neural network, so let's stick to just an input layer and an output layer to keep things simple. Let's keep going."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The last layer: the output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now let's think of the dimensions of the output layer.  We want to connect the twenty neurons from the input layer to each of the neurons in the output layer.  So this means that our weight matrix should have 20 rows, so that each neuron can feed in the outputs from the previous layer.  \n",
    "\n",
    "And how many neurons should we have.  Well we want our network to output ten values, so this means that we should have 10 neurons.  Each neuron is responsible for the probability of the observation being each digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3)\n",
    "W_2 = np.random.randn(20, 10)\n",
    "b_2 = np.random.randn(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now our network looks like the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = sigmoid(x.dot(W_1) + b_1)\n",
    "l2 = l1.dot(W_2) + b_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.68,  0.62, -0.97, -3.57,  0.65, -2.15, -0.89,  0.61,  1.44,\n",
       "       -3.25])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vector above contains pretty standard outputs from our last linear layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking the exponent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now there are a couple of goals that we would like to achieve with this ouput layer.  One is that we want just one number to dominate.  Remember that we have 10 outcomes, and each will be assigned a probability.  So if all of the other 9 outcomes is assigned a small probability like .03, this still only leaves $1 - 9*.03 = .73$ for the outcome we are most confident about. \n",
    "\n",
    "So we can exaggerate the outputs we are confident about by taking the exponent of each value.  Let's see what this does. For small numbers, like say, $1.2$, they are turned into:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.3201169227365472"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But for larger numbers like $4.8$ it's transformed into:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121.51041751873485"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(4.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the exponent really exaggerates our largest number in comparison to all of the other numbers.  And let's see how it treats negative values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.049787068367863944"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It transforms the number into a small positive output, which also is appropriate for returning probabilities.  (We cannot say there is a negative probability that an outcome occurs.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's apply the exponent function to every output from our last layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([39.73,  1.86,  0.38,  0.03,  1.92,  0.12,  0.41,  1.85,  4.2 ,\n",
       "        0.04])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.set_printoptions(precision=2, suppress=True) # turn off scientific notation\n",
    "exp = np.exp(l2)\n",
    "exp.round(2) # round each number to two digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So just as we wanted, we now have one entry that dominates all of the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3783390164769021"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting to probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now does our vector above look like a vector of probabilities? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([39.73,  1.86,  0.38,  0.03,  1.92,  0.12,  0.41,  1.85,  4.2 ,\n",
       "        0.04])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probably not.  For an entry to be a valid probability, it must be between 0 and 1, and the entire vector should add up to one.  \n",
    "\n",
    "> In other words, the probability that our observation is *any* of the potential outcomes (0 through 9) is 1.\n",
    "\n",
    "We can transform our vector this by calculating the percentage that each entry is of the total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50.52667977761745"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_exp = sum(exp)\n",
    "sum_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.79, 0.04, 0.01, 0.  , 0.04, 0.  , 0.01, 0.04, 0.08, 0.  ])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp/sum_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we are saying there is a $.79$ probability of our image being the number 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapping up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may have seen complicated, but this was our entire neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.79, 0.04, 0.01, 0.  , 0.04, 0.  , 0.01, 0.04, 0.08, 0.  ])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1 = sigmoid(x.dot(W_1) + b_1)\n",
    "l2 = l1.dot(W_2) + b_2\n",
    "np.exp(l2)/sum(np.exp(l2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is the formula for the softmax function: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$softmax(x) = \\frac{e^x}{\\sum e^x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So with the softmax function, we can end finish with a neural network that returns the probability of each outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"https://www.jigsawlabs.io/free\" style=\"position: center\"><img src=\"https://storage.cloud.google.com/curriculum-assets/curriculum-assets.nosync/mom-files/jigsaw-labs.png\" width=\"15%\" style=\"text-align: center\"></a>\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
