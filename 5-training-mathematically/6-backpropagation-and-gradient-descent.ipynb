{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "wired-minimum",
   "metadata": {},
   "source": [
    "# BackPropagation and Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entertaining-belgium",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mexican-helping",
   "metadata": {},
   "source": [
    "Ok, so if you've made it this far, you've seen the main components of training a neural network through gradient descent.  We saw that we can calculate the effect of a change in our parameters across multiple layers through the chain rule.  And we also saw how we can descend along a cost curve for a neural network with multiple parameters: we calculate the partial derivative of each parameter and move in proportion to each parameter's partial derivative.  \n",
    "\n",
    "In this lesson, we'll layer on a couple components to our knowledge -- learning about backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recovered-affairs",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### The Setup\n",
    "\n",
    "Now believe it or not, we've already seen both forward propagation and backward propagation.  Let's review what we did in the last lesson and then we'll call out these two steps.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "increased-candle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_area</th>\n",
       "      <th>is_cancerous</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.326</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_area  is_cancerous\n",
       "0      1.001             1\n",
       "1      1.326             1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./cell_data-2.csv', index_col = 0)\n",
    "df[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cathedral-mileage",
   "metadata": {},
   "source": [
    "And converted it into tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "immune-petersburg",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "X_tensor = torch.tensor(df[['mean_area']].values)\n",
    "y_tensor = torch.tensor(df['is_cancerous'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formed-novel",
   "metadata": {},
   "source": [
    "Then we select our first observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "inclusive-monkey",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.0010], dtype=torch.float64), tensor(1))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_X = X[0]\n",
    "first_y = y[0]\n",
    "\n",
    "first_X, first_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surrounded-coordination",
   "metadata": {},
   "source": [
    "And we define the hypothesis function for our neuron: a linear layer and our activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "wrapped-bridges",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_fn(w, x, b):\n",
    "    return w*x + b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "special-webcam",
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_fn(z):\n",
    "    return torch.sigmoid(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "martial-spring",
   "metadata": {},
   "source": [
    "Now we'll need the linear and activation layers to make a prediction.  And we'll need the derivatives we calculated in the previous lessons to update the parameters of our hypothesis function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "framed-migration",
   "metadata": {},
   "source": [
    "Remember that we'll need the components needed to calculate $\\frac{\\delta J}{\\delta w}$ and $\\frac{\\delta J}{\\delta b}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "refined-banana",
   "metadata": {},
   "source": [
    "* $\\frac{\\delta J}{\\delta w} = \\frac{\\delta J}{\\delta \\sigma} * \\frac{\\delta \\sigma}{\\delta z} * \\frac{\\delta z}{\\delta w}$\n",
    "\n",
    "* $\\frac{\\delta J}{\\delta b} = \\frac{\\delta J}{\\delta \\sigma} * \\frac{\\delta \\sigma}{\\delta z} * \\frac{\\delta z}{\\delta b}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educational-crash",
   "metadata": {},
   "source": [
    "And as we calculated in previous lessons, these component derivatives are the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "geographic-pension",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def delta_J_delta_sigma(y_hat, y):\n",
    "    return torch.sum(2*(y_hat - y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "certain-dispute",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_sigma_delta(z):\n",
    "    return torch.sigmoid(z)*(1 - torch.sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "embedded-tanzania",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_z_delta_w(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "involved-dover",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_z_delta_b():\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleased-disease",
   "metadata": {},
   "source": [
    "### The prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medical-costs",
   "metadata": {},
   "source": [
    "Ok, so now it's time to make our prediction.  We'll initialize our parameters with some random values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "graduate-enemy",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.tensor(2.)\n",
    "b = torch.tensor(-2.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposite-mustang",
   "metadata": {},
   "source": [
    "And make our prediction with just our first datapoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "systematic-liberal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0020], dtype=torch.float64)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = linear_fn(w, first_x, b)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "effective-hearts",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5005], dtype=torch.float64)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = activation_fn(z)\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broad-scott",
   "metadata": {},
   "source": [
    "Now this step of calculating the output at each layer is called **forward propagation**.  We are passing data through each layer until we get to a prediction -- above $\\hat{y} = .50005$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "announced-spouse",
   "metadata": {},
   "source": [
    "### Calculating the derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contrary-olympus",
   "metadata": {},
   "source": [
    "Ok, now that we completed the forward propagation, where we passed through our data and calculated an output at each layer, the next step is backward propagation.  With backward propagation, we do something slightly different than we saw in the last lesson.  Previously, we calculated how the parameters $w$ and $b$ impact the cost curve $J$ by multiplying together the component derivatives like so:\n",
    "\n",
    "* $\\frac{\\delta J}{\\delta w} = \\frac{\\delta J}{\\delta \\sigma} * \\frac{\\delta \\sigma}{\\delta z} * \\frac{\\delta z}{\\delta w}$\n",
    "\n",
    "* $\\frac{\\delta J}{\\delta b} = \\frac{\\delta J}{\\delta \\sigma} * \\frac{\\delta \\sigma}{\\delta z} * \\frac{\\delta z}{\\delta b}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consolidated-sample",
   "metadata": {},
   "source": [
    "And it's the same thing in code.  We calculate the component derivatives..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "random-seattle",
   "metadata": {},
   "outputs": [],
   "source": [
    "dj_dsig = delta_J_delta_sigma(y_hat, first_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "hungarian-lewis",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsig_dz = delta_sigma_delta(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "smart-speaking",
   "metadata": {},
   "outputs": [],
   "source": [
    "dz_dw = delta_z_delta_w(first_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "introductory-ordering",
   "metadata": {},
   "source": [
    "And then to calculate $\\frac{\\delta J}{\\delta w}$ we just multiply the components together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "lightweight-wheat",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2500], dtype=torch.float64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dj_dsig*dsig_dz*dz_dw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floral-growing",
   "metadata": {},
   "source": [
    "### Onto backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "according-devices",
   "metadata": {},
   "source": [
    "With backpropagation, we work backwards, calculating each layer's impact on $J$.  It's easier to understand by example:  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alive-validation",
   "metadata": {},
   "source": [
    "1. We start by seeing the impact of a small change in the output of our last layer $\\sigma$ on $J$, $\\frac{\\delta J}{\\delta \\sigma}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recognized-stockholm",
   "metadata": {},
   "source": [
    "> This is the same thing we calculated previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "developing-antenna",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.9990, dtype=torch.float64)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dj_dsig = delta_J_delta_sigma(y_hat, first_y)\n",
    "dj_dsig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "permanent-resident",
   "metadata": {},
   "source": [
    "2. Then we find the impact of a small change in the output of the linear layere on $J$, $\\frac{\\delta J}{\\delta z}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assigned-fundamentals",
   "metadata": {},
   "source": [
    "> This, we have **not** found calculated before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "center-memory",
   "metadata": {},
   "source": [
    "This is the formula: $\\frac{\\delta J}{\\delta z} = \\frac{\\delta \\sigma}{\\delta z}*\\frac{\\delta J}{\\delta \\sigma}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olive-england",
   "metadata": {},
   "source": [
    "Let's think about how the formula above makes sense.  If we want to find a change in $z's$ impact on $J$, this is the impact that $z$ has on the activation layer $\\sigma$ multiplied by $\\sigma$'s impact on $J$.  So to perform this calculation, we only need to calculate one thing new: $\\frac{\\delta \\sigma}{\\delta z}$.  We already calculated $\\frac{\\delta J}{\\delta \\sigma}$ above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "processed-thesaurus",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2497], dtype=torch.float64)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsig_dz = deriv_sigma(z)\n",
    "dz_dJ = dsig_dz*dj_dsig\n",
    "dz_dJ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legislative-agreement",
   "metadata": {},
   "source": [
    "So let's add some terms to our two component derivatives in the formula: $\\frac{\\delta J}{\\delta z} = \\frac{\\delta \\sigma}{\\delta z}*\\frac{\\delta J}{\\delta \\sigma}$.  \n",
    "\n",
    "* We'll call our newly calculated derivative $\\frac{\\delta \\sigma}{\\delta z}$ our **local derivative**, and\n",
    "* we'll call the derivative we calculated in the previous layer $\\frac{\\delta J}{\\delta \\sigma}$ our **upstream derivative**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functional-dominant",
   "metadata": {},
   "source": [
    "As we'll see this is the approach we'll continue to use as we move down through our layers: calculate the new local derivative and multiply it by the derivative upstream derivative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threaded-phone",
   "metadata": {},
   "source": [
    "3. Find the impact of a change in $w$ on $J$, that is $\\frac{\\delta J}{\\delta w}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civilian-villa",
   "metadata": {},
   "source": [
    "Now to make this calculation, our formula is: $\\frac{\\delta J}{\\delta w} = \\frac{\\delta w}{\\delta z}*\\frac{\\delta J}{\\delta z}$.  So once again, it's our local derivative where we find $w's$ impact on $z$.  \n",
    "\n",
    "And then we multiply by the derivative we just calculated upstream, $\\frac{\\delta J}{\\delta z}$, $z$'s impact on our cost function $J$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "concrete-import",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2500], dtype=torch.float64)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dw_dJ = dz_dw*dz_dJ\n",
    "dw_dJ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turkish-sleeping",
   "metadata": {},
   "source": [
    "So we get the same answer when multiplying all three terms together: $\\frac{\\delta J}{\\delta w} = \\frac{\\delta J}{\\delta \\sigma} * \\frac{\\delta \\sigma}{\\delta z} * \\frac{\\delta z}{\\delta w}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "stopped-partner",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2500], dtype=torch.float64)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dj_dsig*dsig_dz*dz_dw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intelligent-defensive",
   "metadata": {},
   "source": [
    "What's nice about this is that **we can reuse $\\frac{\\delta z}{\\delta J}$** when calculating $\\frac{\\delta J}{\\delta b}$.  Let's see this.\n",
    "\n",
    "4. Find the impact of a change in $b$ on $J$, that is $\\frac{\\delta J}{\\delta b}$.\n",
    "\n",
    "This again is our local derivative multiplied by the same directly upstream derivative $\\frac{\\delta z}{\\delta J}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fundamental-methodology",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2497], dtype=torch.float64)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dJ_db = dz_db*dz_dJ\n",
    "dJ_dw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspended-handy",
   "metadata": {},
   "source": [
    "So this is backpropagation.  It's a small trick that allows us to reuse some of our earlier calculations.  With backpropagation, we start with our top most layer, here $\\frac{\\delta \\sigma}{\\delta J}$, and then move down through our layers each time calculating each layer's impact on the cost function $J$ by multiplying the local derivative by the direectly upstream derivative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collectible-segment",
   "metadata": {},
   "source": [
    "### All together now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medical-siemens",
   "metadata": {},
   "source": [
    "Ok, now let's sum our process of forward propagation and backward propagation.  With forward propagation we initialize the values of our parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "constitutional-disposition",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.tensor(2.)\n",
    "b = torch.tensor(-2.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imported-origin",
   "metadata": {},
   "source": [
    "And then pass our data through our layers to calculate the output at each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "detailed-stability",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z =  tensor([0.0020], dtype=torch.float64)\n",
      "y_hat =  tensor([0.5005], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "z = linear_fn(w, first_x, b)\n",
    "print('z = ', z)\n",
    "y_hat = activation_fn(z)\n",
    "print('y_hat = ', y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outstanding-addiction",
   "metadata": {},
   "source": [
    "And with backward propagation, we then calculate how nudging each layer's output will change our cost function.  And we make this calculation by multiplying the local derivative with the upstream derivative.  So to calculate $\\frac{\\delta J}{\\delta w}$, we just move downward through our layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "nuclear-fundamental",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2500], dtype=torch.float64)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dj_dsig = delta_J_delta_sigma(y_hat, first_y)\n",
    "dJ_dz = delta_sigma_delta(z)*dj_dsig\n",
    "dJ_dw = delta_z_delta_w(first_x)*dJ_dz\n",
    "\n",
    "dJ_dw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dried-morning",
   "metadata": {},
   "source": [
    "And then in calculating $\\frac{\\delta J}{\\delta b}$, we already calculated the upstream derivative, so we only have to calculate the local derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "careful-ghost",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2497], dtype=torch.float64)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dJ_db = delta_z_delta_b()*dJ_dz\n",
    "dJ_db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "african-edward",
   "metadata": {},
   "source": [
    "> Notice, that in calculating our derivatives above, we need to use calculations from forward propagation.  For example, we passed through `y_hat` to calculate `delta_J_delta_sigma`.  And we passed through `z` to calculate `delta_sigma_delta(z)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distant-style",
   "metadata": {},
   "source": [
    "Ok, so now let's repeatedly perform forward and backward propagation to descend along the cost curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "gorgeous-specification",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.tensor(2.)\n",
    "b = torch.tensor(-2.)\n",
    "eta = .0001 # define learning rate \n",
    "for i in range(20):\n",
    "    for x, y in zip(X_tensor, y_tensor):\n",
    "    # forward propagation\n",
    "        z = linear_fn(w, x, b)\n",
    "        y_hat = activation_fn(z)\n",
    "    \n",
    "    # backward propagation\n",
    "        dj_dsig = delta_J_delta_sigma(y_hat, y)\n",
    "        dJ_dz = delta_sigma_delta(z)*dj_dsig\n",
    "        dJ_dw = delta_z_delta_w(first_x)*dJ_dz\n",
    "        \n",
    "        dJ_db = delta_z_delta_b()*dJ_dz\n",
    "    \n",
    "    # update params w, b\n",
    "        w = w - eta*dJ_dw\n",
    "        b = b - eta*dJ_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "unavailable-concord",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2.0255], dtype=torch.float64), tensor([-1.9746], dtype=torch.float64))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bored-doctor",
   "metadata": {},
   "source": [
    "And notice how this aligns with the gradient descent approach we saw in Pytorch. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comfortable-malpractice",
   "metadata": {},
   "source": [
    "```python\n",
    "for (x, y) in zip(X_train_tensor_gpu, y_train_tensor_gpu): # loop through observations and labels\n",
    "    net.zero_grad() # remove calculated derivatives\n",
    "    y_hat = net(X_tensor)             # 1. Forward prop: With current weights make a prediction\n",
    "    loss = criterion(y_hat, y_tensor) # 2. See how off the prediction is according to the cost function\n",
    "    loss.backward()                   # 3. Back prop: Calculate how each layer's change in output affects J\n",
    "    opt.step()                        # 4. Update params based on eta and calculated derivatives \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressed-patrick",
   "metadata": {},
   "source": [
    "So that's forward and backward propagation.  The only thing left is to show, how we can use what we know about matrix algebra to perform the tasks above.  We'll tackle that in our next lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coastal-limit",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imposed-collection",
   "metadata": {},
   "source": [
    "In this lesson, we learned about forward and backward propagation.  Forward propagation just means to calculate the output of each layer when we pass data through our hypothesis function.  \n",
    "\n",
    "And with backward propagation we calculate each layer's impact on the cost function by multiplying the local derivative by the upstream derivative.  So moving backwards through our layers, we calculated:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respected-italic",
   "metadata": {},
   "source": [
    "* $\\frac{\\delta J}{\\delta \\sigma}$\n",
    "\n",
    "* $\\frac{\\delta J}{\\delta z} = \\frac{\\delta \\sigma}{\\delta z}*\\frac{\\delta J}{\\delta \\sigma}$\n",
    "\n",
    "* $\\frac{\\delta J}{\\delta w} = \\frac{\\delta w}{\\delta z}*\\frac{\\delta J}{\\delta z}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "academic-blood",
   "metadata": {},
   "source": [
    "And finally, we calculated:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scientific-activity",
   "metadata": {},
   "source": [
    "* $\\frac{\\delta J}{\\delta b} = \\frac{\\delta b}{\\delta z}*\\frac{\\delta J}{\\delta z}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uniform-rogers",
   "metadata": {},
   "source": [
    "Each time, we multiplied the local derivative by the upstream derivative.  \n",
    "\n",
    "Finally, we used forward and backward propagation to perform gradient descent, and saw how it aligned with our code in Pytorch."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
