{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "august-monkey",
   "metadata": {},
   "source": [
    "# BackPropagation and Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "figured-dutch",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noble-uzbekistan",
   "metadata": {},
   "source": [
    "Ok, so if you've made it this far, you've seen the main components of training a neural network through gradient descent.  We saw that we can calculate the effect of a change in our parameters across multiple layers through the chain rule.  And we also saw how we can descend along a cost curve for a neural network with multiple parameters: we calculate the gradient.  That means, we calculate partial derivative of each parameter and move in proportion to each parameter's partial derivative.  \n",
    "\n",
    "In this lesson, we'll see a small update to the way learn neural networks calculate how a change in parameters affects the cost curve -- by performing backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "covered-cooper",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### The Setup\n",
    "\n",
    "Now believe it or not, we've already seen much of what is involvd in both forward propagation and backward propagation -- whatever that means.  Let's review what we did in the last lesson and then we'll call out these two steps.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "motivated-springer",
   "metadata": {},
   "source": [
    "> We'll begin by loading our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "several-greeting",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./cell_multiple.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "broken-harvey",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_area</th>\n",
       "      <th>mean_concavity</th>\n",
       "      <th>is_cancerous</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.001</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.326</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_area  mean_concavity  is_cancerous\n",
       "0      1.001          0.3001             1\n",
       "1      1.326          0.0869             1"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "metropolitan-valuable",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df.to_csv('./cell_multiple.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annual-joint",
   "metadata": {},
   "source": [
    "And converted it into tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "comfortable-department",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "X_tensor = torch.tensor(df[['mean_area', 'mean_concavity']].values).float()\n",
    "y_tensor = torch.tensor(df['is_cancerous']).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "august-highlight",
   "metadata": {},
   "source": [
    "Then we select our first observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "actual-valley",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.0010, 0.3001]), tensor(1.))"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_x = X_tensor[0]\n",
    "first_y = y_tensor[0]\n",
    "\n",
    "first_x, first_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compatible-tennis",
   "metadata": {},
   "source": [
    "### The prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "running-anger",
   "metadata": {},
   "source": [
    "Ok, so now it's time to make our prediction.  We'll define the components of our hypothesis function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "running-stick",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_fn(x, w, b):\n",
    "    return x @ w + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "activated-print",
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_fn(z):\n",
    "    return torch.sigmoid(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "based-embassy",
   "metadata": {},
   "source": [
    "And initialize the related weight vectors and bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "cleared-exhibit",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.tensor([.5, .3]).float()\n",
    "b = torch.tensor(-2.).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silent-cache",
   "metadata": {},
   "source": [
    "And make our prediction with just our first datapoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "necessary-judgment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-1.4095)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_fn(first_x, w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "dramatic-learning",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1669], dtype=torch.float64)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = activation_fn(z)\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "placed-purpose",
   "metadata": {},
   "source": [
    "Now this step of calculating the output at each layer is called **forward propagation**.  We are passing data through each layer until we get to a prediction -- above $\\hat{y} = .1669$.  So that's it forward propagation is just passing data through the layers of the hypothesis function of our neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legislative-georgia",
   "metadata": {},
   "source": [
    "### Reviewing the gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foster-commonwealth",
   "metadata": {},
   "source": [
    "Now, of course, the parameters of our neural network, $w$ and $b$ were just set randomly by us.  We'll need to use gradient desccent to find the parameters that minimize the output from our cost function.  As we saw in the past lesson, we can summarize our gradient descent formula as the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescription-award",
   "metadata": {},
   "source": [
    "$$ \\theta = \\theta - \\eta \\frac{\\delta J}{\\delta \\theta}  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nervous-military",
   "metadata": {},
   "source": [
    "And remember that this term $\\frac{\\delta J}{\\delta \\theta}$ is a vector of each parameter's partial derivative.  \n",
    "\n",
    "Our partial derivatives really consist of the following $\\frac{\\delta J}{\\delta w}$ and $\\frac{\\delta J}{\\delta b}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spanish-german",
   "metadata": {},
   "source": [
    "* $\\frac{\\delta J}{\\delta w} = \\frac{\\delta J}{\\delta \\sigma} * \\frac{\\delta \\sigma}{\\delta z} * \\frac{\\delta z}{\\delta w}$\n",
    "\n",
    "* $\\frac{\\delta J}{\\delta b} = \\frac{\\delta J}{\\delta \\sigma} * \\frac{\\delta \\sigma}{\\delta z} * \\frac{\\delta z}{\\delta b}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gross-farming",
   "metadata": {},
   "source": [
    "And as we saw in previous lessons, these component derivatives are the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "legendary-rental",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def delta_J_delta_sigma(y_hat, y):\n",
    "    return torch.sum(2*(y_hat - y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fossil-commonwealth",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_sigma_delta(z):\n",
    "    return torch.sigmoid(z)*(1 - torch.sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "upper-illustration",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_z_delta_w(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "expensive-question",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_z_delta_b():\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fossil-password",
   "metadata": {},
   "source": [
    "So then we can calculate $\\frac{\\delta J}{\\delta w}$ by multiplying the component derivatives together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "unique-necessity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2319, -0.0695], dtype=torch.float64)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_grad = delta_z_delta_w(first_x)*delta_sigma_delta(z)*delta_J_delta_sigma(y_hat, first_y)\n",
    "w_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efficient-separation",
   "metadata": {},
   "source": [
    "And similarly we can calculate $\\frac{\\delta J}{\\delta b}$ by multiplying the component derivatives together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "tropical-forty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2317], dtype=torch.float64)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_grad = delta_z_delta_b()*delta_sigma_delta(z)*delta_J_delta_sigma(y_hat, first_y)\n",
    "b_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boxed-penalty",
   "metadata": {},
   "source": [
    "Now one thing we'll notice from the above is in calculating `w_grad` and `b_grad` we repeated the same operation twice: `delta_sigma_delta(z)*delta_J_delta_sigma(y_hat, first_y)`.  This may not seem like a big deal, but this number crunching of the gradient is pretty time consuming, so we'd like to be more efficient if possible.  \n",
    "\n",
    "So let's see how through backpropagation, we can avoid this duplication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sapphire-crossing",
   "metadata": {},
   "source": [
    "### Onto backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removable-subject",
   "metadata": {},
   "source": [
    "With backpropagation, this time instead of simply multiplying together the component derivatives at the end, we will work backwards through our layers, and calculatee each layer's impact on $J$.\n",
    "\n",
    "It's easier to understand by example.  Remember that these were our layers of our neural network:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frequent-shield",
   "metadata": {},
   "source": [
    "* $z(x_i) = w_1*x_i $\n",
    "* $\\sigma(z) =  \\frac{1}{1 + e^{-z(x)}} $\n",
    "* $ J(\\hat{y}, y) = \\sum  (y - \\hat{y})^2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "persistent-cornell",
   "metadata": {},
   "source": [
    "So now we work backwards seeing how our last layer, $\\sigma$ has an impact on our cost function.  So that's where we'll start."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "essential-papua",
   "metadata": {},
   "source": [
    "1. Calculate the impact of a small change in the output of our last layer $\\sigma$ on $J$, $\\frac{\\delta J}{\\delta \\sigma}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "pacific-veteran",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-1.6662, dtype=torch.float64)"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dj_dsig = delta_J_delta_sigma(y_hat, first_y)\n",
    "dj_dsig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lonely-surprise",
   "metadata": {},
   "source": [
    "> This is exactly the same calclulation $\\frac{\\delta J}{\\delta \\sigma}$ that we calculated previously, so we just reuse the function from above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesser-folks",
   "metadata": {},
   "source": [
    "2. Then we find the impact of a small change in the output of the linear layer $z$ on $J$, $\\frac{\\delta J}{\\delta z}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mediterranean-personality",
   "metadata": {},
   "source": [
    "> This, we have **not** found calculated before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "previous-intervention",
   "metadata": {},
   "source": [
    "This is the formula: $\\frac{\\delta J}{\\delta z} = \\frac{\\delta \\sigma}{\\delta z}*\\frac{\\delta J}{\\delta \\sigma}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serial-macedonia",
   "metadata": {},
   "source": [
    "Let's think about how the formula above makes sense.  If we want to find a change in $z's$ impact on $J$, this is the impact that $z$ has on the activation layer $\\sigma$ multiplied by $\\sigma$'s impact on $J$.  So to perform this calculation, we only need to calculate one thing new: $\\frac{\\delta \\sigma}{\\delta z}$.  We already calculated $\\frac{\\delta J}{\\delta \\sigma}$ above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "bright-kernel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2317], dtype=torch.float64)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsig_dz = delta_sigma_delta(z)\n",
    "dz_dJ = dsig_dz*dj_dsig\n",
    "dz_dJ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authentic-panic",
   "metadata": {},
   "source": [
    "Next let's add some terms to our two component derivatives in the formula: $\\frac{\\delta J}{\\delta z} = \\frac{\\delta \\sigma}{\\delta z}*\\frac{\\delta J}{\\delta \\sigma}$.  \n",
    "\n",
    "* We'll call our newly calculated derivative $\\frac{\\delta \\sigma}{\\delta z}$ our **local derivative**, and\n",
    "* we'll call the derivative we calculated in the previous layer $\\frac{\\delta J}{\\delta \\sigma}$ our **upstream derivative**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stable-queensland",
   "metadata": {},
   "source": [
    "As we'll see this is the approach we'll continue to use as we move down through our layers: calculate the new local derivative and multiply it by the derivative upstream derivative.\n",
    "\n",
    "Ok, let's now see how the next layer down has an impact on $J$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "successful-halloween",
   "metadata": {},
   "source": [
    "3. Find the impact of a change in $w$ on $J$, that is $\\frac{\\delta J}{\\delta w}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "temporal-producer",
   "metadata": {},
   "source": [
    "Now to make this calculation, our formula is: $\\frac{\\delta J}{\\delta w} = \\frac{\\delta w}{\\delta z}*\\frac{\\delta J}{\\delta z}$.  So once again, it's our local derivative where we find $w's$ impact on $z$.  \n",
    "\n",
    "And then we multiply by the derivative we just calculated upstream, $\\frac{\\delta J}{\\delta z}$, $z$'s impact on our cost function $J$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "tested-arizona",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2500], dtype=torch.float64)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dw_dJ = dz_dw*dz_dJ\n",
    "dw_dJ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intermediate-recipient",
   "metadata": {},
   "source": [
    "So we get the same answer when multiplying all three terms together: $\\frac{\\delta J}{\\delta w} = \\frac{\\delta J}{\\delta \\sigma} * \\frac{\\delta \\sigma}{\\delta z} * \\frac{\\delta z}{\\delta w}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "rough-spectrum",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2500], dtype=torch.float64)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dj_dsig*dsig_dz*dz_dw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "realistic-cycle",
   "metadata": {},
   "source": [
    "What's nice about this is that **we can reuse $\\frac{\\delta z}{\\delta J}$** when calculating $\\frac{\\delta J}{\\delta b}$.  Let's see this.\n",
    "\n",
    "4. Find the impact of a change in $b$ on $J$, that is $\\frac{\\delta J}{\\delta b}$.\n",
    "\n",
    "This again is our local derivative multiplied by the same directly upstream derivative $\\frac{\\delta z}{\\delta J}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "optional-least",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2497], dtype=torch.float64)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dJ_db = dz_db*dz_dJ\n",
    "dJ_dw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "textile-filing",
   "metadata": {},
   "source": [
    "So this is backpropagation.  It's a small trick that allows us to reuse some of our earlier calculations.  With backpropagation, we start with our top most layer, here $\\frac{\\delta \\sigma}{\\delta J}$, and then move down through our layers each time calculating each layer's impact on the cost function $J$ by multiplying the local derivative by the direectly upstream derivative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acceptable-complexity",
   "metadata": {},
   "source": [
    "### All together now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordered-creativity",
   "metadata": {},
   "source": [
    "Ok, now let's sum our process of forward propagation and backward propagation.  With forward propagation we initialize the values of our parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "understanding-wrong",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.tensor([.5, .3]).float()\n",
    "b = torch.tensor(-2.).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fleet-monaco",
   "metadata": {},
   "source": [
    "And then perform forward propagation by passing our data through our layers and calculating the output at each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "worse-terry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z =  tensor(-1.4095)\n",
      "y_hat =  tensor(0.1963)\n"
     ]
    }
   ],
   "source": [
    "z = linear_fn(first_x, w, b)\n",
    "print('z = ', z)\n",
    "y_hat = activation_fn(z)\n",
    "print('y_hat = ', y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrow-saying",
   "metadata": {},
   "source": [
    "Once we calculated the output at each layer, we move to backward propagation.  With backward propagation, we calculate how nudging each layer's output will change our cost function.  And we make this calculation by multiplying the local derivative with the upstream derivative.  So to calculate $\\frac{\\delta J}{\\delta w}$, we just move downward through our layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "christian-transcription",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2500], dtype=torch.float64)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dj_dsig = delta_J_delta_sigma(y_hat, first_y)\n",
    "\n",
    "dJ_dz = delta_sigma_delta(z)*dj_dsig\n",
    "dJ_dw = delta_z_delta_w(first_x)*dJ_dz\n",
    "\n",
    "dJ_dw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designing-airport",
   "metadata": {},
   "source": [
    "And then in calculating $\\frac{\\delta J}{\\delta b}$, we already calculated the upstream derivative $\\frac{\\delta J}{\\delta z}$, and so we only have to calculate the local derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "irish-roberts",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2497], dtype=torch.float64)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dJ_db = delta_z_delta_b()*dJ_dz\n",
    "dJ_db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moved-estonia",
   "metadata": {},
   "source": [
    "> Notice, that in calculating our derivatives above, we did need to use calculations from forward propagation.  For example, we passed through `y_hat` to calculate `delta_J_delta_sigma`.  And we passed through `z` to calculate `delta_sigma_delta(z)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "higher-monster",
   "metadata": {},
   "source": [
    "Ok, so now let's repeatedly perform forward and backward propagation to descend along the cost curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "liable-december",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.tensor([.5, .3]).float()\n",
    "b = torch.tensor(-2.).float()\n",
    "eta = .0001 # define learning rate \n",
    "for i in range(20):\n",
    "    for x, y in zip(X_tensor, y_tensor):\n",
    "    # forward propagation\n",
    "        z = linear_fn(w, x, b)\n",
    "        y_hat = activation_fn(z)\n",
    "    \n",
    "    # backward propagation\n",
    "        dj_dsig = delta_J_delta_sigma(y_hat, y)\n",
    "        dJ_dz = delta_sigma_delta(z)*dj_dsig\n",
    "        dJ_dw = delta_z_delta_w(first_x)*dJ_dz\n",
    "        \n",
    "        dJ_db = delta_z_delta_b()*dJ_dz\n",
    "    \n",
    "    # update params w, b\n",
    "        w = w - eta*dJ_dw\n",
    "        b = b - eta*dJ_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "interested-tablet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.5786, 0.3236]), tensor(-1.9215))"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "burning-graphic",
   "metadata": {},
   "source": [
    "And notice how this aligns with the gradient descent approach we saw in Pytorch. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swiss-style",
   "metadata": {},
   "source": [
    "```python\n",
    "for (x, y) in zip(X_train_tensor_gpu, y_train_tensor_gpu): # loop through observations and labels\n",
    "    net.zero_grad() # remove calculated derivatives\n",
    "    y_hat = net(X_tensor)             # 1. Forward prop: With current weights make a prediction\n",
    "    loss = criterion(y_hat, y_tensor) # 2. See how off the prediction is according to the cost function\n",
    "    loss.backward()                   # 3. Back prop: Calculate how each layer's change in output affects J\n",
    "    opt.step()                        # 4. Update params based on eta and calculated derivatives \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exciting-killing",
   "metadata": {},
   "source": [
    "So that's forward and backward propagation.  Let's now go back to using Pytorch to perform image classification and see if we can understand the process any better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rental-russia",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "configured-issue",
   "metadata": {},
   "source": [
    "In this lesson, we learned about forward and backward propagation.  Forward propagation just means to calculate the output of each layer when we pass data through our hypothesis function.  \n",
    "\n",
    "And with backward propagation we calculate each layer's impact on the cost function by multiplying the local derivative by the upstream derivative.  So moving backwards through our layers, we calculated:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technological-administrator",
   "metadata": {},
   "source": [
    "* $\\frac{\\delta J}{\\delta \\sigma}$\n",
    "\n",
    "* $\\frac{\\delta J}{\\delta z} = \\frac{\\delta \\sigma}{\\delta z}*\\frac{\\delta J}{\\delta \\sigma}$\n",
    "\n",
    "* $\\frac{\\delta J}{\\delta w} = \\frac{\\delta w}{\\delta z}*\\frac{\\delta J}{\\delta z}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expanded-constitutional",
   "metadata": {},
   "source": [
    "And finally, we calculated:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functioning-framework",
   "metadata": {},
   "source": [
    "* $\\frac{\\delta J}{\\delta b} = \\frac{\\delta b}{\\delta z}*\\frac{\\delta J}{\\delta z}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considered-turning",
   "metadata": {},
   "source": [
    "Each time, we multiplied the local derivative by the upstream derivative.  \n",
    "\n",
    "Finally, we used forward and backward propagation to perform gradient descent, and saw how it aligned with our code in Pytorch."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
