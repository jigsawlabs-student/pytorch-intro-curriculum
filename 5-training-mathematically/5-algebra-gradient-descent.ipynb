{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "according-cameroon",
   "metadata": {},
   "source": [
    "# Matrix Algebra with Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protective-controversy",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interior-uruguay",
   "metadata": {},
   "source": [
    "So in the last lesson, we saw how we can descend a multidimensional cost curve with gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "innovative-america",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url = \"./cost_curve_three_d.json\"\n",
    "\n",
    "# df = pd.read_json(url)\n",
    "\n",
    "# go.Figure(df.to_dict('records'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mounted-china",
   "metadata": {},
   "source": [
    "And from there, we described our strategy as leaning in each direction, and then walking in proportion to the steepness in each direction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incoming-drunk",
   "metadata": {},
   "source": [
    "<img src=\"./walk-downhill.png\" width=\"30%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handy-education",
   "metadata": {},
   "source": [
    "We can can think of this as the biggest bang for our buck approach -- the steeper the slope descends, the more we move in that direction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broadband-pollution",
   "metadata": {},
   "source": [
    "### A review, and the gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appropriate-elimination",
   "metadata": {},
   "source": [
    "Now we saw that to this idea of leaning to the left and leaning to the right really involved taking the partial derivative with respect to each parameter.  So when our linear function is:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "employed-boost",
   "metadata": {},
   "source": [
    "$z = w*x + b$ and"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collective-naples",
   "metadata": {},
   "source": [
    "Then taking the partial derivative with respect to each parameter means taking the partial derivative first with respect to $w$ and then with respect to $b$.  So remember that whatever variables we *are not* taking the partial derivative with respect to, we treat as just a number.  \n",
    "\n",
    "* So when taking the partial derivative $\\frac{\\delta z}{\\delta w}$, we treat $b$ as if it were just a number, which gives us the following: \n",
    "\n",
    "    * $z = w*x + b$ and\n",
    "    * $\\frac{\\delta z}{\\delta w} = w^0*x + 0 = x$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "israeli-turkey",
   "metadata": {},
   "source": [
    "* And when taking the partial derivative $\\frac{\\delta z}{\\delta b}$, we treat $w$ as if it were just a number, which gives us the following:\n",
    "\n",
    "    * $z = w*x + b$ and\n",
    "    \n",
    "    * $\\frac{\\delta z}{\\delta b} = w*x + b = 0 +  b^{0} = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consistent-romantic",
   "metadata": {},
   "source": [
    "The partial derivatives that we found above can be represented as a vector.  A vector that represents the rate of change each direction.  And we represent this vector with the greek letter nabla,$\\nabla$, which represents our gradient.  So our gradient looks like the following:\n",
    "\n",
    "$$\\nabla z(w,b) = \\begin{bmatrix}\n",
    "\\frac{\\delta z}{\\delta w} \\\\\n",
    "  \\frac{\\delta z}{\\delta b} \n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unnecessary-flash",
   "metadata": {},
   "source": [
    "Or applying this to gradient of our linear function above, we get the following $z(w, b) = wx + b$\n",
    "\n",
    "$$\\nabla z(w,b) = \\begin{bmatrix}\n",
    "x \\\\\n",
    " 1 \n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaged-canvas",
   "metadata": {},
   "source": [
    "> The **gradient** of a function is a vector whose entries are the partial derivatives of the function.  It is the direction of fastest increase.  For gradient descent, we move in the direction of the negative gradient -- or the direction of greatest decrease."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expanded-kelly",
   "metadata": {},
   "source": [
    "So moving in the greatest descent, applied to our function $z$ above, we get the following:\n",
    "\n",
    "$$ - \\nabla z(w,b) = \\begin{bmatrix}\n",
    " -x \\\\\n",
    "  -1 \n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metric-prayer",
   "metadata": {},
   "source": [
    "And this is our vector of our partial derivatives with respect to each of our parameters in our linear function: $\\nabla z(w,b) = \\begin{bmatrix}\n",
    "\\frac{\\delta z}{\\delta w} \\\\\n",
    "  \\frac{\\delta z}{\\delta b} \n",
    "\\end{bmatrix}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historical-quantity",
   "metadata": {},
   "source": [
    "### Extending to multiple weights "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interpreted-spine",
   "metadata": {},
   "source": [
    "Now remember that our linear function will often multiple weights -- one for each feature in an observation.  So let's assume that we have a linear function that looks like the following: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dynamic-wrong",
   "metadata": {},
   "source": [
    "$z = w_1*x_1 + w_2*x_2 + b$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norwegian-injection",
   "metadata": {},
   "source": [
    "Then taking the partial derivative with respect to each parameter, we get the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adequate-ireland",
   "metadata": {},
   "source": [
    "*  $\\frac{\\delta z}{\\delta w_1} = w_1^0*x_1 + 0 + 0 = x_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pediatric-brain",
   "metadata": {},
   "source": [
    "*  $\\frac{\\delta z}{\\delta w_2} = 0 + w_2^0*x_2 + 0 = x_2$\n",
    "\n",
    "*  $\\frac{\\delta z}{\\delta b} = 0 + 0 + b^0 = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solid-investigation",
   "metadata": {},
   "source": [
    "So because we treat each term we are not taking the partial derivative with respect to as a just a number, for every term other than the one we are taking our partial derivative of, the term turns to a zero.  So if we want to express our gradient, it looks like the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comprehensive-pottery",
   "metadata": {},
   "source": [
    "$\\nabla z(w_1,w_2, b) = \\begin{bmatrix}\n",
    "\\frac{\\delta z}{\\delta w_1} \\\\\n",
    "\\frac{\\delta z}{\\delta w_2} \\\\\n",
    "  \\frac{\\delta z}{\\delta b} \n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "  1\n",
    "\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confidential-stockholm",
   "metadata": {},
   "source": [
    "So when we move to a multiparameter linear function, this is our gradient -- it's just the values of the vector $x$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quarterly-courage",
   "metadata": {},
   "source": [
    "### Confirming in Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "micro-developer",
   "metadata": {},
   "source": [
    "We can confirm this if we move to pytorch.  We can translate our linear function above:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprised-material",
   "metadata": {},
   "source": [
    "$z = w_1*x_1 + w_2*x_2 + b$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superior-forwarding",
   "metadata": {},
   "source": [
    "Into our expression with the dot product."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supported-fraction",
   "metadata": {},
   "source": [
    "$z = x \\cdot w + b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "brief-rachel",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([1., 2., 3.])\n",
    "\n",
    "w = torch.tensor([2., 4., 8.], requires_grad = True)\n",
    "\n",
    "b = torch.tensor(3., requires_grad = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "suburban-sierra",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(37., grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = x @ w + b\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "robust-original",
   "metadata": {},
   "source": [
    "And the derivative should just be the vector $x$ followed by the number 1 for the bias term.  We can see this in Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "sought-farming",
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "together-memphis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advised-singing",
   "metadata": {},
   "source": [
    "> So above, this is saying that the gradient of $w$ is just the vector $x$, $[x_1, x_2, x_3]$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "little-journey",
   "metadata": {},
   "source": [
    "And if we look at the gradient of $b$, we see this is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "spanish-jungle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agreed-binding",
   "metadata": {},
   "source": [
    "### Extending To The Cost Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coastal-impact",
   "metadata": {},
   "source": [
    "So above, we saw how to calculate the gradient of our linear function $z = w_1*x_1 + w_2*x_2 + b$, which we saw was a vector of partial derivatives with respect to each of our parameters:\n",
    "\n",
    "$\\nabla z(w_1,w_2, b) = \\begin{bmatrix}\n",
    "\\frac{\\delta z}{\\delta w_1} \\\\\n",
    "\\frac{\\delta z}{\\delta w_2} \\\\\n",
    "  \\frac{\\delta z}{\\delta b} \n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "  1\n",
    "\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "violent-reality",
   "metadata": {},
   "source": [
    "Now this gradient, indicates to us how the output of the linear function changes as we nudge any of our parameter terms.  And we saw that for our weight vector $w$, this was just the vector $x$, and for the bias term it was just the number $1$.  Notice that this is essentially what we wrote in the previous lesson.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "chubby-brother",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_z_delta_w(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "pressing-barrel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_z_delta_b():\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "black-union",
   "metadata": {},
   "source": [
    "But here, `delta_z_delta_w` returns the entire vector $x$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "thirty-coupon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_z_delta_w(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smooth-library",
   "metadata": {},
   "source": [
    "Now ultimately, we don't want how each of our parameters alter the linear function but how a change in the output of our linear function affects our activation function which affects our cost function.  This looks like the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manual-bruce",
   "metadata": {},
   "source": [
    "$\\nabla J(w_1,w_2, b) = \\begin{bmatrix}\n",
    "\\frac{\\delta J}{\\delta \\sigma} * \\frac{\\delta \\sigma}{\\delta z} * \\frac{\\delta z}{\\delta w_1} \\\\\n",
    "\\frac{\\delta J}{\\delta \\sigma} * \\frac{\\delta \\sigma}{\\delta z} * \\frac{\\delta z}{\\delta w_2} \\\\\n",
    "\\frac{\\delta J}{\\delta \\sigma} * \\frac{\\delta \\sigma}{\\delta z} * \\frac{\\delta z}{\\delta b}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "\\frac{\\delta J}{\\delta \\sigma} * \\frac{\\delta \\sigma}{\\delta z}*x_1\\\\\n",
    "\\frac{\\delta J}{\\delta \\sigma} * \\frac{\\delta \\sigma}{\\delta z}*x_2\\\\\n",
    "\\frac{\\delta J}{\\delta \\sigma} * \\frac{\\delta \\sigma}{\\delta z}* 1\n",
    "\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outstanding-warrior",
   "metadata": {},
   "source": [
    "And we calculated the components of how a change in the linear function affects the activation function, and how the activation function affects the cost function in the previous lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "powered-night",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deriv_sigma(z):\n",
    "    return torch.sigmoid(z)*(1 - torch.sigmoid(z))\n",
    "\n",
    "def delta_J_delta_sigma(y_hat, y):\n",
    "    return torch.sum(2*(y_hat - y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heavy-actor",
   "metadata": {},
   "source": [
    "So starting with our "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "personal-basics",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1., 2., 3.])\n",
    "y = torch.tensor(1.)\n",
    "\n",
    "w = torch.tensor([.5, .3, .1])\n",
    "\n",
    "b = torch.tensor(-1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "cooperative-argument",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_fn(w, x, b):\n",
    "    return x @ w + b \n",
    "\n",
    "def activation_fn(z):\n",
    "    return torch.sigmoid(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "functioning-yacht",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5987)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = linear_fn(w, x, b)\n",
    "\n",
    "y_hat = activation_fn(z)\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spare-maldives",
   "metadata": {},
   "source": [
    "And then we can see that we should update by the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "mounted-prototype",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1928, -0.3857, -0.5785], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_J_w = delta_z_delta_w(x)*deriv_sigma(z)*delta_J_delta_sigma(y_hat, y)\n",
    "grad_J_w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "printable-delaware",
   "metadata": {},
   "source": [
    "So this tells us how we should update each of our terms: $w_1$, $w_2$, and $w_3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "political-drain",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.1928, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_J_b = delta_z_delta_b()*deriv_sigma(z)*delta_J_delta_sigma(y_hat, y)\n",
    "grad_J_b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elect-kingston",
   "metadata": {},
   "source": [
    "### Checking with Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "induced-louisville",
   "metadata": {},
   "source": [
    "We can also get Pytorch to calculate this gradient with the `backward` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "sustained-position",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1., 2., 3.])\n",
    "y = torch.tensor(1.)\n",
    "\n",
    "w = torch.tensor([.5, .3, .1], requires_grad = True)\n",
    "\n",
    "b = torch.tensor(-1., requires_grad = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "detected-optimum",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = linear_fn(w, x, b)\n",
    "y_hat = activation_fn(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appropriate-origin",
   "metadata": {},
   "source": [
    "And if we pass through:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "dietary-disney",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = torch.sum((y - y_hat)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "effective-producer",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "collaborative-event",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1928, -0.3857, -0.5785])"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "stopped-script",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.1928)"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "young-lunch",
   "metadata": {},
   "source": [
    "So this is what occurs, we repeatedly calculate the gradient to see how to update each parameter.  If we call the `backward` function on the cost, like we did above, then Pytorch calculates how nudging our tensors affect the cost function.  From there, we just perform our gradient descent procedure like we did previously.  Repeatedly using the gradient to update our parameters.  Mathematically, we represent the set of our parameters as $\\theta$, and our gradient descent formula looks like the following: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "known-constitutional",
   "metadata": {},
   "source": [
    "$$ \\theta = \\theta - \\eta \\nabla \\theta $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporated-typing",
   "metadata": {},
   "source": [
    "This just means that repeatedly update each parameter by the negative partial derivative with respect to that parameter, like we saw above, multiplied by a learning rate.  Or in code we perform this with the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greenhouse-spanish",
   "metadata": {},
   "source": [
    "```python\n",
    "w = torch.tensor([.5, .3, .1])\n",
    "b = torch.tensor(-1.)\n",
    "\n",
    "eta = .00005\n",
    "\n",
    "for i in range(10):\n",
    "    # for each loop we first need to calculate the vals z and y_hat to pass into our derivative functions\n",
    "    z = linear_fn(w, x_vals, b)\n",
    "    y_hat = activation_fn(z)\n",
    "    \n",
    "    w = w - torch.mean(eta*delta_J_delta_sigma(y_hat, y_vals)*deriv_sigma(z)*delta_z_delta_w(x_vals))\n",
    "    b = b - torch.mean(eta*delta_J_delta_sigma(y_hat, x_vals)*deriv_sigma(z)*delta_z_delta_b())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sized-local",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corrected-radiation",
   "metadata": {},
   "source": [
    "In this lesson, we learned that to take the gradient, means to take the partial derivative with respect to each term in the function.  In calculating the gradient, we started by calculating the gradient of the linear layer.  That is, we calculated the amount our linear function's output would change, as we nudged each parameter:\n",
    "\n",
    "For our function: \n",
    "\n",
    "$z = w_1x_1 + w_2x_2 + b$, the gradient is:\n",
    "\n",
    "$\\nabla z(w_1,w_2, b) = \\begin{bmatrix}\n",
    "\\frac{\\delta z}{\\delta w_1} \\\\\n",
    "\\frac{\\delta z}{\\delta w_2} \\\\\n",
    "  \\frac{\\delta z}{\\delta b} \n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "  1\n",
    "\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dutch-antenna",
   "metadata": {},
   "source": [
    "So one way to represent the gradient is simply as the feature vector $x$ followed by a $1$ for the bias term."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bored-mortgage",
   "metadata": {},
   "source": [
    "And we saw this in Pytorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "natural-knock",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.tensor([.5, .3, .1], requires_grad = True)\n",
    "b = torch.tensor(-1., requires_grad = True)\n",
    "x = torch.tensor([1., 2., 3.])\n",
    "\n",
    "z = x @ w + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "sublime-watershed",
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "welsh-detail",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 2., 3.]), tensor(1.))"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.grad, b.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organizational-diana",
   "metadata": {},
   "source": [
    "And then to see how our cost function changes with a nudge in each parameter, then we still took the partial derivative with respect to each parameter, but this time also needed to apply the chain rule.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfied-compromise",
   "metadata": {},
   "source": [
    "$\\nabla J(w_1,w_2, b) = \\begin{bmatrix}\n",
    "\\frac{\\delta J}{\\delta \\sigma} * \\frac{\\delta \\sigma}{\\delta z} * \\frac{\\delta z}{\\delta w_1} \\\\\n",
    "\\frac{\\delta J}{\\delta \\sigma} * \\frac{\\delta \\sigma}{\\delta z} * \\frac{\\delta z}{\\delta w_2} \\\\\n",
    "\\frac{\\delta J}{\\delta \\sigma} * \\frac{\\delta \\sigma}{\\delta z} * \\frac{\\delta z}{\\delta b}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "\\frac{\\delta J}{\\delta \\sigma} * \\frac{\\delta \\sigma}{\\delta z}*x_1\\\\\n",
    "\\frac{\\delta J}{\\delta \\sigma} * \\frac{\\delta \\sigma}{\\delta z}*x_2\\\\\n",
    "\\frac{\\delta J}{\\delta \\sigma} * \\frac{\\delta \\sigma}{\\delta z}* 1\n",
    "\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "essential-ordinary",
   "metadata": {},
   "source": [
    "The derivatives $\\frac{\\delta J}{\\delta \\sigma}$ and $\\frac{\\delta \\sigma}{\\delta z}$ are exactly what we calculated in the previous lesson.  And so were able to calculate the gradient of our cost function by making use of the previously determined derivatives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "stopped-chuck",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1928, -0.3857, -0.5785], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_J_w = delta_z_delta_w(x)*deriv_sigma(z)*delta_J_delta_sigma(y_hat, y)\n",
    "grad_J_w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infinite-television",
   "metadata": {},
   "source": [
    "And again, we saw the same results when we checked our wrk in Pytorch, by this time calling `backward` on our cost, and then calculating the gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broadband-gamma",
   "metadata": {},
   "source": [
    "```python\n",
    "z = linear_fn(w, x, b)\n",
    "y_hat = activation_fn(z)\n",
    "cost = torch.sum((y - y_hat)**2)\n",
    "\n",
    "cost.backward()\n",
    "w.grad # tensor([-0.1928, -0.3857, -0.5785])\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
