{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "outstanding-liquid",
   "metadata": {},
   "source": [
    "# Understanding the Chain Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loose-shaft",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "treated-harris",
   "metadata": {},
   "source": [
    "Ok, so now that we understand a bit more about derivatives, it's time to go a bit deeper with gradient descent.  Remember that this is why we learned about derivatives.  Derivatives allow us to calculate the slope of our cost curve, or to put it another way, they allow us to calculate how our cost function changes as we change our parameters in our neural network.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acute-correlation",
   "metadata": {},
   "source": [
    "<img src=\"./cost-curve-slopes.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italic-netherlands",
   "metadata": {},
   "source": [
    "Now we generally define our cost function as $J(\\theta)$.  So our cost changes as we change our parameters $\\theta$.  So really when we have our formula of $next\\_w_1 = w_1 - learning\\_rate*slope\\_at(w_1)$ it's generally written as:\n",
    "\n",
    "* $next\\_w_1 = w_1 - learning\\_rate* \\frac{\\delta J}{\\delta w_1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "younger-bradley",
   "metadata": {},
   "source": [
    "Now, while we've seen some rules for calculating the derivative at a certain value, this one is different.  In the past, we were given a function like $f(x) = x^2$ and calculated that $\\frac{\\delta f}{\\delta x} = 2x$.  And the expression $\\frac{\\delta f}{\\delta x}$ means to calculate the how our function's output changes as we nudge the value of $x$.  \n",
    "\n",
    "This time we'll want to calculate how the cost function $J$ changes as we change the value of $w$ -- $\\frac{\\delta J}{\\delta w}$.  What makes this difficult is that our value $w$ is set way back in our linear layer, and it's impact isn't felt until further down in our cost function.  \n",
    "\n",
    "In this lesson, we'll see how neural networks solve this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confident-caution",
   "metadata": {},
   "source": [
    "### Seeing the Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brave-spank",
   "metadata": {},
   "source": [
    "Before seeing the solution, it help to first make sure we understand the issue presented to us.  We'll see the issue by once again using our cancer dataset.  Let's start by loading up our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "loved-praise",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_area</th>\n",
       "      <th>is_cancerous</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.326</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_area  is_cancerous\n",
       "0      1.001             1\n",
       "1      1.326             1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "df = pd.read_csv('./cell_data-2.csv', index_col = 0)\n",
    "df[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ready-latitude",
   "metadata": {},
   "source": [
    "So above, we can see that again want to use the mean area to predict if a cell is cancerous."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approved-albuquerque",
   "metadata": {},
   "source": [
    "> Now let's convert to Pytorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "multiple-snake",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(df[['mean_area']].values).float()\n",
    "y_actuals = torch.tensor(df['is_cancerous'].values).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frank-bennett",
   "metadata": {},
   "source": [
    "Ok, now with gradient descent, ultimately our goal is to figure out how nudging our parameter $w$ will change the output from our entire cost function.  And we get an output from the cost function by passing from one layer to the other until it ultimately calculates the cost.  Mathematically, this looks like the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "announced-guarantee",
   "metadata": {},
   "source": [
    "* $z(x_i) = w_1*x_i $\n",
    "* $a(z) =  \\frac{1}{1 + e^{-z(x)}} $\n",
    "* $ SSE = \\sum  (y\\_actual - y\\_hat)^2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solar-surge",
   "metadata": {},
   "source": [
    "And we can translate these layers into code with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "confirmed-gauge",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.tensor([[1.]], requires_grad = True) # we randomly assign \n",
    "\n",
    "def linear_layer(x):\n",
    "    z = x @ W\n",
    "    print('z = X * W = ', float(z[0]))\n",
    "    return z\n",
    "\n",
    "def activation_layer(z):\n",
    "    a = torch.sigmoid(z)\n",
    "    print('sigma(z) = ', float(a))\n",
    "    return a\n",
    "    \n",
    "def sse(y_actuals, y_hats):\n",
    "    squared_errors = (y_actuals - y_hats)**2\n",
    "    sum_squares = torch.sum(squared_errors)\n",
    "    print('sse = sum (y_actuals - y_hats)**2 = ', float(sum_squares))\n",
    "    return sum_squares"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "activated-likelihood",
   "metadata": {},
   "source": [
    "Ok, so let's see what what occurs if we pass through a single value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "presidential-acquisition",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = X[0] # 1.0010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "greater-separation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z = X * W =  1.0010000467300415\n",
      "sigma(z) =  0.7312551140785217\n",
      "sse = sum (y_actuals - y_hats)**2 =  0.07222381234169006\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.0722, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = linear_layer(x)\n",
    "y_hat = activation_layer(z)\n",
    "sse(y_actuals[0], y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "juvenile-accuracy",
   "metadata": {},
   "source": [
    "So above, we wrote out our functions so that we print out the output from each layer until we ultimately get cost, $.072$.  The point of printing these outputs is to better illustrate the problem.  With gradient descent, we want to see how a change in $W$, back in layer one, will ultimately change our cost in layer 3.  And this affect is indirect.  Changing a parameter in our linear layer, will change the output of the linear layer, which will then change the output of the activation layer, which then changes the SSE. \n",
    "\n",
    "We can of course see this by changing $W$ to from `1.0` to `2` just a little bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "instant-witch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z = X * W =  1.0110100507736206\n",
      "sigma(z) =  0.7332177758216858\n",
      "sse = sum (y_actuals - y_hats)**2 =  0.07117275148630142\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.0712, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# W = torch.tensor([[1.]], requires_grad = True) \n",
    "W = torch.tensor([[1.01]], requires_grad = True) \n",
    "\n",
    "z = linear_layer(x)\n",
    "y_hat = activation_layer(z)\n",
    "sse(y_actuals[0], y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "illegal-bacon",
   "metadata": {},
   "source": [
    "So again, there's a chain reaction that trickles down the line.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indie-liquid",
   "metadata": {},
   "source": [
    "### Starting the calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complicated-capture",
   "metadata": {},
   "source": [
    "Ok, so we ultimately want to calculate how changing $w$ will change our output of our cost function.  But to get there, really we need to tackle this a layer at a time.  This means that our first task is see how the output of our linear layer $z$ changes as we change our parameter value $w$.  So let's do this: \n",
    "\n",
    "* $z(w) = x*w $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defined-stretch",
   "metadata": {},
   "source": [
    "Now we represent the how $z$ changes as we change $w$ as the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "false-reset",
   "metadata": {},
   "source": [
    "$\\frac{\\delta z}{\\delta w}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legitimate-seeking",
   "metadata": {},
   "source": [
    "And this means that in calculating our derivative, $w$ is our changing variable, and we treat $x$ just like a number.  This gives us: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conservative-railway",
   "metadata": {},
   "source": [
    "$\\frac{\\delta z}{\\delta w} = x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "complicated-cliff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_z_delta_w(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lyric-colonial",
   "metadata": {},
   "source": [
    "That's it!  For every unit that we increase our weight, we increase the output by the value of $x$.  Not so bad.  Let's use the value from our first observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "protecting-entrepreneur",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_area</th>\n",
       "      <th>is_cancerous</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.326</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_area  is_cancerous\n",
       "0      1.001             1\n",
       "1      1.326             1"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./cell_data-2.csv', index_col = 0)\n",
    "df[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bizarre-kidney",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.001"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 1.001\n",
    "\n",
    "delta_z_delta_w(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appointed-houston",
   "metadata": {},
   "source": [
    "And we can even check this with what we saw when printing our outputs above: remember that we first passed through $w = 1.0$, and got an output of $z = X * W =  1.001$ and then we changed let $w =  1.01$ and got an output of $z = X * W =  1.011$.  And if we plug this into our derivative formula: \n",
    "\n",
    "$\\frac{\\delta f}{\\delta x} = \\frac{f(x_1) - f(x_0)}{x_1 - x_0}$ we get the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "underlying-advice",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0010000000000066"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_x0 = 1.00100\n",
    "f_x1 = 1.01101\n",
    "x0 = 1.0\n",
    "x1 = 1.01\n",
    "\n",
    "(f_x1 - f_x0)/(x1 - x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comic-feature",
   "metadata": {},
   "source": [
    "Looks pretty good."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sealed-thailand",
   "metadata": {},
   "source": [
    "### The next layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "derived-qatar",
   "metadata": {},
   "source": [
    "Now the next step is to calculate the derivative of the second layer $\\sigma(z)$.  Now the actual calculation is not so important -- we can essentially give to you.  What's more important is to see why we need to make this calculation.  The reason we need it is because again, if our goal is to see how nudging our parameter $w$ will ultimately alter our cost function, then now that we calculated how changing $w$ effects the output of $z$, because the output of $z$ then gets passed to our activation layer, the next step is to see how this change in $z$ will change the output of $\\sigma$.  Or to put it mathematically, we need to calculate $\\frac{\\delta \\sigma}{\\delta z}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genuine-gauge",
   "metadata": {},
   "source": [
    "So what is $\\frac{\\delta \\sigma}{\\delta z}?$  Well we'll just tell you.\n",
    "\n",
    "We saw before that $\\sigma(x) =  \\frac{1}{1 + e^{-x}} $, and it turns out that:\n",
    "\n",
    "$\\frac{\\delta \\sigma}{\\delta x} = \\sigma(x)*(1 - \\sigma(x))$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specialized-ukraine",
   "metadata": {},
   "source": [
    "And in code is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fabulous-hardwood",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def deriv_sigma(z):\n",
    "    return torch.sigma(z)*(1 - torch.sigma(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statewide-tender",
   "metadata": {},
   "source": [
    "> But again, it's not important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "representative-teacher",
   "metadata": {},
   "source": [
    "So now let's think back to our hypothesis function for our neuron. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "major-action",
   "metadata": {},
   "source": [
    "* $z(mean\\_area) = w_1*mean\\_area $\n",
    "* $\\sigma(z) =  \\frac{1}{1 + e^{-z(x)}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "homeless-character",
   "metadata": {},
   "source": [
    "We want to calculate the impact that nudging $w$ has, not just on our linear layer, but on our activation layer as well.  In math terms we want to find:\n",
    "\n",
    "$\\frac {\\delta \\sigma}{\\delta w}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "awful-ceremony",
   "metadata": {},
   "source": [
    "So to do that we first calculate the change that nudging $w$ has on $z$, and then multiply this by the impact that nudging $z$ has on $\\sigma$.  Or in other words:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passing-facial",
   "metadata": {},
   "source": [
    "$\\frac{\\delta \\sigma}{\\delta w} = \\frac{\\delta z}{\\delta w} * \\frac{\\delta \\sigma}{\\delta z}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collectible-century",
   "metadata": {},
   "source": [
    "Now what's nice is that we've already calculated both of these components, $\\frac{\\delta z}{\\delta w}$, and  $\\frac{\\delta \\sigma}{\\delta z}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virgin-block",
   "metadata": {},
   "source": [
    "$\\frac{\\delta z}{\\delta w} = x = 1.001$, and $\\frac{\\delta \\sigma}{\\delta z} = \\sigma(z)*(1 - \\sigma(z))$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "celtic-launch",
   "metadata": {},
   "source": [
    "Ok, now let's calculate $\\frac{\\delta \\sigma}{\\delta w} = \\frac{\\delta z}{\\delta w} * \\frac{\\delta \\sigma}{\\delta z}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "living-swiss",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1967)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "def deriv_sigma(z):\n",
    "    return torch.sigmoid(z)*(1 - torch.sigmoid(z))\n",
    "\n",
    "z = torch.tensor(1.001) # z = w*x = 1 * 1.001 \n",
    "delta_z_delta_w = 1.001\n",
    "\n",
    "delta_z_delta_w*deriv_sigma(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twelve-movement",
   "metadata": {},
   "source": [
    "And we can check our work by looking at the change in outputs that we printed out above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "empty-baker",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19626617431640608"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_0 = 1.0\n",
    "x_1 = 1.01\n",
    "\n",
    "a_1 = 0.7332177758216858\n",
    "a_0 = 0.7312551140785217\n",
    "# and then again use our derivative formula \n",
    "\n",
    "(a_1 - a_0)/(x_1 - x_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tight-liberal",
   "metadata": {},
   "source": [
    "Ok, so what we just saw is called the chain rule.  And the point is that we can calculate chain reaction that nudging an value like $w$  in our linear function $z$, will have on an upstream function like $\\sigma$ by multiplying the two component derivatives together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bulgarian-edwards",
   "metadata": {},
   "source": [
    "### A trick to check our work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "australian-sharing",
   "metadata": {},
   "source": [
    "Now ultimately, we want to see how nudging $w$ will impact our cost function $J$.  But first let's cover a couple of tricks about the chain rule.  The key to getting the chain rule right is just to frame it properly.  Above we want to see the impact that a change in $w$ has on our activation function $\\sigma$.  So far we wrote our hypothesis function like so:  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "declared-reaction",
   "metadata": {},
   "source": [
    "* $z(w_1, x) = w_1*x$\n",
    "* $\\sigma(z) =  \\frac{1}{1 + e^{-z(w1, x)}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technological-wednesday",
   "metadata": {},
   "source": [
    "And then to see the impact that the changing $w_1$ has down on the activation function $\\sigma$, or $\\frac{\\delta \\sigma}{\\delta w}$.  And then to calculate this, it's just the derivative of the first function times the derivative of the second."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standard-parallel",
   "metadata": {},
   "source": [
    "$\\frac{\\delta \\sigma}{\\delta w} = \\frac{\\delta z}{\\delta w} * \\frac{\\delta \\sigma}{\\delta z} = \\frac{\\delta \\sigma}{\\delta w}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunrise-opportunity",
   "metadata": {},
   "source": [
    "We can check our simply by cross multiplying.  So above, notice that the $\\delta z$'s cancel out, and what's remaining is $\\delta \\sigma$ on top and $\\delta w$ on the bottom.  Same as on the left:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reverse-enforcement",
   "metadata": {},
   "source": [
    "<img src=\"./practice.png\" width=\"20%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respected-contact",
   "metadata": {},
   "source": [
    "Ok, so we can see that everything looks good."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesser-medium",
   "metadata": {},
   "source": [
    "### Putting it together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tracked-culture",
   "metadata": {},
   "source": [
    "Ok so in our task to see what effect nudging our parameter $w$ will have on our cost function $J$, we next have to calculate the derivative of our cost function.  That is, we need to determine how a change in the output of our hypothesis function will change our cost function $ J(w) = \\frac{1}{n}\\sum_{i=0}^n (y_i - h(x_i))^2 = \\frac{1}{n}\\sum_{i=0}^n (y_i - \\sigma(x_i))^2 $.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mineral-auditor",
   "metadata": {},
   "source": [
    "If you want to calculate this derivative, we walk you through in the bonus section below.  But for now, just take our word for it:\n",
    "    \n",
    "$\\frac{\\delta J}{\\delta \\sigma} = \\frac{1}{n}\\sum_{i = 0}^n 2(\\sigma(x_i) - y_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chemical-shepherd",
   "metadata": {},
   "source": [
    "Which we can code as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "scientific-panic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def delta_J_delta_sigma(x_i, y_i):\n",
    "    return 2*(torch.sigmoid(x_i) - y_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "maritime-religion",
   "metadata": {},
   "source": [
    "So now, if our goal is to determine how the changing $w$ back in layer 1 effects our cost, we multiply the derivative from each layer together, that is:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "drawn-adelaide",
   "metadata": {},
   "source": [
    "$\\frac{\\delta J}{\\delta w} = \\frac{\\delta J}{\\delta \\sigma} * \\frac{\\delta \\sigma}{\\delta z} * \\frac{\\delta z}{\\delta w}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "environmental-custom",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.1057)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_i = torch.tensor(1.001) \n",
    "y_i = torch.tensor(1)\n",
    "\n",
    "# we get the values above from the data in our first observation\n",
    "\n",
    "delta_J_delta_sigma(x_i, y_i)*deriv_sigma(z)*delta_z_delta_w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welsh-watson",
   "metadata": {},
   "source": [
    "And if you want to see this written out in math, it looks like the line below (yes, a bit overwhelming)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contained-communist",
   "metadata": {},
   "source": [
    "$\\frac{\\delta J}{\\delta \\sigma} * \\frac{\\delta \\sigma}{\\delta z} * \\frac{\\delta z}{\\delta w} = \\frac{1}{n}\\sum_{i = 0}^n 2(\\sigma(x_i) - y_i) * \\sigma(z)*(1 - \\sigma(z)) * x$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comprehensive-relation",
   "metadata": {},
   "source": [
    "Ok, so now let's again check our calculation above, where we got `-0.1057` with the change in the cost function that we printed out above.  These are the numbers we saw above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "scientific-marker",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.10510608553886404"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_0 = 1.\n",
    "x_1 = 1.01\n",
    "\n",
    "y_0 = 0.07222381234169006\n",
    "y_1 = 0.07117275148630142\n",
    "\n",
    "# and then using our rate of change formula we get...\n",
    "\n",
    "(y_1 - y_0)/(x_1 - x_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "average-immunology",
   "metadata": {},
   "source": [
    "Ok, pretty damn close to our calculation of `-0.1057`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compound-resource",
   "metadata": {},
   "source": [
    "Ok, so this how we can perform gradient descent:\n",
    "\n",
    "$next\\_w_1 = w_1 - learning\\_rate* \\frac{\\delta J}{\\delta w_1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fancy-fundamentals",
   "metadata": {},
   "source": [
    "We just update our $w_1$ by the learning rate, and no matter how many layers there are to our neural network, we just apply the chain rule."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "received-grass",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electric-greenhouse",
   "metadata": {},
   "source": [
    "In this lesson, we saw we can use the chain rule to calculate how nudging our parameter $w_1$ will affect the cost function $J$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "above-fleece",
   "metadata": {},
   "source": [
    "Now the challenge of performing this calculation is that the effect is indirect.  Changing $w_1$ affects the output in the linear layer, which affects the output from our activation layer, which affects the output from our cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "incorporated-review",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z = X * W =  1.0010000467300415\n",
      "sigma(z) =  0.7312551140785217\n",
      "sse = sum (y_actuals - y_hats)**2 =  0.07222381234169006\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.0722, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = torch.tensor([[1.]], requires_grad = True) # we randomly assign \n",
    "\n",
    "z = linear_layer(x)\n",
    "y_hat = activation_layer(z)\n",
    "sse(y_actuals[0], y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identical-clerk",
   "metadata": {},
   "source": [
    "But we needed to perform this calculation to see how changing nudging our parameter would change our cost function in gradient descent:\n",
    "\n",
    "* $next\\_w_1 = w_1 - learning\\_rate* \\frac{\\delta J}{\\delta w_1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "healthy-deadline",
   "metadata": {},
   "source": [
    "To make this calculation, we used the chain rule.  With the chain rule, we calculate the derivative of each component and then multiply the derivatives together.  So to calculate the impact of nudging $w$ through the following layers: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "searching-dictionary",
   "metadata": {},
   "source": [
    "* $z(x_i) = w_1*x_i $\n",
    "* $\\sigma(z) =  \\frac{1}{1 + e^{-z(x)}} $\n",
    "* $ SSE = \\sum  (y\\_actual - y\\_hat)^2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confirmed-artwork",
   "metadata": {},
   "source": [
    "We need to calculate: $\\frac{\\delta J}{\\delta w} = \\frac{\\delta z}{\\delta w}*\\frac{\\delta \\sigma}{\\delta z}*\\frac{\\delta J}{\\delta \\sigma}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "korean-spencer",
   "metadata": {},
   "source": [
    "And the logic of this is that this represents that chain reaction we printed out above: we can track the impact of nudging $w$ on $J$, through calculating the change in $w$'s impact on $z$, and $z$'s impact on $\\sigma$ and $\\sigma$'s impact on $J$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspended-emission",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"https://www.jigsawlabs.io/free\" style=\"position: center\"><img src=\"jigsaw-icon.png\" width=\"10%\" style=\"text-align: center\"></a>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increased-newport",
   "metadata": {},
   "source": [
    "### Bonus -- Calculating the derivative of our cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modern-reader",
   "metadata": {},
   "source": [
    "Ok so in our task to see what effect nudging our parameter $w$ will have on our cost function $J$, we next have to calculate the derivative of our cost function.  That is, we need to determine how a change in the output of our hypothesis function will change our cost function $ J(w) = \\frac{1}{n}\\sum_{i=0}^n (y_i - h(x_i))^2 $.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "completed-ivory",
   "metadata": {},
   "source": [
    "Ok, we can still simplify this a bit further.  First, is to know that we can just ignore the $\\frac{1}{n}\\sum_{i = 0}^n$ when calculating the derivative.  So let's get rid of it, and then add it back in at the end.  So we'll define this function without the summation as $j$ and have:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "treated-siemens",
   "metadata": {},
   "source": [
    "$j(w) = (y_i - h(x_i))^2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enormous-master",
   "metadata": {},
   "source": [
    "Next is to use the chain rule again, by first defining an error function $\\epsilon$ where $\\epsilon_i = y_i - h(x_i)$ and then via substitution we can define $j$ to be $j = (y_i - h(x_i))^2  = \\epsilon_i^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharing-union",
   "metadata": {},
   "source": [
    "* $\\epsilon = y_i - \\sigma(x_i)$\n",
    "\n",
    "* $j(x) = \\epsilon^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opening-brush",
   "metadata": {},
   "source": [
    "So now we can calculate each of the derivatives again:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defined-dragon",
   "metadata": {},
   "source": [
    "* $\\frac{\\delta \\epsilon}{\\delta \\sigma} = 0 - \\sigma(x_i)^{1 - 1} = - \\sigma(x_i)^{0} = -1$\n",
    "\n",
    "* $\\frac{\\delta j}{\\delta \\epsilon} = 2 \\epsilon $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integrated-challenge",
   "metadata": {},
   "source": [
    "So we get that: $\\frac{\\delta j}{\\delta \\sigma} = \\frac{\\delta j}{\\delta \\epsilon} * \\frac{\\delta \\epsilon}{\\delta \\sigma} = 2*\\epsilon *-1 $ and because $\\epsilon = y_i - \\sigma(x_i)$ via substitution we get:\n",
    "\n",
    "* $\\frac{\\delta j}{\\delta \\sigma} = -2(y_i - \\sigma(x_i)) = 2(\\sigma(x_i) - y_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serial-reserve",
   "metadata": {},
   "source": [
    "And finally adding back in our summation and mean, we get:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preceding-rebel",
   "metadata": {},
   "source": [
    "$\\frac{\\delta J}{\\delta \\sigma} = \\frac{1}{n}\\sum_{i = 0}^n 2(\\sigma(x_i) - y_i)$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
