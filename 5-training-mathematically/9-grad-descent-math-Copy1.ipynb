{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "billion-operator",
   "metadata": {},
   "source": [
    "# Gradient Descent with Math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "australian-lease",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranking-penguin",
   "metadata": {},
   "source": [
    "Ok, so now that we understand a bit more about derivatives, it's time to go a bit deeper with gradient descent.  Remember that this is why we learned about derivatives.  Derivatives allow us to calculate the slope of our cost curve, or to put it another way, they allow us to calculate how our cost function changes as we change our parameters in our neural network.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupied-locking",
   "metadata": {},
   "source": [
    "<img src=\"./cost-curve-slopes.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unavailable-physiology",
   "metadata": {},
   "source": [
    "Now we generally define our cost function as $J(\\theta)$.  So our cost changes as we change our parameters $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "injured-customs",
   "metadata": {},
   "source": [
    "So really when we have our formula of $next\\_w_1 = w_1 - learning\\_rate*slope\\_at(w_1)$ it's generally written as:\n",
    "\n",
    "* $next\\_w_1 = w_1 - learning\\_rate* \\frac{\\delta J}{\\delta w_1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certain-porter",
   "metadata": {},
   "source": [
    "That is we update our weight $w_1$ proportional to the amount that our cost function $J$ changes as we nudge $w_1$.  \n",
    "\n",
    "In this lesson, we'll see how Pytorch applies the rules we saw for the derivative, and how we can use the derivative to descend a cost curve with multiple parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quiet-worth",
   "metadata": {},
   "source": [
    "### Back to Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dried-raise",
   "metadata": {},
   "source": [
    "Now let's make sense of what it means for Pytorch to use the derivative in gradient descent. Ok, now with gradient descent ultimately our goal is to figure out how nudging our parameter $w$ will change the output from our entire cost function -- but let's just start with how nudging $w$ will change the output from our linear layer.\n",
    "\n",
    "So let's go back to loading up our cancer cell data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "casual-blend",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "df = pd.read_csv('./cell_data-2.csv', index_col = 0)\n",
    "\n",
    "X = torch.tensor(df[['mean_area']].values).float()\n",
    "y = torch.tensor(df['is_cancerous'].values).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eight-native",
   "metadata": {},
   "source": [
    "And then defining the linear layer of our neuron with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "latter-invasion",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = X[0] # 1.0010\n",
    "W = torch.tensor([[1.]], requires_grad = True) \n",
    "\n",
    "# linear layer\n",
    "z = x @ W\n",
    "# activation layeer\n",
    "a = torch.sigmoid(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olympic-explorer",
   "metadata": {},
   "source": [
    "Ok, now let's think about how the output of our linear layer will change as we change $w$.  Mathematically, we can represent this problem as the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dressed-penetration",
   "metadata": {},
   "source": [
    "$l(w) = w* 1.0010$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "working-induction",
   "metadata": {},
   "source": [
    "> We call our linear function $l$, and notice that we define it as a function of $w$, as that is what we'll be changing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enabling-church",
   "metadata": {},
   "source": [
    "Then using our rules for the derivative can calculate the derivative as:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "political-closer",
   "metadata": {},
   "source": [
    "$l'(w) = 1*w^{0}*1.0010 = 1.0010$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "checked-slope",
   "metadata": {},
   "source": [
    "> So for every unit we increase $w$ by, we increase the output by $1.0010$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acceptable-rainbow",
   "metadata": {},
   "source": [
    "And this is what we can see this is what Pytorch calculates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "commercial-stuff",
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "external-coalition",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0010]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colored-arnold",
   "metadata": {},
   "source": [
    "Now as we said Pytorch doesn't *only* have the task of calculating how the output from the linear layer changes, but from the linear layer to the activation layer, to the cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "utility-romantic",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
