{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding what's next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in the last lesson, we saw the Pytorch code needed to train a neural network that can determine the labels of images of handwritten digits.  But what we focused on so far, was the *hypothesis function* of a neural network.  In other words, we focused in on the construction of our different layers, and saw how we could pass through features of an observation to ultimately get a prediction.  \n",
    "\n",
    "But what we still have yet to learn some of the crucial ways that a neural network learns the parameter values -- the numbers in our linear layers -- that will predict our neural network.  In other words, we still have more to learn about what occurs in training a neural network.  \n",
    "\n",
    "Now we have learned a bit about training a neural network, but it's a bit more complicated than we've seen so far.  In this lesson, we'll review what we've learned so far about how a neural network trains through gradient descent, and then we'll see what makes gradient descent so challenging when working with a neural network with many parameters, and many linear layers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now so far we have learned how to train a single neuron that has a single parameter.  Let's jog our memory by returning to our example of trying to predict if a cell is cancerous.  We can begin by loading up our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_area</th>\n",
       "      <th>is_cancerous</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.326</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.203</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_area  is_cancerous\n",
       "0      1.001             1\n",
       "1      1.326             1\n",
       "2      1.203             1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "url = 'https://raw.githubusercontent.com/jigsawlabs-student/pytorch-intro-curriculum/main/5-training-mathematically/cell_data-2.csv'\n",
    "df = pd.read_csv(url, index_col = 0)\n",
    "df[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then convert this into our Pytorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0010],\n",
       "        [1.3260]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "mean_area = df[['mean_area']].values # select just the mean area data\n",
    "\n",
    "X_tensor = torch.tensor(mean_area).float() # convert to tensors\n",
    "X_tensor[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([569, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then let's do the same for our target data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tensor = torch.tensor(df['is_cancerous']).float()\n",
    "y_tensor[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reviewing the hypothesis function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we'll want to to feed this data into a neuron that can predict whether each slide has cells that are cancerous or not.  How do we do this?  Well we'll start with our linear function and feed the outputs from this linear function into an activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $z(mean\\_area) = w_1*mean\\_area $\n",
    "* $a(z) =  \\frac{1}{1 + e^{-z(x)}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So above, we pass multiply the mean area by a weight, $w_1$, and pass this output to our sigmoid function that gives us a value between 0 and 1.  The closer the output is to one, the more confident the prediction of cancer.  So now, we can define our neuron with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.tensor([[1.]], requires_grad = True)\n",
    "\n",
    "def linear_layer(X):\n",
    "    z = X @ W\n",
    "    return z\n",
    "    \n",
    "def activation_layer(z):\n",
    "    a = torch.sigmoid(z)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in the linear_layer above, we created a feature vector `W` that has a single neuron (one column) with a single weight (one row) whose value we randomly set to just be the number `1`.  After initially setting this value, we'll need toÂ learn the value that will produce predictions that best predict the labels in the training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before doing this training let's remember how this works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We can select a single observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0010]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_obs = X_tensor[:1]\n",
    "\n",
    "first_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Pass it through the linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0010]], grad_fn=<MmBackward>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = linear_layer(first_obs) # z = X @ w\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then pass this output through the activation layer to get a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7313]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation_layer(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which we can then compare with the actual value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tensor[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so now let's review how we perform training. Above we made our prediction with a random parameter value of $W = [[1]]$.  But our next step is to see if parameter produces predictions that are anywhere close to the actual labels.  And then from there we update the parameter value.\n",
    "\n",
    "So our next step is to check our predictions against the actual labels in the training data.  Our predictions look like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7313],\n",
       "        [0.7902]], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hats.shape # torch.Size([569, 1])\n",
    "\n",
    "y_hats[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But our actual labels takes on a different shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1.])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tensor.shape # torch.Size([569])\n",
    "\n",
    "y_tensor[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's transform the data so that it takes the same shape, and then we can perform the subtraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [1.]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_actual = y_tensor.view(-1, 1)\n",
    "\n",
    "y_actual.shape # torch.Size([569, 1])\n",
    "\n",
    "y_actual[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so now what's pretty cool, is that we can perform the calculate $e_i = (y\\_actual - y\\_hat)$ to get a vector of errors at each position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2687],\n",
       "        [0.2098]], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors = (y_actual - y_hats)\n",
    "\n",
    "errors[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we square each of these errors, and take the sum: $SSE = \\sum  (y\\_actual - y\\_hat)^2 $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "squared_errors = (y_actual - y_hats)**2\n",
    "\n",
    "sse = torch.sum(squared_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(152.0503, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> So this is how we can calculate how well initial parameter value of $w_1 = 1$ performs against the entire training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert this to a mean squared error, we can simply divide by the number of observations like so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2672, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squared_errors = (y_actual - y_hats)**2\n",
    "n = len(y_actual) # 569 observations\n",
    "\n",
    "mse = torch.sum(squared_errors)/n\n",
    "\n",
    "mse # mse = sse/n => average squared error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding some Math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we would probably like to keep moving through this review, but things will soon become difficult for us, if we do not get some math notation out of the way.  Let's go through some notation for better expressing our hypothesis function and our cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The hypothesis function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so we just defined our hypothesis function as the linear layer followed by our activation layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7313], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = X_tensor[0]\n",
    "\n",
    "z = linear_layer(x)\n",
    "activation_layer(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if we want we can package both of these functions up into a hypothesis function `h(x)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h(x):\n",
    "    z = linear_layer(x)\n",
    "    return activation_layer(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7313], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function $h(x)$ is how we represent our function mathematically as well.  And if we want, we can express our hypothesis function as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $h(x) = \\sigma(z(x))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the function above from the inner, out.  So we pass $x$ to our linear function $z$, and we pass this output to our activation function $\\sigma$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, to make this even more explicit, we may write this add a little letter $i$ underneath the $x$:\n",
    "\n",
    "$h(x_i) = \\sigma(z(x_i))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This signifies that we are passing through a single observation $x$ into our hypothesis function, and thus are getting a single output.  This is as opposed to passing through all of the observations at once, to get back a list of outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7313],\n",
       "        [0.7902],\n",
       "        [0.7691],\n",
       "        [0.5953],\n",
       "        [0.7853],\n",
       "        [0.6171],\n",
       "        [0.7388],\n",
       "        [0.6406],\n",
       "        [0.6271],\n",
       "        [0.6168]], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h(X_tensor)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> See the difference?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To specify that we are just passing through one input, we add our $x_i$: $h(x_i) = \\sigma(z(x_i))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we previously expressed our cost function with something like the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$SSE = \\sum  (y - \\hat{y})^2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $\\hat{y}$ was our predicted value, and $y$ was our actual value.  But if you think about it, our predicted value is the value that is outputted from our hypothesis function: $h(x_i)$, so we can rewrite that our squared error above to be:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $squared\\_error = (y_i - h(x_i))^2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we are indicating that the squared error of an individual observation is an actual value $y_i$ minus the output from our hypothesis function squared.\n",
    "\n",
    "And to then indicate we are adding the squared error for each individual observation, we add back in the summation function: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$SSE = \\sum_{i = 1}^n  (y_i - h(x_i))^2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time under the summation function we added an $i = 1$ on the bottom and an $n$ on top.  This is because in math, like programming there is the concept of an index for a list of elements.  So here, it is saying from the first element (we begin at index 1 in math) to the last element $n$, add up the squared error of each.  Finally, for mean squared error, we divide by the number of elements to get the average squared error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$MSE = \\frac{1}{n}\\sum_{i = 1}^n  (y_i - h(x_i))^2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more thing before we move on.  We saw that our hypothesis function is given the notation $h(x)$.  We'll use $J(x)$ to indicate our cost function.  So now we have:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$J(x) = \\frac{1}{n}\\sum_{i = 1}^n  (y_i - h(x_i))^2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> And notice that the $x$ in J(x) does not have a little $i$ underneath.  This is to indicate that the *total cost* for our set of observations is the summation of all of the individual squared errors, divided by the number of observations.\n",
    "\n",
    "Maybe read that sentence above twice so that it sinks in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so the math review is over."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now remember what comes next.  We want to find the parameter value for $w_1$ that minimizes the cost curve.  Now we could try every value, but instead it's more efficient to use the slope of the cost curve to update our parameter value such that it will move towards the minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"cost-curve-slopes.png\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we started with our random value for $w_1$, and then our approach will be calculate the slope of the cost curve for that value of $w_1$.  And we'll descend along the cost curve by taking the negative value of the slope, and multiplying it by a learning rate:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$next\\_w_1 = w_1 - learning\\_rate*slope\\_at(w_1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, more generally we write this as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\theta_{next} = \\theta_{current} -\\eta*slope\\_at(\\theta_{current})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $\\theta$ represents a parameter, and $\\eta$ is the learning rate.  This process is called gradient descent.  And it's called gradient descent because, that *slope* is referred to as the **gradient**, and we use the gradient to descend along our cost curve. \n",
    "\n",
    "In summary, with gradient descent we take the current parameter value, find the slope of the cost curve at that value, and update our parameter by the negative slope multiplied by a learning rate.  Then we repeat.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember this is what we saw above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "for (x, y) in zip(X_train_tensor_gpu, y_train_tensor_gpu): # loop through observations and labels\n",
    "    net.zero_grad() # explained below\n",
    "    y_hat = net(X_tensor)             # 1. start with current weights in our neural net and make a prediction\n",
    "    loss = criterion(y_hat, y_tensor) # 2. See how off the prediction is according to the cost function\n",
    "    loss.backward()                   # 3a. Calculate the slope of the cost curve at that weight\n",
    "    opt.step()                        # 4. Update the parameters based on learning rate and the calculated slope \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Behind the scenes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the key component of neural networks which we still need to understand is what occurs when we call `loss.backward()`.  What occurs is when we call `loss.backward()`, Pytorch calculates the slopes we see in our cost curve picture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"cost-curve-slopes.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, it calculates how nudging the current parameter value in $w_1$ will alter our cost curve.  We can see this if we look at `w.grad`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Press shift + return, and **nothing happens**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "W.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So **nothing happens** when we call `w.grad`.  But if we first call `sse.backward()` and this will trigger Pytorch calculating this slope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "sse.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So notice that this time Pytorch has calculated the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[27.4926]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our procedure would be to repeatedly update this parameter value by the negative gradient (ie slope of the cost curve) multiplied by a learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\theta_{next} = \\theta_{current} -\\eta*gradient\\_at(\\theta_{current})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "w = w - .01 * w.grad\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now see if that training loop makes even more sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "for (x, y) in zip(X_train_tensor_gpu, y_train_tensor_gpu): # loop through observations and labels\n",
    "    net.zero_grad() # explained later\n",
    "    y_hat = net(X_tensor)             # 1. start with current weights in our neural net and make a prediction\n",
    "    loss = criterion(y_hat, y_tensor) # 2. See how off the prediction is according to the cost function\n",
    "    loss.backward()                   # 3a. Calculate the slope of the cost curve at that weight\n",
    "    opt.step()                        # 4. Update the parameters based on learning rate and the calculated slope \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `zero_grad` line simply clears the calculated gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A moment of appreciation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we saw what that `sse.backward()` call does.  It calculates how the cost curve changes as we change our parameter $w_1$.  This may seem pretty simple, but there's a lot behind it.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. There's a lot of parameters\n",
    "\n",
    "First, while in this example we performed the operation by calculating how changing *a single parameter* changes the cost curve -- with neural networks we need to calculate how each of our thousands of parameters in change the cost curve.  \n",
    "\n",
    "But it's not just the number of parameters that makes difficult.  \n",
    "\n",
    "2. There are multiple layers\n",
    "\n",
    "Let's take another look at what we did above above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.tensor([[1.]], requires_grad = True)\n",
    "\n",
    "z = X_tensor @ w # layer 1\n",
    "a = torch.sigmoid(z) # layer 2\n",
    "\n",
    "mse = torch.sum((y_tensor.view(-1, 1) - a)**2)/len(y_tensor) # layer 3, total cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we then called `sse.backward()`, our neural network had to calculate the effect of changing $w$ back in layer 1, on the sum of squared errors down in layer 3.  So to calculate the effect of changing a parameter in layer 1, we need to consider how $w$ affects the output of the linear layer, which changes the output in the activation layer, which then changes the output of the cost function.  Changing this parameter back in layer 1, causes a chain reaction, and Pytorch needs to account for it.\n",
    "\n",
    "So this is what we'll need to cover in the coming lessons.  First, how neural descend a cost curve when there is more than one parameter.  And secondly, how neural networks calulates the indirect affect that changing a parameter has on the cost curve through multiple layers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we reviewed the key ideas behind training the parameters for a single neuron.  We first defined our hypothesis function as the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $z(mean\\_area) = w_1*mean\\_area $\n",
    "* $a(z) =  \\frac{1}{1 + e^{-z(x)}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And coded it like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_layer(X):\n",
    "    W = torch.tensor([[1.]], requires_grad = True)\n",
    "    z = X @ w\n",
    "    return z\n",
    "    \n",
    "def activation_layer(z):\n",
    "    a = torch.sigmoid(z)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From there, we moved to training our hypothesis function.  And saw that we do this with gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, in Pytorch, as the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "for (x, y) in zip(X_train_tensor_gpu, y_train_tensor_gpu): # loop through observations and labels\n",
    "    net.zero_grad() # explained later\n",
    "    y_hat = net(X_tensor)             # 1. start with current weights in our neural net and make a prediction\n",
    "    loss = criterion(y_hat, y_tensor) # 2. See how off the prediction is according to the cost function\n",
    "    loss.backward()                   # 3a. Calculate the slope of the cost curve at that weight\n",
    "    opt.step()                        # 4. Update the parameters based on learning rate and the calculated slope \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then saw that the `loss.backward()` step calculates the gradient -- that is the slope of our cost curve at the current value of our parameter.  And this is what we use to guide how we repeatedly update our parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\theta_{next} = \\theta_{current} -\\eta*slope\\_at(\\theta_{current})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we discussed what's next.  First, we need to learn how gradient descent works when we move beyond a single parameter.  And second, we need to see how we can calculate the effect of changing a parameter on multiple layers of our neural network, until it ultimately affects our cost function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
