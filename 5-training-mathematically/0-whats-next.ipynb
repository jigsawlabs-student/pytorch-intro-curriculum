{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding what's next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in the last lesson, we saw the Pytorch code needed to train a neural network that can determine the labels of images of handwritten digits.  But what we focused on so far, was the *hypothesis function* of a neural network.  In other words, we focused in on the construction of our different layers, and saw how we could pass through features of an observation to ultimately get a prediction.  \n",
    "\n",
    "But what we still have yet to learn some of the crucial ways that a neural network learns the parameter values -- the numbers in our linear layers -- that will predict our neural network.  In other words, we still have more to learn about what occurs in training a neural network.  \n",
    "\n",
    "Now we have learned a bit about training a neural network, but it's a bit more complicated than we've seen so far.  In this lesson, we'll review what we've learned so far about how a neural network trains through gradient descent, and then we'll see what makes gradient descent so challenging when working with a neural network with many parameters, and many linear layers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now so far we have learned how to train a single neuron that has a single parameter.  Let's jog our memory by returning to our example of trying to predict if a cell is cancerous.  We can begin by loading up our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_area</th>\n",
       "      <th>is_cancerous</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.326</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.203</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_area  is_cancerous\n",
       "0      1.001             1\n",
       "1      1.326             1\n",
       "2      1.203             1"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "url = './cell_data-2.csv'\n",
    "df = pd.read_csv(url, index_col = 0)\n",
    "df[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then convert this into our Pytorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0010],\n",
       "        [1.3260]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "mean_area = df[['mean_area']].values # select just the mean area data\n",
    "\n",
    "X_tensor = torch.tensor(mean_area).float() # convert to tensors\n",
    "X_tensor[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([569, 1])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then let's do the same for our target data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1.])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tensor = torch.tensor(df['is_cancerous']).float()\n",
    "y_tensor[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reviewing the hypothesis function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we'll want to to feed this data into a neuron that can predict whether each slide has cells that are cancerous or not.  How do we do this?  Well we'll start with our linear function and feed the outputs from this linear function into an activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $z(mean\\_area) = w_1*mean\\_area $\n",
    "* $a(z) =  \\frac{1}{1 + e^{-z(x)}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So above, we pass multiply the mean area by a weight, $w_1$, and pass this output to our sigmoid function that gives us a value between 0 and 1.  The closer the output is to one, the more confident the prediction of cancer.  So now, we can define our neuron with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_layer(X):\n",
    "    W = torch.tensor([[1.]], requires_grad = True)\n",
    "    z = X @ w\n",
    "    return z\n",
    "    \n",
    "def activation_layer(z):\n",
    "    a = torch.sigmoid(z)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in the linear_layer above, we created a feature vector `W` that has a single neuron (one column) with a single weight (one row) whose value we randomly set to just be the number `1`.  After initially setting this value, we'll need toÂ learn the value that will produce predictions that best predict the labels in the training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before doing this training let's remember how this works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We can select a single observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0010]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_obs = X_tensor[:1]\n",
    "\n",
    "first_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Pass it through the linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0010]], grad_fn=<MmBackward>)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = linear_layer(first_obs) # z = X @ w\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then pass this output through the activation layer to get a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7313]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation_layer(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which we can then compare with the actual value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tensor[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now one thing to notice is that we can also pass through multiple observations and the matrix algebra will work out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$z = x_{569x1} \\cdot W_{1x1}$\n",
    "\n",
    "$a = \\sigma(z)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So remembering our matrix algebra, in our linear layer, the inner dimensions align, and looking at the outer dimensions, we can see that we'll get one output for each observation (569 in all), and these to our sigmoid function will then give us 569 predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = linear_layer(X_tensor) # z = X @ w\n",
    "y_hats = activation_layer(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([569, 1])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7313],\n",
       "        [0.7902]], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hats[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so now to keep this review going, remember that we made this prediction with a random parameter value of $W = [[1]]$.  But our next step is to see if parameter produces predictions that are anywhere close to the actual labels.  And then from there we update the parameter value.\n",
    "\n",
    "So our next step is to check our predictions against the actual labels in the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our predictions look like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7313],\n",
       "        [0.7902]], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hats.shape # torch.Size([569, 1])\n",
    "\n",
    "y_hats[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But our actual labels takes on a different shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1.])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tensor.shape # torch.Size([569])\n",
    "\n",
    "y_tensor[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's transform the data so that it takes the same shape, and then we can perform the subtraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [1.]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_actual = y_tensor.view(-1, 1)\n",
    "\n",
    "y_actual.shape # torch.Size([569, 1])\n",
    "\n",
    "y_actual[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so now what's pretty cool, is that we can perform the calculate $e_i = (y\\_actual - y\\_hat)$ to get a vector of errors at each position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2687],\n",
       "        [0.2098]], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors = (y_actual - y_hats)\n",
    "\n",
    "errors[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we square each of these errors, and take the sum: $SSE = \\sum  (y\\_actual - y\\_hat)^2 $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "squared_errors = (y_actual - y_hats)**2\n",
    "\n",
    "sse = torch.sum(squared_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(152.0503, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> So this is how we can calculate how well initial parameter value of $w_1 = 1$ performs against the entire training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now remember what comes next.  We want to find the parameter value for $w_1$ that minimizes the cost curve.  Now we could try every value, but instead it's more efficient to use the slope of the cost curve to update our parameter value such that it will move towards the minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"cost-curve-slopes.png\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we started with our random value for $w_1$, and then our approach will be calculate the slope of the cost curve for that value of $w_1$.  And we'll descend along the cost curve by taking the negative value of the slope, and multiplying it by a learning rate:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$next\\_w_1 = w_1 - learning\\_rate*slope\\_at(w_1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, more generally we write this as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\theta_{next} = \\theta_{current} -\\eta*slope\\_at(\\theta_{current})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $\\theta$ represents a parameter, and $\\eta$ is the learning rate.  This process is called gradient descent.  And it's called gradient descent because, that *slope* is referred to as the **gradient**, and we use the gradient to descend along our cost curve. \n",
    "\n",
    "In summary, with gradient descent we take the current parameter value, find the slope of the cost curve at that value, and update our parameter by the negative slope multiplied by a learning rate.  Then we repeat.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember this is what we saw above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "for (x, y) in zip(X_train_tensor_gpu, y_train_tensor_gpu): # loop through observations and labels\n",
    "    net.zero_grad() # explained below\n",
    "    y_hat = net(X_tensor)             # 1. start with current weights in our neural net and make a prediction\n",
    "    loss = criterion(y_hat, y_tensor) # 2. See how off the prediction is according to the cost function\n",
    "    loss.backward()                   # 3a. Calculate the slope of the cost curve at that weight\n",
    "    opt.step()                        # 4. Update the parameters based on learning rate and the calculated slope \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Behind the scenes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the key component of neural networks which we still need to understand is what occurs when we call `loss.backward()`.  What occurs is when we call `loss.backward()`, Pytorch calculates the slopes we see in our cost curve picture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"cost-curve-slopes.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, it calculates how nudging the current parameter value in $w_1$ will alter our cost curve.  We can see this if we look at `w.grad`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Press shift + return, and **nothing happens**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But first call `sse.backward()` and this will trigger Pytorch calculating this slope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "sse.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So notice that this time Pytorch has calculated the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[27.4926]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our procedure would be to repeatedly update this parameter value by the negative gradient (ie slope of the cost curve) multiplied by a learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\theta_{next} = \\theta_{current} -\\eta*gradient\\_at(\\theta_{current})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "w = w - .01 * w.grad\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now see if that training loop makes even more sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "for (x, y) in zip(X_train_tensor_gpu, y_train_tensor_gpu): # loop through observations and labels\n",
    "    net.zero_grad() # explained later\n",
    "    y_hat = net(X_tensor)             # 1. start with current weights in our neural net and make a prediction\n",
    "    loss = criterion(y_hat, y_tensor) # 2. See how off the prediction is according to the cost function\n",
    "    loss.backward()                   # 3a. Calculate the slope of the cost curve at that weight\n",
    "    opt.step()                        # 4. Update the parameters based on learning rate and the calculated slope \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `zero_grad` line simply clears the calculated gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A moment of appreciation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we saw what that `sse.backward()` caall does.  It calculates how the cost curve changes as we change our parameter $w_1$.  This may seem pretty simple, but there's a lot behind it.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. There's a lot of parameters\n",
    "\n",
    "First, while in this example we performed the operation by calculating how changing *a single parameter* changes the cost curve -- with neural networks we need to calculate how each of our thousands of parameters in change the cost curve.  \n",
    "\n",
    "But it's not just the number of parameters that makes difficult.  \n",
    "\n",
    "2. There are multiple layers\n",
    "\n",
    "Let's take another look at what we did above above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.tensor([[1.]], requires_grad = True)\n",
    "\n",
    "z = X_tensor @ w # layer 1\n",
    "a = torch.sigmoid(z) # layer 2\n",
    "\n",
    "sse = torch.sum((y_tensor.view(-1, 1) - a)**2) # layer 3, total cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we then called `sse.backward()`, our neural network had to calculate the effect of changing $w$ back in layer 1, on the sum of squared errors down in layer 3.  So to calculate the effect of changing a parameter in layer 1, we need to consider how $w$ affects the output of the linear layer, which changes the output in the activation layer, which then changes the output of the cost function.  Changing this parameter back in layer 1, causes a chain reaction, and Pytorch needs to account for it.\n",
    "\n",
    "So this is what we'll need to cover in the coming lessons.  First, how neural descend a cost curve when there is more than one parameter.  And secondly, how neural networks calulates the indirect affect that changing a parameter has on the cost curve through multiple layers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we reviewed the key ideas behind training the parameters for a single neuron.  We first defined our hypothesis function as the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $z(mean\\_area) = w_1*mean\\_area $\n",
    "* $a(z) =  \\frac{1}{1 + e^{-z(x)}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And coded it like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_layer(X):\n",
    "    W = torch.tensor([[1.]], requires_grad = True)\n",
    "    z = X @ w\n",
    "    return z\n",
    "    \n",
    "def activation_layer(z):\n",
    "    a = torch.sigmoid(z)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From there, we moved to training our hypothesis function.  And saw that we do this with gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, in Pytorch, as the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "for (x, y) in zip(X_train_tensor_gpu, y_train_tensor_gpu): # loop through observations and labels\n",
    "    net.zero_grad() # explained later\n",
    "    y_hat = net(X_tensor)             # 1. start with current weights in our neural net and make a prediction\n",
    "    loss = criterion(y_hat, y_tensor) # 2. See how off the prediction is according to the cost function\n",
    "    loss.backward()                   # 3a. Calculate the slope of the cost curve at that weight\n",
    "    opt.step()                        # 4. Update the parameters based on learning rate and the calculated slope \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then saw that the `loss.backward()` step calculates the gradient -- that is the slope of our cost curve at the current value of our parameter.  And this is what we use to guide how we repeatedly update our parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\theta_{next} = \\theta_{current} -\\eta*slope\\_at(\\theta_{current})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we discussed what's next.  First, we need to learn how gradient descent works when we move beyond a single parameter.  And second, we need to see how we can calculate the effect of changing a parameter on multiple layers of our neural network, until it ultimately affects our cost function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
