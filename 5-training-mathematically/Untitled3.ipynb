{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "complicated-strap",
   "metadata": {},
   "source": [
    "* Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raising-chemical",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atomic-appliance",
   "metadata": {},
   "source": [
    "So in the last lesson we learned about both forward propagation and backward propagation.  With forward propagation we saw the output from our neural network at each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "environmental-newark",
   "metadata": {},
   "source": [
    "* $z(x_i) = w_1*x_i $\n",
    "* $a(z) =  \\frac{1}{1 + e^{-z(x)}} $\n",
    "* $ J(\\hat{y}, y) = \\sum  (y - \\hat{y})^2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sticky-parish",
   "metadata": {},
   "source": [
    "And with backward propagation, we moved backward through each of our layers, each time calculating how a change in the layer's output affected the cost function.  Remember that with backpropagation, we perform this calculatin by multiplying the local derivative by the upstream derivative we calculated in the previous layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funded-dealer",
   "metadata": {},
   "source": [
    "* $\\frac{\\delta J}{\\delta \\sigma}$\n",
    "\n",
    "* $\\frac{\\delta J}{\\delta z} = \\frac{\\delta \\sigma}{\\delta z}*\\frac{\\delta J}{\\delta \\sigma}$\n",
    "\n",
    "* $\\frac{\\delta J}{\\delta w} = \\frac{\\delta w}{\\delta z}*\\frac{\\delta J}{\\delta z}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "great-basic",
   "metadata": {},
   "source": [
    "But in the last lesson, we did this with only a single parameter in our neuron.  And we updated our neural network only one observation at a time.  In this lesson, we'll see how we can use matrix algebra to build more complex neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "another-question",
   "metadata": {},
   "source": [
    "### Getting Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graduate-combine",
   "metadata": {},
   "source": [
    "Now we'll need the linear and activation layers to make a prediction.  And we'll need the derivatives we calculated in the previous lessons to update the parameters of our hypothesis function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manual-pottery",
   "metadata": {},
   "source": [
    "Remember that we'll need the components needed to calculate $\\frac{\\delta J}{\\delta w}$ and $\\frac{\\delta J}{\\delta b}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporated-fraction",
   "metadata": {},
   "source": [
    "* $\\frac{\\delta J}{\\delta w} = \\frac{\\delta J}{\\delta \\sigma} * \\frac{\\delta \\sigma}{\\delta z} * \\frac{\\delta z}{\\delta w}$\n",
    "\n",
    "* $\\frac{\\delta J}{\\delta b} = \\frac{\\delta J}{\\delta \\sigma} * \\frac{\\delta \\sigma}{\\delta z} * \\frac{\\delta z}{\\delta b}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blond-boulder",
   "metadata": {},
   "source": [
    "And as we calculated in previous lessons, these component derivatives are the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "published-bargain",
   "metadata": {},
   "source": [
    "### Understanding the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unauthorized-wellington",
   "metadata": {},
   "source": [
    "### Multiple Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "material-interest",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_area</th>\n",
       "      <th>mean_concavity</th>\n",
       "      <th>is_cancerous</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.001</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.326</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_area  mean_concavity  is_cancerous\n",
       "0      1.001          0.3001             0\n",
       "1      1.326          0.0869             0"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./cell_data.csv')\n",
    "df[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "residential-trade",
   "metadata": {},
   "source": [
    "And we convert our data into tensors like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "executive-variance",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "X_tensor = torch.tensor(df[['mean_area', 'mean_concavity']].values).float()\n",
    "y_tensor = torch.tensor((df['is_cancerous'] == 0).values).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separate-birthday",
   "metadata": {},
   "source": [
    "Now when using multiple features, really our same procedure of forward and backward propagation hold.  We start with forward propagation, where we pass our data through multiple layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "fuzzy-dimension",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0010, 0.3001])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_x = X_tensor[0] \n",
    "first_y = y_tensor[0]\n",
    "\n",
    "\n",
    "first_y # tensor(1.)\n",
    "first_x # tensor([1.0010, 0.3001])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "temporal-denial",
   "metadata": {},
   "source": [
    "And then we'll define our hypothesis function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "considered-winning",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_fn(w, x, b):\n",
    "    return x @ w + b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "compound-surgeon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_fn(z):\n",
    "    return torch.sigmoid(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operating-chess",
   "metadata": {},
   "source": [
    "And then we perform forward propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "tested-basic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6022)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.tensor([2., 2.])\n",
    "b = torch.tensor(-2.)\n",
    "\n",
    "z = linear_fn(w, first_x, b)\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enormous-vietnamese",
   "metadata": {},
   "source": [
    "> So, in the linear layer above, we perform multiplication of $x_{1x2}$ and $w_{2x1}$ giving us a single output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demanding-freedom",
   "metadata": {},
   "source": [
    "And from there, we pass this output through our activation function, where we stay with an individual output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "black-garlic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6462)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = activation_fn(z)\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "activated-mongolia",
   "metadata": {},
   "source": [
    "### Backward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monthly-trinity",
   "metadata": {},
   "source": [
    "Now what's more interesting is backward propagation.  It turns out that all of our same formulas hold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "irish-listing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def delta_J_delta_sigma(y_hat, y):\n",
    "    return torch.sum(2*(y_hat - y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "signal-license",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_sigma_delta(z):\n",
    "    return torch.sigmoid(z)*(1 - torch.sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "graphic-investing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_z_delta_w(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "veterinary-demographic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_z_delta_b():\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "present-facility",
   "metadata": {},
   "source": [
    "So really, even though we are using a multiparameter hypothesis function we can use the same rules as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "automatic-manufacturer",
   "metadata": {},
   "outputs": [],
   "source": [
    "dj_dsig = delta_J_delta_sigma(y_hat, first_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "unsigned-intersection",
   "metadata": {},
   "outputs": [],
   "source": [
    "dz_dJ = delta_sigma_delta(z)*dj_dsig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "occupied-twist",
   "metadata": {},
   "outputs": [],
   "source": [
    "dw_dJ = delta_z_delta_w(first_x)*dz_dJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "sacred-offering",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1620, -0.0486])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dw_dJ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liable-radiation",
   "metadata": {},
   "source": [
    "Now the most interesting part is what happens at the bottom-most layer -- $\\frac{\\delta z}{\\delta w}$.  Remember, at this layer, our hypothesis function looks like the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controlling-matrix",
   "metadata": {},
   "source": [
    "$z = w_1*x_1 + w_2*x_2 + ... b$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swiss-newton",
   "metadata": {},
   "source": [
    "And remember, with gradient descent, we want to determine how to nudge each of our weights, $w_1, w_2, ... w_n$ and our bias term $b$.  So when we find $\\frac{\\delta z}{\\delta w_1}$ we treat every term but the first term as a constant, and so we just get $x_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enclosed-proportion",
   "metadata": {},
   "source": [
    "$\\frac{\\delta z}{\\delta w_1} = x_1 + 0 + 0 +  ... 0 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eastern-entrance",
   "metadata": {},
   "outputs": [],
   "source": [
    "For the second term"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "committed-carrier",
   "metadata": {},
   "source": [
    "$\\frac{\\delta z}{\\delta w_2} = x_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifty-patent",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
