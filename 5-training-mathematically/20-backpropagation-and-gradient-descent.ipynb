{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "inner-light",
   "metadata": {},
   "source": [
    "# BackPropagation and Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaningful-registrar",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varying-majority",
   "metadata": {},
   "source": [
    "Ok, so if you've made it this far, you've seen the main components of training a neural network through gradient descent.  We saw that we can calculate the effect of a change in our parameters across multiple layers through the chain rule.  And we also saw how we can descend along a cost curve for a neural network with multiple parameters by calculating the gradient.  In other words, to descend with multiple parameters we calculate partial derivative of each parameter and then move in proportion to each parameter's partial derivative.  \n",
    "\n",
    "In this lesson, we'll see a small update to the way neural networks calculate the gradient -- by performing backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expanded-intranet",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### The Setup\n",
    "\n",
    "In this lesson we'll learn about both forward and backpropagation.  Even though we may have never heard these terms before, as we'll see, we've already learned a lot about both of these steps.  Let's review what we did in the last lesson and then we'll call out these two steps.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gross-leonard",
   "metadata": {},
   "source": [
    "> We'll begin by loading our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "significant-virgin",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url = \"https://raw.githubusercontent.com/jigsawlabs-student/pytorch-intro-curriculum/main/5-training-mathematically/cell_multiple.csv\"\n",
    "df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "banned-gamma",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_area</th>\n",
       "      <th>mean_concavity</th>\n",
       "      <th>is_cancerous</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.001</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.326</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_area  mean_concavity  is_cancerous\n",
       "0      1.001          0.3001             1\n",
       "1      1.326          0.0869             1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dietary-november",
   "metadata": {},
   "source": [
    "And converted it into tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ahead-membrane",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "X_tensor = torch.tensor(df[['mean_area', 'mean_concavity']].values).float()\n",
    "y_tensor = torch.tensor(df['is_cancerous']).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frank-benefit",
   "metadata": {},
   "source": [
    "Then we select our first observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "secondary-student",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.0010, 0.3001]), tensor(1.))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_x = X_tensor[0]\n",
    "first_y = y_tensor[0]\n",
    "\n",
    "first_x, first_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charitable-gathering",
   "metadata": {},
   "source": [
    "### The prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distributed-hierarchy",
   "metadata": {},
   "source": [
    "Ok, so now that we downloaded our data, converted it into tensors, and selected the first observation, it's time to make our prediction.  Let's define the components of our hypothesis function:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atlantic-piece",
   "metadata": {},
   "source": [
    "> Press shift + return on the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "female-plant",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_fn(x, w, b):\n",
    "    return x @ w + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "sixth-waters",
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_fn(z):\n",
    "    return torch.sigmoid(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "junior-buyer",
   "metadata": {},
   "source": [
    "And then we can initialize the related weight vectors and bias term for a single neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "artistic-bennett",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.tensor([.5, .3]).float()\n",
    "b = torch.tensor(-2.).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "national-encyclopedia",
   "metadata": {},
   "source": [
    "And make our prediction with just our first datapoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "downtown-walnut",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-1.4095)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = linear_fn(first_x, w, b)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "stainless-democrat",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1963)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = activation_fn(z)\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understanding-province",
   "metadata": {},
   "source": [
    "Now this step of calculating the output at each layer is called **forward propagation**.  We are passing data through each layer until we get to a prediction -- above $\\hat{y} = .1669$.  So that's it. Forward propagation is just passing data through the layers of the hypothesis function of our neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silver-underground",
   "metadata": {},
   "source": [
    "### Reviewing the gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neither-fifth",
   "metadata": {},
   "source": [
    "Ok, let's keepe going with our review, moving onto the gradient.  Now, of course, the parameters of our neural network, $w$ and $b$ were just set randomly by us above.  We'll need to use gradient desccent to find the parameters that minimize the output from our cost function.  As we saw in the past lesson, we can summarize our gradient descent formula as the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polar-arctic",
   "metadata": {},
   "source": [
    "$$ \\theta = \\theta - \\eta \\frac{\\delta J}{\\delta \\theta}  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "middle-style",
   "metadata": {},
   "source": [
    "And remember that this term $\\frac{\\delta J}{\\delta \\theta}$ is a vector of each parameter's partial derivative.  \n",
    "\n",
    "Our partial derivatives really consist of the following $\\frac{\\delta J}{\\delta w}$ and $\\frac{\\delta J}{\\delta b}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colored-astronomy",
   "metadata": {},
   "source": [
    "$$\\frac{\\delta J}{\\delta w} = \\frac{\\delta J}{\\delta \\sigma} * \\frac{\\delta \\sigma}{\\delta z} * \\frac{\\delta z}{\\delta w}$$\n",
    "\n",
    "$$\\frac{\\delta J}{\\delta b} = \\frac{\\delta J}{\\delta \\sigma} * \\frac{\\delta \\sigma}{\\delta z} * \\frac{\\delta z}{\\delta b}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tracked-letter",
   "metadata": {},
   "source": [
    "And as we saw in previous lessons, these component derivatives are the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "combined-vulnerability",
   "metadata": {},
   "source": [
    "> Press shift + return on the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "electrical-upper",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def delta_J_delta_sigma(y_hat, y):\n",
    "    return torch.sum(2*(y_hat - y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "sunrise-doctrine",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_sigma_delta(z):\n",
    "    return torch.sigmoid(z)*(1 - torch.sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "champion-motel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_z_delta_w(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "correct-intranet",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_z_delta_b():\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laughing-bearing",
   "metadata": {},
   "source": [
    "So then we can calculate $\\frac{\\delta J}{\\delta w}$ by multiplying the component derivatives together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "featured-service",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2539, -0.0761])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_grad = delta_z_delta_w(first_x)*delta_sigma_delta(z)*delta_J_delta_sigma(y_hat, first_y)\n",
    "w_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flush-sydney",
   "metadata": {},
   "source": [
    "And similarly we can calculate $\\frac{\\delta J}{\\delta b}$ by multiplying the component derivatives together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bulgarian-setting",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.2536)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_grad = delta_z_delta_b()*delta_sigma_delta(z)*delta_J_delta_sigma(y_hat, first_y)\n",
    "b_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "involved-question",
   "metadata": {},
   "source": [
    "One thing we'll notice from the above is in calculating `w_grad` and `b_grad` we repeated the same operation twice: `delta_sigma_delta(z)*delta_J_delta_sigma(y_hat, first_y)`.  This may not seem like a big deal, but this number crunching of the gradient is pretty time consuming, so we'd like to be more efficient if possible.  \n",
    "\n",
    "So now it's time to learn something new.  We'll see how through backpropagation, we can avoid this duplication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relevant-villa",
   "metadata": {},
   "source": [
    "### Onto backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "balanced-impact",
   "metadata": {},
   "source": [
    "Now backpropagation is easiest to understand by example.  Remember that these were our layers of our neural network:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technical-minutes",
   "metadata": {},
   "source": [
    "* $z(x_i) = w_1*x_i $\n",
    "* $\\sigma(z) =  \\frac{1}{1 + e^{-z(x)}} $\n",
    "* $ J(\\hat{y}, y) = \\sum  (y - \\hat{y})^2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liked-marble",
   "metadata": {},
   "source": [
    "This time instead of simply multiplying together the component derivatives at the very end, we'll work backwards through our layers, and **calculate each layer's impact on our cost function $J$**.  \n",
    "> So working backwards, we start by seeing how our last layer, $\\sigma$, has an impact on our cost function.  Then see how a change in $z$ has an impact on the cost function $J$, and then see how a change in parameters $w$ and $b$ impact the cost function.  In other words, we'll calculate:\n",
    "\n",
    "> * $\\frac{\\delta J}{\\delta \\sigma}$\n",
    "\n",
    "> * $\\frac{\\delta J}{\\delta z} = \\frac{\\delta \\sigma}{\\delta z}*\\frac{\\delta J}{\\delta \\sigma}$\n",
    "\n",
    "> * $\\frac{\\delta J}{\\delta w} = \\frac{\\delta w}{\\delta z}*\\frac{\\delta J}{\\delta z}$\n",
    "\n",
    "Let's understand this approach better by moving on with the example.  So we work backwards, first calculating the impact of $\\sigma$ on the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unsigned-banana",
   "metadata": {},
   "source": [
    "1. Calculate the impact of a small change in $\\sigma$ on $J$, $\\frac{\\delta J}{\\delta \\sigma}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "turkish-contrast",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-1.6074)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dj_dsig = delta_J_delta_sigma(y_hat, first_y)\n",
    "dj_dsig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thick-milan",
   "metadata": {},
   "source": [
    "> This is exactly the same calclulation $\\frac{\\delta J}{\\delta \\sigma}$ that we calculated previously, so we just reuse the function from above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classical-deadline",
   "metadata": {},
   "source": [
    "2. Then we find the impact of a small change in the output of the linear layer $z$ on $J$, $\\frac{\\delta J}{\\delta z}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opponent-focus",
   "metadata": {},
   "source": [
    "> This, we have **not** found calculated before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "professional-lebanon",
   "metadata": {},
   "source": [
    "This is the formula: $\\frac{\\delta J}{\\delta z} = \\frac{\\delta \\sigma}{\\delta z}*\\frac{\\delta J}{\\delta \\sigma}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entitled-phoenix",
   "metadata": {},
   "source": [
    "Let's think about how the formula above makes sense.  We want to find a change in $z$'s impact on $J$.  And remember, these are our layers.\n",
    "\n",
    "* $z(x_i) = w_1*x_i $\n",
    "* $\\sigma(z) =  \\frac{1}{1 + e^{-z(x)}} $\n",
    "* $ J(\\hat{y}, y) = \\sum  (y - \\hat{y})^2 $\n",
    "\n",
    "So to find $z$'s impact on $J$, this is the impact that a change in $z's$ output has on the activation layer $\\sigma$, $\\frac{\\delta \\sigma}{\\delta z}$, multiplied by $\\sigma$'s impact on $J$, $\\frac{\\delta J}{\\delta \\sigma}$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indoor-praise",
   "metadata": {},
   "source": [
    "And because we worked backwards, we just calculated $\\frac{\\delta J}{\\delta \\sigma}$ in step 1.  So now we just need to calculate $\\frac{\\delta \\sigma}{\\delta z}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "received-directive",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.2536)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsig_dz = delta_sigma_delta(z)\n",
    "dz_dJ = dsig_dz*dj_dsig\n",
    "dz_dJ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desperate-heart",
   "metadata": {},
   "source": [
    "Next let's add some terms to our two component derivatives in the formula: $\\frac{\\delta J}{\\delta z} = \\frac{\\delta \\sigma}{\\delta z}*\\frac{\\delta J}{\\delta \\sigma}$.  \n",
    "\n",
    "* We'll call our newly calculated derivative $\\frac{\\delta \\sigma}{\\delta z}$ our **local derivative**, and\n",
    "* We'll call the derivative we calculated in the previous layer $\\frac{\\delta J}{\\delta \\sigma}$ our **upstream derivative**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monetary-syntax",
   "metadata": {},
   "source": [
    "As we'll see, we'll continue to use this approach as we move down through our layers: calculate the new local derivative and multiply it by the derivative upstream derivative.\n",
    "\n",
    "Ok, let's keep going and calculate how the next layer down has an impact on $J$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ahead-insert",
   "metadata": {},
   "source": [
    "3. Find the impact of a change in $w$ on $J$, that is $\\frac{\\delta J}{\\delta w}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "official-arbor",
   "metadata": {},
   "source": [
    "Now to make this calculation, our formula is: $\\frac{\\delta J}{\\delta w} = \\frac{\\delta w}{\\delta z}*\\frac{\\delta J}{\\delta z}$.  So once again, it's (1) our local derivative where we find $w's$ impact on $z$, and then we multiply by the derivative we just calculated upstream, $\\frac{\\delta J}{\\delta z}$, $z$'s impact on our cost function $J$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "joined-mechanics",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2539, -0.0761])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dz_dw = delta_z_delta_w(first_x)\n",
    "dw_dJ = dz_dw*dz_dJ\n",
    "dw_dJ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "popular-energy",
   "metadata": {},
   "source": [
    "> So the partial derivative with respect $w_1$ is $ \\frac{\\delta J}{\\delta w_1} = -.2539$, and the partial derivative with respect to the second parameter is $ \\frac{\\delta J}{\\delta w_2} = -.0761$.  And we calculated this by working backwards through our layers, and repeatedly multiplying the local gradient by the upstream gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bottom-singing",
   "metadata": {},
   "source": [
    "Notice that we calculatedd the same number above, when multiplying all three terms together: $\\frac{\\delta J}{\\delta w} = \\frac{\\delta J}{\\delta \\sigma} * \\frac{\\delta \\sigma}{\\delta z} * \\frac{\\delta z}{\\delta w}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "thirty-adelaide",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2539, -0.0761])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dj_dsig*dsig_dz*dz_dw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "external-threshold",
   "metadata": {},
   "source": [
    "But what's nice about this new technique is that **we can reuse our upstream gradient $\\frac{\\delta z}{\\delta J}$**, when calculating $\\frac{\\delta J}{\\delta w}$ and $\\frac{\\delta J}{\\delta b}$.  Let's see this.\n",
    "\n",
    "4. Find the impact of a change in $b$ on $J$, that is $\\frac{\\delta J}{\\delta b}$.\n",
    "\n",
    "This again is our local derivative multiplied by the same directly upstream derivative $\\frac{\\delta z}{\\delta J}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "clinical-divorce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2497], dtype=torch.float64)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dJ_db = dz_db*dz_dJ\n",
    "dJ_dw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upper-password",
   "metadata": {},
   "source": [
    "So this is backpropagation.  It's a small trick that allows us to reuse some of our earlier calculations.  With backpropagation, we start with our top most layer, here $\\frac{\\delta \\sigma}{\\delta J}$, and then move down through our layers each time calculating each layer's impact on the cost function $J$ by multiplying the local derivative by the direectly upstream derivative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tight-netherlands",
   "metadata": {},
   "source": [
    "### All together now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controversial-converter",
   "metadata": {},
   "source": [
    "Ok, now let's sum our process of forward propagation and backward propagation.  With forward propagation we initialize the values of our parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "breeding-pillow",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.tensor([.5, .3]).float()\n",
    "b = torch.tensor(-2.).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abandoned-bailey",
   "metadata": {},
   "source": [
    "And then perform forward propagation by passing our data through our layers and calculating the output at each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "difficult-squad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z =  tensor(-1.4095)\n",
      "y_hat =  tensor(0.1963)\n"
     ]
    }
   ],
   "source": [
    "z = linear_fn(first_x, w, b)\n",
    "print('z = ', z)\n",
    "y_hat = activation_fn(z)\n",
    "print('y_hat = ', y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comprehensive-treasurer",
   "metadata": {},
   "source": [
    "Once we calculated the output at each layer, we move to backward propagation.  With backward propagation, we calculate how nudging each layer's output will change our cost function.  And we make this calculation by multiplying the local derivative with the upstream derivative.  So to calculate $\\frac{\\delta J}{\\delta w}$, we just move backward through our layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ambient-tongue",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2500], dtype=torch.float64)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dj_dsig = delta_J_delta_sigma(y_hat, first_y)\n",
    "\n",
    "dJ_dz = delta_sigma_delta(z)*dj_dsig # local_deriv*upstream\n",
    "dJ_dw = delta_z_delta_w(first_x)*dJ_dz # local_deriv*upstream\n",
    "\n",
    "dJ_dw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classified-writing",
   "metadata": {},
   "source": [
    "And then in calculating $\\frac{\\delta J}{\\delta b}$, we already calculated the upstream derivative $\\frac{\\delta J}{\\delta z}$, and so we only have to calculate the local derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "little-supplement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2497], dtype=torch.float64)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dJ_db = delta_z_delta_b()*dJ_dz\n",
    "dJ_db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "growing-morning",
   "metadata": {},
   "source": [
    "> Notice, that in calculating our derivatives above, we *did need* to use calculations from forward propagation.  For example, we passed through `y_hat` to calculate `delta_J_delta_sigma`.  And we passed through `z` to calculate `delta_sigma_delta(z)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "discrete-strain",
   "metadata": {},
   "source": [
    "Ok, so now let's repeatedly perform forward and backward propagation to descend along the cost curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "developmental-green",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.tensor([.5, .3]).float()\n",
    "b = torch.tensor(-2.).float()\n",
    "eta = .0001 # define learning rate \n",
    "for i in range(20):\n",
    "    for x, y in zip(X_tensor, y_tensor):\n",
    "    # forward propagation\n",
    "        z = linear_fn(w, x, b)\n",
    "        y_hat = activation_fn(z)\n",
    "    \n",
    "    # backward propagation\n",
    "        dj_dsig = delta_J_delta_sigma(y_hat, y)\n",
    "        dJ_dz = delta_sigma_delta(z)*dj_dsig\n",
    "        dJ_dw = delta_z_delta_w(first_x)*dJ_dz\n",
    "        \n",
    "        dJ_db = delta_z_delta_b()*dJ_dz\n",
    "    \n",
    "    # update params w, b\n",
    "        w = w - eta*dJ_dw\n",
    "        b = b - eta*dJ_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "experimental-jefferson",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.5786, 0.3236]), tensor(-1.9215))"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vital-mandate",
   "metadata": {},
   "source": [
    "And notice how this aligns with the gradient descent approach we saw in Pytorch. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developing-liberty",
   "metadata": {},
   "source": [
    "```python\n",
    "for (x, y) in zip(X_train_tensor_gpu, y_train_tensor_gpu): # loop through observations and labels\n",
    "    net.zero_grad() # remove calculated derivatives\n",
    "    y_hat = net(X_tensor)             # 1. Forward prop: With current weights make a prediction\n",
    "    loss = criterion(y_hat, y_tensor) # 2. See how off the prediction is according to the cost function\n",
    "    loss.backward()                   # 3. Back prop: Calculate how each layer's change in output affects J\n",
    "    opt.step()                        # 4. Update params based on eta and calculated derivatives \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "severe-organization",
   "metadata": {},
   "source": [
    "So that's forward and backward propagation.  Let's now go back to using Pytorch to perform image classification and see if we can understand the process any better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "located-handling",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reasonable-draft",
   "metadata": {},
   "source": [
    "In this lesson, we learned about forward and backward propagation.  Forward propagation just means to calculate the output of each layer when we pass data through our hypothesis function.  \n",
    "\n",
    "And with backward propagation we calculate each layer's impact on the cost function by multiplying the local derivative by the upstream derivative.  So moving backwards through our layers, we calculated:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "committed-excerpt",
   "metadata": {},
   "source": [
    "* $\\frac{\\delta J}{\\delta \\sigma}$\n",
    "\n",
    "* $\\frac{\\delta J}{\\delta z} = \\frac{\\delta \\sigma}{\\delta z}*\\frac{\\delta J}{\\delta \\sigma}$\n",
    "\n",
    "* $\\frac{\\delta J}{\\delta w} = \\frac{\\delta w}{\\delta z}*\\frac{\\delta J}{\\delta z}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "working-oregon",
   "metadata": {},
   "source": [
    "And finally, we calculated:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "listed-conversation",
   "metadata": {},
   "source": [
    "* $\\frac{\\delta J}{\\delta b} = \\frac{\\delta b}{\\delta z}*\\frac{\\delta J}{\\delta z}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swedish-congo",
   "metadata": {},
   "source": [
    "Each time, we multiplied the local derivative by the upstream derivative.  \n",
    "\n",
    "Finally, we used forward and backward propagation to perform gradient descent, and saw how it aligned with our code in Pytorch."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
