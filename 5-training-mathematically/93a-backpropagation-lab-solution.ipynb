{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we'll code out both the hypothesis functions and training mechanisms for a sigmoid function.  We'll do so to walk through the chain rule once more, and hopefully, to place it in context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our Starting Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that when we make a prediction with a single neuron, we do so with the following hypothesis function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$h(x) = \\sigma(w x + b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's start by translating this into two functions `z(x)` and our sigmoid function.\n",
    "\n",
    "> Let z be a function of $w$, $b$ and $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z(w, b, x):\n",
    "    return w*x + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z(1.5, .5, 1)\n",
    "\n",
    "# 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, let's have our parameters be stored in a dictionary with keys of $w$ and $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z(params, x):\n",
    "    return params['w']*x + params['b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.308"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {'w': -1.0, 'b': .5}\n",
    "\n",
    "z(params, -1.808)\n",
    "\n",
    "# 2.308401245182045"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's write our sigmoid function, which translates this into a value between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8807970779778823"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {'w': 1.5, 'b': .5}\n",
    "x = 1\n",
    "\n",
    "sigmoid(z(params, x))\n",
    "\n",
    "# 0.8807970779778823"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now that's starting to feel like a hypothesis.  Finally, let's wrap this in a function called `h(params, x)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h(params, x):\n",
    "    return sigmoid(z(params, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8807970779778823"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h(params, 1)\n",
    "\n",
    "# 0.8807970779778823"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've written out our hypothesis function, let's begin writing the code for our cost function.  We can begin by loading up some data from the breast cancer dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "import pandas as pd\n",
    "cancer_dataset = load_breast_cancer()\n",
    "\n",
    "X = pd.DataFrame(cancer_dataset.data[:, :1], columns = cancer_dataset.feature_names[:1])\n",
    "y = pd.Series(cancer_dataset.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([17.99, 20.57, 19.69, 11.42])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_radiuses = X.iloc[:, 0].values\n",
    "\n",
    "mean_radiuses[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Ok, let's write a functin called loss which calculates the loss for a single observation, given a set of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(x, y, params):\n",
    "    return (y - h(params, x))**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_mean_radius = mean_radiuses[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999976854"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(first_mean_radius, y[0], params)\n",
    "\n",
    "# 0.9999999999976854"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's move to our cost function, which calculates the total losses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$J_{w, b}(X) = \\sum_{i = 1}^n( y - h(x))^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_cost(X, y, params):\n",
    "    return np.sum([loss(mean_radius, y, params)for mean_radius, y in \n",
    "                    zip(mean_radiuses, y.values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "211.99999962013467"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_cost(x, y, params)\n",
    "\n",
    "# 211.99999962013467"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have some initial parameters and have calculated our first total cost, the next task is to work on updating the parameters to minimize this cost.  To do so, we need to determine the amount that we should update the parameters by to approach the minimum step by step.  \n",
    "\n",
    "As we know, we do this by moving in the negative direction of the gradient.  So if our hypothesis function is the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$h(x) = \\sigma(z(w, x, b))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the question is how much does nudging our parameter $w$:\n",
    "\n",
    "1. Change the output of $z$,\n",
    "2. Which then changes the output of $\\sigma$\n",
    "3. Which then changes the total cost $J(X)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's break this down into steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Calculate the change in the loss as we have a change in our hypothesis function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Here, let's calculate the change in the loss for a single observation, as we change the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\delta \\ell}{\\delta \\sigma} = (y - h(x))^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Find $\\frac{d\\ell}{d\\sigma}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\delta z}{\\delta w}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\delta z}{\\delta b}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'w': 1.0, 'b': -1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dl_dsig(params, x, y):\n",
    "    loss = h(params, x) - y\n",
    "    return 2*loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7550813375962908"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x = 1, y = 0\n",
    "params = {'w': -1.0, 'b': .5}\n",
    "\n",
    "dl_dsig(params, 1, y[0])\n",
    "# 0.7550813375962908"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that with the current parameters, the change in output of $h(x) = \\sigma(x)$ results in a `0.0014` change in the cost at a single observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this calculated cost is only useful in that we'll need it later on.  Let's keep going."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$h(x) = \\sigma(z(w, x, b))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Finding $\\frac{\\delta \\ell}{\\delta z}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start we know that a change in the output in z will change the output in sigma which will then change the our loss of $\\ell$.  So we get:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $\\frac{\\delta \\ell}{\\delta z} = \\frac{\\delta \\ell}{\\delta \\sigma} \\frac{\\delta \\sigma}{\\delta z}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we already wrote functions for calculating $\\frac{\\delta l}{\\delta \\sigma}$, above so what's left is to calculate $\\frac{\\delta \\sigma}{\\delta z}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now the derivative $\\frac{\\delta \\sigma}{\\delta z}$ is the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\sigma'(z(x)) = \\sigma(z(x))*(1 - \\sigma(z(x)))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now translate this into code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dsig_dz(params, x, y):\n",
    "    return sigmoid(z(params, x))*(1 - sigmoid(z(params, x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2350037122015945"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 1  #y[0]\n",
    "params = {'w': -1.0, 'b': .5}\n",
    "dsig_dz(params, x, y[0])\n",
    "\n",
    "# 0.2350037122015945"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Finding $\\frac{\\delta l}{\\delta w}$\n",
    "\n",
    "* Now the change in the loss with respect to a change $w$ is the following"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $\\frac{\\delta l}{\\delta w} = \\frac{\\delta \\ell}{\\delta \\sigma} \\frac{\\delta \\sigma}{\\delta z} \\frac{\\delta z}{\\delta w}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we already wrote functions to calculate $\\frac{\\delta \\ell}{\\delta \\sigma}$ and $\\frac{\\delta \\sigma}{\\delta w}$.  So now it's time to write a function that calculates $\\frac{\\delta z}{\\delta w}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dz_dw(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dz_dw(x)\n",
    "# 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Finding $\\frac{\\delta l}{\\delta b}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Similarly, finding the change in the loss with respect to a change $b$ is the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $\\frac{\\delta l}{\\delta b} = \\frac{\\delta \\ell}{\\delta \\sigma} \\frac{\\delta \\sigma}{\\delta z} \\frac{\\delta z}{\\delta b}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now, let's just write a function that returns the $\\frac{\\delta z}{\\delta b}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dz_db():\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it together with backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we've now written functions that will allow us to calculate how much the loss function changes as we change $w$ and $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x):\n",
    "    z = wx + b\n",
    "    h = sigmoid(z)\n",
    "    l = (y - h)^2\n",
    "\n",
    "def backwards():\n",
    "    dl/dh =  2*(h - y) # 4\n",
    "    dl/dz = sigmoid(z)(1 - sigmoid(z))*dl/dh # 3*4\n",
    "    dl/dw = x*dl/dz\n",
    "    dz/db = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dJ_dw_and_dJ_db(params, x, y):\n",
    "    \n",
    "    dJ_dsig_result = dl_dsig(params, x, y)\n",
    "    dJ_dz_result = dJ_dsig_result*dsig_dz(params, x, y)\n",
    "    # the upstream derivatives above, are shared\n",
    "    # by the partial derivatives below\n",
    "    \n",
    "    dJ_dw = dJ_dz_result*dz_dw(x)\n",
    "    dJ_db = dJ_dz_result*dz_db()\n",
    "    return (dJ_dw.round(6), dJ_db.round(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.026414, 0.013207)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {'w': 3, 'b': -1}\n",
    "x, y_val = 2, 0\n",
    "dJ_dw_and_dJ_db(params, x, y_val)\n",
    "\n",
    "# (0.026414, 0.013207)\n",
    "# y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we should move our parameters in the negative direction of the gradient.  And here, we see that we should update our parameters, w and b by some fraction of $0.026$ and $0.013$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so the function `dJ_dw_and_dJ_db` calculates the gradient for w and b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(gradient, current_params, learning_rate):\n",
    "    new_w = current_params['w'] - learning_rate*gradient[0] \n",
    "    new_b = current_params['b'] - learning_rate*gradient[1]\n",
    "    return {'w': new_w, 'b': new_b}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying it out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we just wrote the components to calculate the hypothesis of our neuron as well as how to update our neuron.  Let's see how our code performs.\n",
    "\n",
    "As a first step, we should scale our data to ensure that our model is not too influenced by outliers, or uneven data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_transformed = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's initialize some parameters, set a learning rate, and repeatedly update our parameters according to our calculated gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> One way that our neural network is not quite right is that we are making an update at each instance instead of calculating the gradient of the total cost.  \n",
    "\n",
    "Still let's give it a shot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -3, 1\n",
    "params = {'w': 0, 'b': 0}\n",
    "learning_rate = .001\n",
    "\n",
    "# stochastic gradient descent  \n",
    "for i in range(1000):\n",
    "    for (x, y_value) in zip(X_transformed[:, 0], y):\n",
    "        gradient = dJ_dw_and_dJ_db(params, x, y_value)\n",
    "        params = step(gradient, params, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'w': -3.4708154290001905, 'b': 0.6869705380000174}"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "# model.fit(X_transformed, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-3.31967618]]), array([0.64237044]))"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coef_, model.intercept_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see that this performs pretty similarly to our logistic regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we went through the complete steps of coding out a single neuron, and then we compared how this performed to an equivalent model in sklearn.\n",
    "\n",
    "We started by coding out the hypothesis function:\n",
    "\n",
    "$h(x) = \\sigma(wx + b)$\n",
    "\n",
    "And then moved onto the loss function:\n",
    "\n",
    "$\\ell = (y - h(x))^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, to perform gradient descent, we needed to write a function in charge of calculating the gradients of both $w$ and $b$.  To do so, we used the chain rule to calculate the impact that altering either parameter has on our cost function.  And we used backpropagation so that we could reuse the intermediary derivatives in our calculation of the gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Problems here: \n",
    "    \n",
    "    1. X and y values are not explicit \n",
    "    2. Changed between y and targets\n",
    "    3. The loss function is reversed  mean_radiuses, and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
