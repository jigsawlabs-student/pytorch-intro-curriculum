{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an Initial Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last lesson, we saw that we can represent the weights of our neurons with a weight matrix, $W$.  In that matrix, each vector represents the weights of a different neuron.  So in the matrix below, W contains the weights of two neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$W = \\begin{bmatrix}\n",
    "|  & |  \\\\\n",
    "w_1  & w_2 \\\\\n",
    "|   & |\n",
    "\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we used the weight matrix to calculate outputs of the linear functions of each neuron, by multiplying our weight matrix by a feature vector $x$, and adding the bias of each neuron.\n",
    "\n",
    "> So below is the ouput of the linear component of the first layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$z = \\begin{bmatrix}\n",
    "- & x &  -  \n",
    "\\end{bmatrix} \\cdot \\begin{bmatrix}\n",
    "|  & |  \\\\\n",
    "w_1  & w_2 \\\\\n",
    "|   & |\n",
    "\\end{bmatrix} + \\begin{bmatrix} b_1 & b_2 \\end{bmatrix} = \\begin{bmatrix}\n",
    "x \\cdot w_1 & x \\cdot w_2 \\end{bmatrix} + \\begin{bmatrix} b_1 & b_2 \\end{bmatrix} = \\begin{bmatrix} h(x) & h(x) \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Random Neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in previous lessons, the weights for each of our neurons were given to us.  But as from the section on training, our neural network **does not know** which weights to begin with.  Rather, it learns these weights over time.  So for us, this means that we can just initialize our neural network with random weights, and the important thing is to get the dimensions of each neuron, and our layer correct.  Let's walk through this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's continue with our domain of using neurons to predict if a cell is cancerous.  And say that we have a feature vector $x$ to represent our first observation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# perimeter, radius, volume, # asymmetries, bumpiness    \n",
    "x = np.array([.5, .1, .3, .4, .8])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to initialize a neuron that accepts each of these features, we need to initialize a vector with five random weights: one for each feature.  And we need to initialize a random bias term.  We can do this with our linear layer function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "ll = nn.Linear(5, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              Parameter containing:\n",
       "              tensor([[ 0.0661,  0.1877,  0.0044,  0.1915, -0.2948]], requires_grad=True)),\n",
       "             ('bias',\n",
       "              Parameter containing:\n",
       "              tensor([0.1442], requires_grad=True))])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll._parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Below we use the `np.random.randn` function.  This function takes two arguments, the number of rows to return and the number of columns we want to initialize.  So below, we call `randn(5, 1)` as we want a single neuron that has weights for five features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.62434536],\n",
       "       [-0.61175641],\n",
       "       [-0.52817175],\n",
       "       [-1.07296862],\n",
       "       [ 0.86540763]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "np.random.seed(1)\n",
    "n_1_w = np.random.randn(5, 1)\n",
    "n_1_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the values above are between -2 and -2.  This is because `np.random.randn` draws values from the standard normal distribution.  \n",
    "\n",
    "> If you're not familiar with this distribution, it's not critical to your understanding going forward.  The important point is that we will draw values that on average are 0, and that over 99%Â of the time, the numbers drawn will be between -3 and 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's initialize the bias term.  Here we just want to produce a single random value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.97727788])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_1_b = np.random.randn(1)\n",
    "n_1_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we just initialized the weights and bias term for a single neuron that takes five features with the following lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "np.random.seed(1)\n",
    "n_1_w = np.random.randn(5, 1)\n",
    "\n",
    "n_1_b = np.random.randn(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code, `np.random.randn(5, 1)` constructed a matrix with five rows and one column.  The one column was for the single neuron and the five rows are for the five weights to correspond with the features of an observation.  The `n_1_b` only has a single random value as the linear component of a neuron has one bias term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $z(x) = w_1x_1 + w_2x_2 + ... + w_nx_n + b $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a random layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now let's initialize weight matrix to represent *two neurons*, each with five weights.  Here it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.62434536, -0.61175641],\n",
       "       [-0.52817175, -1.07296862],\n",
       "       [ 0.86540763, -2.3015387 ],\n",
       "       [ 1.74481176, -0.7612069 ],\n",
       "       [ 0.3190391 , -0.24937038]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "W = np.random.randn(5, 2)\n",
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this represents two neurons, with each neuron initialized with five differents weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the bias is simply a vector with two entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Press shift + return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.46210794, -2.06014071])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = np.random.randn(2)\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have initialized the weights and biases of our two neurons, we can then use these produce an output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do this by starting with our feature vector $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And passing this feature vector through our linear layer, followed by our activation layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $z(x) = x \\cdot W + b$\n",
    "* $a(z) = \\frac{1}{1 + e^{-z}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.43424171, -3.66775645])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# perimeter, radius, volume, # asymmetries, bumpiness    \n",
    "x = np.array([.5, .1, .3, .4, .8])\n",
    "z = x.dot(W) + b\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9687577 , 0.02489796])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sigmoid(z): return 1/(1 + np.exp(-z))\n",
    "\n",
    "sigmoid(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that each of our neurons makes a separate prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practicing with Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's say that we want to initialize the weights and biases for a neural network that looks like the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./first-layer.png\" width=\"30%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What the graph symbolizes above is that each observation $x$ has four features, $x_1$ through $x_4$ -- as indicated by the blue circles.  And we have a layer with four neurons.\n",
    "\n",
    "> The lines drawn from each feature to each neuron to each circle indicate that each neuron receives each feature of the observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's initialize parameters for the layer of the neural network that we see above.  \n",
    "\n",
    "So that would be four columns (one for each neuron) and four rows (as each neuron has four weights) matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01791712,  0.20491942,  0.12166726,  0.85470516],\n",
       "       [-0.54014973,  0.20377669, -0.64654733,  1.51752847],\n",
       "       [-1.27656353, -0.42488659,  0.02028611, -0.53855715],\n",
       "       [-0.00312273,  1.6536728 , -1.13278799, -0.18014837]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "W = np.random.randn(4,4)\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.51094515, -0.4406927 ,  0.58420857,  0.43645582])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = np.random.randn(4)\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So notice that above we have four different neurons, each taking in the the four features of an observation x, and making a prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we saw how we can use numpy to initialize a layer of a neural network.  We do with the `np.random.randn` function, and then specifying the specifying the dimensions of our layer.  For the weight matrix W, the number of rows corresponds to the length of the feature vector.  And there is one vector for each neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"https://www.jigsawlabs.io/free\" style=\"position: center\"><img src=\"https://storage.cloud.google.com/curriculum-assets/curriculum-assets.nosync/mom-files/jigsaw-labs.png\" width=\"15%\" style=\"text-align: center\"></a>\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
