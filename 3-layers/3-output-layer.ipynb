{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "linear-abortion",
   "metadata": {},
   "source": [
    "# The Output Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anticipated-victim",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "In the last lesson, we learned how to build a neural network with multiple layers.  And we saw that, through multiple layers, our neural network can make more and more abstract assessments.  Now something like this is really powerful when working with image data, where earlier layers can make concrete assessements like whether there is are straight or curved lines, and later layers can combine the outputs from earlier layers to see determine if the combination of curved and straight lines makes our picture likely to represent a specific number like 2 or 3.\n",
    "\n",
    "Now how does a neural network indicate a prediction of a 2 or 3, or a number 9?  That's what we'll learn in this lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structural-worthy",
   "metadata": {},
   "source": [
    "### Representing image data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fluid-fabric",
   "metadata": {},
   "source": [
    "Before we understand how our neural network will output a prediction about an image, let's briefly see what it means to take in a prediction.  When we pass an image into our neural network, really what we're passing through is a list of pixels from an image.  Our image data will be black and white images of handwritten digits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advisory-mercy",
   "metadata": {},
   "source": [
    "<img src=\"./mnist.png\" width=\"30%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numeric-scanning",
   "metadata": {},
   "source": [
    "So each image is a different observation and each individual pixel is represented as a different number, indicating how light or dark that pixel is.\n",
    "\n",
    "So to generate some input data, let's just create a random list of 784 numbers to represent a single image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "grateful-ability",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 784])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(1)\n",
    "\n",
    "\n",
    "x = torch.randint(100, (1, 784))\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composed-reaction",
   "metadata": {},
   "source": [
    "So above we chose random numbers from one to one hundred, and did so to produce a tensor 784 elements long."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifty-budapest",
   "metadata": {},
   "source": [
    "### The output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infectious-graphic",
   "metadata": {},
   "source": [
    "So we just saw what we feed into a neural network -- a vector where every pixel of an image is represented as a different value.  What do we want as the output of a neural network?  Well a classification problem -- which we have been using thus far -- it's generally one prediction per each potential outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ongoing-packet",
   "metadata": {},
   "source": [
    "So for our handwriting dataset, the potential outcomes are each of digits that an image could represent.  This means that the neural network should output a tensor with ten entries.  And each entry will represent the probability of a different entry.  \n",
    "\n",
    "Let's see this below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multiple-delight",
   "metadata": {},
   "source": [
    "> Below, our neural network predicts a $.05$ percent probability of the picture being a 0, a $.62$ probability of the picture being a 1, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "sealed-parish",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.05, 0.62, 0.03, 0.03, 0.05, 0.03, 0.15, 0.01, 0.02, 0.01]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = [.05, .62, .03, .03, .05, .03, .15, .01, .02, .01]\n",
    "output\n",
    "# 0    1    2   3    4     5   6   7     8   9  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chicken-birthday",
   "metadata": {},
   "source": [
    "Ok, so this is what we want outputted from our neural network.  And with this in mind, let's move forward to constructing it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "damaged-recording",
   "metadata": {},
   "source": [
    "### Defining our Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "freelance-memorial",
   "metadata": {},
   "source": [
    "Ok, now it's time to build our neural network.  We know that each neuron in the first layer must take 784 features, and it's also true that we can have as many neurons as we want in that first layer.  \n",
    "\n",
    "> A good rule of thumb is to have around half the number of features.  Here, that would be 392, but we're building a smaller neural network so we'll choose way less. \n",
    "\n",
    "We'll specify 64 neurons in the first layer, and then pass these 64 outputs to the sigmoid function.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "through-agenda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=784, out_features=64, bias=True)\n",
       "  (1): Sigmoid()\n",
       "  (2): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(5)\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(784, 64),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(64, 10)\n",
    ")\n",
    "\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valid-playlist",
   "metadata": {},
   "source": [
    "Then in the last layer, we'll see that each neuron takes in 64 features, and that we have 10 neurons.  We need 10 neurons in that last layer to produce the 10 outputs from the neural network.  And we need each neuron to take in 64 features, because we pass the 64 outputs from the previous layer to each of the neurons in our next layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "front-console",
   "metadata": {},
   "source": [
    "Let's make sure we understand what's going on under the hood by representing our neural network mathematically.  Mathematically, our our neural network is performing the following operation to produce the output of 10 numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stylish-state",
   "metadata": {},
   "source": [
    "$z_{1x64} = x_{1x784} \\cdot W_{784x64} + b_{1x64}$\n",
    "\n",
    "$a_{1x64} = \\sigma(Z_{1x64})$\n",
    "\n",
    "$z_{1x10} = A_{1x64} \\cdot W_{64x10} + b_{1x10}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "french-thesis",
   "metadata": {},
   "source": [
    "> Confirm our rules that when performing matrix multiplication, the inner numbers are equal.  And that the ouput of the matrix multiplication is determined by outer dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hourly-petite",
   "metadata": {},
   "source": [
    "And we can confirm that we get an output of tensor of length 10, by passing through our data like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "provincial-backup",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 784])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "hairy-tonight",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0716,  0.1905,  0.2812, -0.0302,  0.4545, -0.0408,  1.0216, -0.1237,\n",
       "         -0.3579,  0.8472]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = net(x.float())\n",
    "\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modular-absorption",
   "metadata": {},
   "source": [
    "Ok, so we did produce 10 numbers to output from our neural network -- but there are a couple of issues with the output above.  We'll discuss, and remedy, those issues below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "timely-restoration",
   "metadata": {},
   "source": [
    "### Taking the exponent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alone-america",
   "metadata": {},
   "source": [
    "Ok, so there are a couple of goals that we would like to achieve with this ouput layer.  One is that we want just one prediction to dominate.  For example, if our neural network thinks that the digit is most likely the number 0, we want that first slot to have a probability of .9 to express that confidence.  \n",
    "\n",
    "But, if our output, we have to represent 10 outcomes, and each potential output is assigned at least some probability, this becomes difficult.  For example, if each of the other 9 outcomes is assigned a small probability like $.03$, this would only leave something like $1 - 9*.03 = .73$ for the outcome we are most confident about.  And a $.73$ doesn't exactly express confidence about a particular outcome.  \n",
    "\n",
    "To fix this, we have our neural network exaggerate the outputs it is most confident about.  And it does this by taking the exponent.  Let's see how this works through a few examples. For small numbers, like say, $1.2$, applying the exponent produces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "stopped-transformation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3201)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num = torch.tensor(1.2)\n",
    "\n",
    "torch.exp(num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "drawn-printer",
   "metadata": {},
   "source": [
    "But for larger numbers like $4.8$, applying the exponent produces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "inner-watershed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(121.5104)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "larger_num = torch.tensor(4.8)\n",
    "\n",
    "torch.exp(larger_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vulnerable-coach",
   "metadata": {},
   "source": [
    "So the exponent really exaggerates the larger numbers -- blowing up the number $4.8$ while only slightly increeasing the number $1.2$. \n",
    "\n",
    "And let's see how it treats negative values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "reserved-palestinian",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0498)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_num = torch.tensor(-3.)\n",
    "\n",
    "torch.exp(neg_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beginning-estimate",
   "metadata": {},
   "source": [
    "So it transforms the negative number into a small positive output, which also is appropriate for returning probabilities.  (After all we cannot say there is a negative probability that an outcome occurs.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boring-fundamentals",
   "metadata": {},
   "source": [
    "So let's apply the exponent function to every output from our last layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intellectual-crime",
   "metadata": {},
   "source": [
    "> We start with the following prediction from our neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "environmental-graph",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0716,  0.1905,  0.2812, -0.0302,  0.4545, -0.0408,  1.0216, -0.1237,\n",
       "         -0.3579,  0.8472]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collective-medium",
   "metadata": {},
   "source": [
    "And applying the exponent produces the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "optical-creature",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9309, 1.2098, 1.3248, 0.9702, 1.5754, 0.9600, 2.7777, 0.8836, 0.6991,\n",
       "         2.3330]], grad_fn=<ExpBackward>)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp = torch.exp(prediction)\n",
    "exp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statutory-worker",
   "metadata": {},
   "source": [
    "So we can see that one number, $2.3182$, tended to dominate the others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "covered-senator",
   "metadata": {},
   "source": [
    "### Converting to probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fixed-behavior",
   "metadata": {},
   "source": [
    "Now does our vector above look like a vector of probabilities? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "convenient-atlantic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9309, 1.2098, 1.3248, 0.9702, 1.5754, 0.9600, 2.7777, 0.8836, 0.6991,\n",
       "         2.3330]], grad_fn=<ExpBackward>)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "curious-operations",
   "metadata": {},
   "source": [
    "Not really.  For an entry to be a valid probability, it must be between 0 and 1, and the entire vector should add up to one.  \n",
    "\n",
    "> In other words, there is a probability of 1, that our observation is *any* of the potential outcomes (0 through 9).\n",
    "\n",
    "But we can can transform our vector into a probability by calculating each entry's percentage of the total is of the total.  \n",
    "\n",
    "> So we get the total by adding up the vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "understanding-confidentiality",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(13.6647, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_exp = torch.sum(exp)\n",
    "sum_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norwegian-cancer",
   "metadata": {},
   "source": [
    "And then divide each entry by that total with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "atmospheric-findings",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0681, 0.0885, 0.0969, 0.0710, 0.1153, 0.0703, 0.2033, 0.0647, 0.0512,\n",
       "         0.1707]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(exp/sum_exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chubby-luther",
   "metadata": {},
   "source": [
    "So now we are saying there is a $.20$ probability of our image being the number 6.  Not exactly the dominant prediction we were looking for, but it's a start."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greek-hampshire",
   "metadata": {},
   "source": [
    "### Wrapping up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strange-reverse",
   "metadata": {},
   "source": [
    "So the operation that we learned above of applying the exponent to each of our outputs, and then dividing by the sum of those exponents is called the softmax function.  And as we saw, it just looks like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "strange-psychiatry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0681, 0.0885, 0.0969, 0.0710, 0.1153, 0.0703, 0.2033, 0.0647, 0.0512,\n",
       "         0.1707]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(prediction)/torch.sum(torch.exp(prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portable-access",
   "metadata": {},
   "source": [
    "Or, mathematically, it's the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developing-parade",
   "metadata": {},
   "source": [
    "$softmax(x) = \\frac{e^x}{\\sum e^x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yellow-alert",
   "metadata": {},
   "source": [
    "And we can tack it onto the end of our neural network, so that we get an output of probabilities, where one number tends to be dominant with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "brutal-unknown",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=784, out_features=64, bias=True)\n",
       "  (1): Sigmoid()\n",
       "  (2): Linear(in_features=64, out_features=10, bias=True)\n",
       "  (3): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(5)\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(784, 64),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(64, 10),\n",
    "    nn.Softmax(dim = 1)\n",
    ")\n",
    "\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "absent-coordinate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0681, 0.0885, 0.0969, 0.0710, 0.1153, 0.0703, 0.2033, 0.0647, 0.0512,\n",
       "         0.1707]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(x.float())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pediatric-amount",
   "metadata": {},
   "source": [
    "And if we want to represent this mathematically, this was our entire neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varying-fifty",
   "metadata": {},
   "source": [
    "$z_{1x64} = x_{1x784} \\cdot W_{784x64} + b_{1x64}$\n",
    "\n",
    "$a_{1x64} = \\sigma(z_{1x64})$\n",
    "\n",
    "$z_{1x10} = A_{1x64} \\cdot W_{64x10} + b_{1x10}$\n",
    "\n",
    "$a_{1x10} = \\frac{e^z}{\\sum e^z}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southeast-reviewer",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "listed-carpet",
   "metadata": {},
   "source": [
    "In this lesson, we wrapped up the prediction function for a neural network.  In doing so, we saw that we want our neural network to predict a different probability for each potential output.  So in predicting which of 10 digits to output, we'd want something like the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "white-israel",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = [.05, .62, .03, .03, .05, .03, .15, .01, .02, .01]\n",
    "output\n",
    "# 0    1    2   3    4     5   6   7     8   9  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intimate-verse",
   "metadata": {},
   "source": [
    "To produce an output like the above, we first need our final linear layer to have a separate neuron for each output.  Then our next goal is to turn this output into a set of probabilities where one number tends to dominate.  To get one number to dominate, we apply the exponent to each entry, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "express-grocery",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = torch.tensor([[-0.0716,  0.1905,  0.2812, -0.0302,  0.4545, -0.0408,  1.0216, -0.1237,\n",
    "         -0.3579,  0.8472]])\n",
    "\n",
    "exp_output = torch.exp(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classified-delay",
   "metadata": {},
   "source": [
    "Applying the exponent, exaggerates our larger numbers, increasing them by a lot -- while leaving the smaller numbers significantly less affected.  Finally, we need to turn our outputs into probabilities.  To do this, each entry must be between 0 and 1, and the entire set of outputs should add up to one.  It turns out we can achieve both goals, just by turning the above set of outputs into percentages, by dividing by the total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "early-threshold",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0681, 0.0885, 0.0969, 0.0710, 0.1153, 0.0703, 0.2033, 0.0647, 0.0512,\n",
       "         0.1707]])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_output/torch.sum(exp_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rotary-memphis",
   "metadata": {},
   "source": [
    "We saw that this procedure is called the softmax, and represented mathematically as: $softmax(x) = \\frac{e^x}{\\sum e^x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foreign-space",
   "metadata": {},
   "source": [
    "And, we represented all of the layers of our neural network with the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brave-tiffany",
   "metadata": {},
   "source": [
    "$z_{1x64} = x_{1x784} \\cdot W_{784x64} + b_{1x64}$\n",
    "\n",
    "$a_{1x64} = \\sigma(Z_{1x64})$\n",
    "\n",
    "$z_{1x10} = A_{1x64} \\cdot W_{64x10} + b_{1x10}$\n",
    "\n",
    "$a_{1x10} = \\frac{e^z}{\\sum e^z}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subsequent-douglas",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"https://www.jigsawlabs.io/free\" style=\"position: center\"><img src=\"./jigsaw-icon.png\" width=\"15%\" style=\"text-align: center\"></a>\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
