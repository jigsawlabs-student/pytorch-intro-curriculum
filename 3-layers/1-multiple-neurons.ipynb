{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so in previous lessons, we saw that we can write the linear function for a single neuron with the dot product.  Specifically, we rewrote our linear function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$z(x) = w_1x_1 + w_2x_2 + b$ as: \n",
    "\n",
    "$z(x) =  \\begin{bmatrix}\n",
    "w_1 & w_2 \\\\\n",
    "\\end{bmatrix}\\cdot \\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \n",
    "\\end{bmatrix} + b = (w \\cdot x + b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And remember that if this we pass through features of an observation, like features of a cancer cell, and the linear function returns a positive number, then our neuron fires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "# cell_area 2, cell_concavity 1\n",
    "x = torch.tensor([2, 1])\n",
    "             \n",
    "w = torch.tensor([3, 4])\n",
    "\n",
    "b = -8\n",
    "\n",
    "w.dot(x) + b\n",
    "# z = 3*2 + 4*1 - 8 = 2 -> neuron fires"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we take another look at our diagram of a neural network, notice that don't just have a single neuron making a prediction but rather have many neurons making many different assessments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./artificial-network.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And even in a single layer, there are multiple neurons.  In this lesson, we'll discuss what it means too have *multiple neurons* in a single layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So why have multiple neurons in a layer?  Well we can think of each neuron being is in charge of making a different assessment about an observation. For example, in our example of a neural network that determines if cells are cancerous, we may have one neuron that determines if a cell is overly large, and another neuron that determines if a cell has an abnormal shape.  This sort of separation of concerns is a big part of where a layer of a neural network gets its power.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we'll need to create neurons that operate side by side.  And this time, our neurons will not ultimately predict whether or not a cell is cancerous, but rather perform a sort of division of labor to assess different components of each observation.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's translate this to Python, starting with an observation of a cancerous cell, represented in the following feature vector $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# area, perimeter, number concave points, symmetry error  \n",
    "x = torch.tensor([4, 2, 3, 2]).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the comment above, the first two elements describe the size of the cell (area and perimeter), and the second two elements (3 and 2) describe the cell's shape (number of concavities and symmetry). \n",
    "\n",
    "And now let's say that we have one neuron that focuses on determining the cell's size, and looks something like the following:    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# area, perimeter, number concave points, symmetry error  \n",
    "w_size = torch.tensor([2, 3, -.5, 0])\n",
    "\n",
    "b_size = -11.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice a couple things from the vector `w_size` above.  First, we can see that even the neuron is only in charge of determining a cell's size, it still observes **all** of the features from our observation.  But we can also see that it has weights with larger absolute values with the first two elements (2 and 3), that correspond to a cell's size.  \n",
    "\n",
    "This means that if the cell's size changes (like the perimeter or area) it will have a large impact on the neuron firing, where a change to the shape would have less impact, as the associated weights are only $-.5$ and $0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.5000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.dot(w_size) + b_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is our first neuron, and it is sensitive to changes in a neuron's size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A second neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's say we have a second neuron, with this neuron in charge of whether the cell has an irregular shape (as opposed size with our neuron above).  Our observation cell will still be the same, and so has the same four features with the same values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# area, perimeter, number concave points, symmetry error  \n",
    "x = torch.tensor([4., 2., 3., 2.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this time, our weight vector for this new neuron is different.  Take a look at `w_shape` neuron below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# area, perimeter, number concave points, symmetry error  \n",
    "w_shape = torch.tensor([0, -.5, 3, 1.5])\n",
    "\n",
    "b_shape = -14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So with this neuron, the last two weights have a larger absolute value, making the neuron more sensitive to shape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The neuron also has it's own bias for assessing shape, which is $-14$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-3.)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.dot(w_shape) + b_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this second neuron predicts whether or not the cell is misshapen, and here outputs a negative number, assessing that the cell is not misshaped.  In total we have two different neurons.  They each receive all of the features of an observation, but they focus on different characteristics of the cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thinking with multiple neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so let's make sure we understand what happened above. \n",
    "\n",
    "#### 1. Different weights same input\n",
    "\n",
    "The first point, is that we use the **same feature vector** $x$ as an input to each neuron.  The two neurons simply weigh these inputs differently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$z_{size}(x) = \\begin{bmatrix}\n",
    "x_1 & x_2 & x_3 & x_4 \\\\\n",
    "\\end{bmatrix}\\cdot \\begin{bmatrix}\n",
    "2 \\\\\n",
    "3 \\\\\n",
    "-.5 \\\\\n",
    "0 \\\\\n",
    "\\end{bmatrix} - 12 = .5$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$z_{shape}(x) = \\begin{bmatrix}\n",
    "x_1 & x_2 & x_3 & x_4 \\\\\n",
    "\\end{bmatrix}\\cdot \\begin{bmatrix}\n",
    "0 \\\\\n",
    "-.5 \\\\\n",
    "3 \\\\\n",
    "1.5 \\\\\n",
    "\\end{bmatrix} - 14 = 1.5$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> So above, that feature vector $\\begin{bmatrix}\n",
    "x_1 & x_2 & x_3 & x_4 \\\\\n",
    "\\end{bmatrix}$ represents a single cell with one set of values, and that feature vector is going to two different neurons, for one neuron to make a determination about size, and the other to make a determination about shape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The diagram below makes the same point.  The diagram represents the first layer of a neural network, and notice: the attributes of an observation $x_1$ through $x_4$, are fed to each of the (this time four, not two) neurons in the layer (each neuron is a white circle).  That's why each feature has a line drawn to each neuron.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./first_layer_2.svg\" width=\"30%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So just to recap, every neuron in a layer receives the same inputs.  And each neuron just has separate values for weights and biases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making it brief"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we understand that each neuron receives the same inputs, let's see if we can condense our representation of our neurons.  To do this, let's start by removing the biases of our two neurons.  This gives us:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$g_{size}(x) = \\begin{bmatrix}\n",
    "2 & 4 & 3 & 1 \\\\\n",
    "\\end{bmatrix}\\cdot \\begin{bmatrix}\n",
    "2 \\\\\n",
    "3 \\\\\n",
    "-.5 \\\\\n",
    "0 \\\\\n",
    "\\end{bmatrix} = 11$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$g_{shape}(x) = \\begin{bmatrix}\n",
    "2 & 4 & 3 & 1 \\\\\n",
    "\\end{bmatrix}\\cdot \\begin{bmatrix}\n",
    "0 \\\\\n",
    "-.5 \\\\\n",
    "3 \\\\\n",
    "1.5 \\\\\n",
    "\\end{bmatrix}= 8.5$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, observe that if we combine these two weight vectors into a *matrix*, then multiplying our feature vector $x$ by our weight matrix gives us the following: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{bmatrix}\n",
    "2 & 4 & 3 & 1 \\end{bmatrix} \\cdot\n",
    "\\begin{bmatrix}\n",
    "1 & 0\\\\\n",
    "3 & -.5 \\\\\n",
    "-.5 & 3 \\\\\n",
    "0 & 1.5\\end{bmatrix} = \\begin{bmatrix}\n",
    "11 & 8.5 \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or more generally: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{bmatrix}\n",
    "- & x &  -  \n",
    "\\end{bmatrix} \\cdot \\begin{bmatrix}\n",
    "|  & |  \\\\\n",
    "w_{1}  & w_{2} \\\\\n",
    "|   & |\n",
    "\\end{bmatrix}  = \\begin{bmatrix}\n",
    "x \\cdot w_{1} & x \\cdot w_{2} \\end{bmatrix}  = \\begin{bmatrix} g_{1}(x) & g_{2}(x) \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's show this in code.  We'll start by placing our feature vectors in a weight matrix $W$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  1.0000],\n",
       "        [-0.5000,  3.0000],\n",
       "        [ 3.0000, -0.5000],\n",
       "        [ 1.5000,  0.0000]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_size = torch.tensor([0, -.5, 3, 1.5])\n",
    "w_shape = torch.tensor([1, 3, -.5, 0])\n",
    "\n",
    "# weight matrix\n",
    "W = torch.stack((w_size, w_shape), dim = 0).T\n",
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we'll multiply the attributes of our observation $x$ by the weights of these neurons with the $@$ symbol like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([11.0000,  8.5000])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = x @ W\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have just seen that we can use matrix multiplication to calculate the weighted sum of multiple neurons.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **What's a matrix?** We've used the term matrix a couple of times now.  A matrix is a rectangular array of numbers arranged in rows and columns. \n",
    "\n",
    "If we call `shape` on our matrix, we'll see the number of rows followed by the number of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A vector, by contrast, consists of only one row or column, and it's size can be expressed with a single number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So $x$ is a vector of length 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now above we've seen how we can calculate the weighted sum.\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "- & x &  -  \n",
    "\\end{bmatrix} \\cdot \\begin{bmatrix}\n",
    "|  & |  \\\\\n",
    "w_{1}  & w_{2} \\\\\n",
    "|   & |\n",
    "\\end{bmatrix}  = \\begin{bmatrix}\n",
    "x \\cdot w_{1} & x \\cdot w_{2} \\end{bmatrix}  = \\begin{bmatrix} g_{1}(x) & g_{2}(x) \\end{bmatrix}$\n",
    "\n",
    "\n",
    "But we have still have not included the biases of our linear layer.  To complete our linear layer, we'll add a bias of $-12$ for $z_{size}$ and our bias of $-14$ for $z_{shape}$.  We do so by placing the two biases into a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.tensor([-12, -7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> So we have a bias term for each neuron, and collect them into a bias vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then can calculate the output of passing an observation through a linear layer with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.0000,  1.5000])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x @ W + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or summarizing the above, we can calculate the outputs of both linear functions with:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{bmatrix}\n",
    "- & x &  -  \n",
    "\\end{bmatrix} \\cdot \\begin{bmatrix}\n",
    "|  & |  \\\\\n",
    "w_1  & w_2 \\\\\n",
    "|   & |\n",
    "\\end{bmatrix} + \\begin{bmatrix} b_1 & b_2 \\end{bmatrix} = \\begin{bmatrix}\n",
    "x \\cdot w_1 & x \\cdot w_2 \\end{bmatrix} + \\begin{bmatrix} b_1 & b_2 \\end{bmatrix} = \\begin{bmatrix} z_1(x) & z_2(x) \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which we can summarize as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$z = x\\cdot W  + b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $z$ is a vector consisting of the output of each neuron's linear function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5, 1.5])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.dot(W) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Your turn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now consider that we have two different neurons that detect size and shape.  They do so with the following weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  1.0000],\n",
       "        [-0.5000,  3.0000],\n",
       "        [ 3.0000, -0.5000],\n",
       "        [ 1.5000,  0.0000]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([2, 4, 3, 1])\n",
    "\n",
    "w_size = torch.tensor([2, 0, 1, 2])\n",
    "w_shape = torch.tensor([1, 1, 2, 0])\n",
    "\n",
    "W_new = torch.stack((w_size, w_shape), dim = 0).T\n",
    "W_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to calculate the output of $x \\cdot W$, where $x$ is the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your answer here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check your answer with the dot product below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Including the sigmoid activation layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So above, we saw the output from the linear layer by performing matrix vector multiplication like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.0000,  1.5000])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "b = torch.tensor([-12, -7])\n",
    "\n",
    "w_size = torch.tensor([0, -.5, 3, 1.5])\n",
    "w_shape = torch.tensor([1, 3, -.5, 0])\n",
    "\n",
    "# weight matrix\n",
    "W = torch.stack((w_size, w_shape), dim = 0).T\n",
    "W\n",
    "\n",
    "z = x @ W + b\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then, next we would pass these outputs to the sigmoid activation function like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2689, 0.8176])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sigmoid(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or mathematically, we write this entire hypothesis function as the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\sigma(x \\cdot W + b) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where sigma is applied to each entry of the vector resulting from $W\\cdot x + b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\sigma (x \\cdot W + b) = \\begin{bmatrix} \\sigma(z_1) & \\sigma(z_2) \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or expressing the above formula as two layers of a neural network, our linear layer and an activation layer.  We can express this as the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$z = (x \\cdot W + b)$\n",
    "\n",
    "$a = \\sigma(z)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $z$ is our linear layer and $a$ is our activation layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson we saw the components to build a layer of a neural network.  A single layer is a combination of a weighted input and a sigmoid activation function.  \n",
    "\n",
    "The weighted input can be represented by $x \\cdot W + b$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{bmatrix}\n",
    "- & x &  -  \n",
    "\\end{bmatrix} \\cdot \\begin{bmatrix}\n",
    "|  & |  \\\\\n",
    "w_1  & w_2 \\\\\n",
    "|   & |\n",
    "\\end{bmatrix} + \\begin{bmatrix} b_1 & b_2 \\end{bmatrix} = \\begin{bmatrix}\n",
    "x \\cdot w_1 & x \\cdot w_2 \\end{bmatrix} + \\begin{bmatrix} b_1 & b_2 \\end{bmatrix} = \\begin{bmatrix} l_1(x) & l_2(x) \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The row vector $x$ represents the features of a single observation.\n",
    "* Each column of the matrix W, contains the weights of a separate neuron, with the entries of $b$ as the corresponding biases.\n",
    "\n",
    "The output of the weighted input is fed into the activation function, which applies an entrywise operation.  Here, we use the sigmoid function.  So we can summarize the operations of our entire layer as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\sigma (W\\cdot x + b) = \\begin{bmatrix} \\sigma(z_1) \\\\ \\sigma(z_2) \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can break up the above as a linear layer $z$ and an activation layer $a$ where:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$z = (x \\cdot W + b)$\n",
    "\n",
    "$a = \\sigma(z)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson we saw the components to build a layer of a neural network.  A single layer is a combination of a weighted input and a sigmoid activation function.  \n",
    "\n",
    "The weighted input can be represented by $x \\cdot W + b$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{bmatrix}\n",
    "- & x &  -  \n",
    "\\end{bmatrix} \\cdot \\begin{bmatrix}\n",
    "|  & |  \\\\\n",
    "w_1  & w_2 \\\\\n",
    "|   & |\n",
    "\\end{bmatrix} + \\begin{bmatrix} b_1 & b_2 \\end{bmatrix} = \\begin{bmatrix}\n",
    "x \\cdot w_1 & x \\cdot w_2 \\end{bmatrix} + \\begin{bmatrix} b_1 & b_2 \\end{bmatrix} = \\begin{bmatrix} z_1(x) & z_2(x) \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The row vector $x$ represents the features of a single observation.\n",
    "* Each column of the matrix W, contains the weights of a separate neuron, with the entries of $b$ as the corresponding biases.\n",
    "\n",
    "The output of the weighted input is fed into the activation function, which applies an entrywise operation.  Here, we use the sigmoid function.  So we can summarize the operations of our entire layer as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\sigma (W\\cdot x + b) = \\begin{bmatrix} \\sigma(z_1) \\\\ \\sigma(z_2) \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can break up the above as a linear layer $z$ and an activation layer $a$ where:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$z = (x \\cdot W + b)$\n",
    "\n",
    "$a = \\sigma(z)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"https://www.jigsawlabs.io/free\" style=\"position: center\"><img src=\"./jigsaw-icon.png\" width=\"15%\" style=\"text-align: center\"></a>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9, 12])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x @ W_new"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
