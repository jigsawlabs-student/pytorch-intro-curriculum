{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sacred-married",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "complex-canyon",
   "metadata": {},
   "source": [
    "### Our image data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contrary-given",
   "metadata": {},
   "source": [
    "So the image data that we'll work with, is a set of different handwritten digits -- the numbers 0 through 9.  And the easiest way to get these digits is to use a library called keras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latter-thirty",
   "metadata": {},
   "source": [
    "> Don't worry, we're still learning pytorch.  We'll just use keras for downloading our data, as it's a bit easier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "english-scroll",
   "metadata": {},
   "source": [
    "Let's see how easy it is to load some data with `keras`.  Simply press `shift + return` on the line of code below, and keras will download a set of images and their corresponding target values for us."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aging-cincinnati",
   "metadata": {},
   "source": [
    "> Note: If trying to install keras on your laptop, run the following from the terminal:  `sudo pip3 install --upgrade --ignore-installed tensorflow`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "recreational-premises",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "looking-saying",
   "metadata": {},
   "source": [
    "So we just downloaded our handwritten digits dataset.  And we assigned it's components to `X_train` `y_train`, `X_test`, `y_test`.  We'll learn about those different components later `X_test` and `y_test`, later on in this lesson, but for now let's focus on what we assigned to the `X_train` variable.  \n",
    "\n",
    "The `X_train` variable points to list of observations, where each observation is a photo of a different digit.  Let's see that.  We'll select the first image from our list with `X_train[0]`, and then plot it with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adolescent-interface",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x16152cee0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focused-process",
   "metadata": {},
   "source": [
    "> And if you want to see the second image we can simply swap out the number `0` inside of the brackets and replace it with the number `1`, and then press `shift + return` again. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elementary-consequence",
   "metadata": {},
   "source": [
    "Ok, so we saw that our first item is a picture of the digit 5, and that, overall, `X_train` simply contains a list of different digits.  We'll want our neural network to be able to identify each of these images.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scenic-submission",
   "metadata": {},
   "source": [
    "### How neural networks use our data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mounted-ranch",
   "metadata": {},
   "source": [
    "So how will our neural network do that?  Well first we should understand that each image is really just a grid of numbers -- one number for each pixel in the picture.  Let's take a look."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "burning-transcription",
   "metadata": {},
   "source": [
    "> Uncomment the line below (by removing the `#` symbol) and pressing `shift + return`, then hide all of those numbers by adding back in the `#` in front of our code, and pressing `shift + return`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "several-slave",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stainless-horizon",
   "metadata": {},
   "source": [
    "The zeros essentially represent the black space around the edge of the frame, and then we have different grayscale numbers for the pixels representing the drawing.\n",
    "\n",
    "> This is how all black and white pictures are represented digitally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "golden-detection",
   "metadata": {},
   "source": [
    "So we can see that when we say `X_train` represents a list of pictures, and that each observation is really a grid of numbers where each number represents the grayscale value of the corresponding pixel.  So this is what we'll feed into our neural network -- a bunch pixels at different positions.  In other words, *each pixel in a picture is a different* **feature**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adopted-stage",
   "metadata": {},
   "source": [
    "And our target is simply a number that corresponds to that target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "mature-softball",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4], dtype=uint8)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considerable-reynolds",
   "metadata": {},
   "source": [
    "### Working with our Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "potential-atlanta",
   "metadata": {},
   "source": [
    "One thing that may be confusing is how to work our data -- especially now that it is a grid of numbers.  So there are a couple of things to make sure we get straight.  The first is that, if were going to feed our data into a Pytorch neural net, that data needs to be a tensor. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "given-radio",
   "metadata": {},
   "source": [
    "To convert the data into a tensor, we simply wrap our data in the tensor function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "finite-cooperation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "X_tensor = torch.tensor(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "blessed-village",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "commercial-address",
   "metadata": {},
   "source": [
    "Now beyond converting the data to a Pytorch tensor, we'll also need to make sure the data is of the proper type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fuzzy-london",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.uint8"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tensor.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multiple-rainbow",
   "metadata": {},
   "source": [
    "Currently, it is not.  Our data is type integer, but we need it to be of type float.  But it's easy enough to convert our data with the `.float()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ranking-nudist",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tensor = X_tensor.float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optical-manitoba",
   "metadata": {},
   "source": [
    "And now if we look at our data, we'll see it's of the correct type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "voluntary-method",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tensor.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inner-freeware",
   "metadata": {},
   "source": [
    "> A float just means that our data can have decimals, where an integer only allows for whole numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flush-safety",
   "metadata": {},
   "source": [
    "Alright now let's work with our `y_train` data, converting it into a tensor, and making it of type float all at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "composite-still",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tensor = torch.tensor(y_train).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "atlantic-porcelain",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tensor.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "right-pressure",
   "metadata": {},
   "source": [
    "### Data Dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "right-integrity",
   "metadata": {},
   "source": [
    "Now our data not only needs to be of type tensor, and of type float, but we also need the dimensions to be correct.  Now we can see the dimensions of our tensors with the `shape` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "understanding-silly",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thrown-inspection",
   "metadata": {},
   "source": [
    "So the above is saying that our `X_tensor` has 60,000 elements, where each element is a 28x28 grid of numbers like we saw above.  It turns out that to feed our data into a neural net, we cannot pass through a *grid* of pixels into our neural net, but rather will need to unspool each image into one long list.  \n",
    "\n",
    "To see how we can do this let's just focus in on one image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "artistic-writer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 28])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_image = X_tensor[0]\n",
    "\n",
    "first_image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sized-willow",
   "metadata": {},
   "source": [
    "So this image is a $28x28$ grid of pixels and the total number of pixels is $28x28 = 784$.  And if we want to convert this into a list of length 784, we can do so with the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "amended-reunion",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped_img = first_image.view(784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "common-tablet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([784])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshaped_img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "false-record",
   "metadata": {},
   "source": [
    "> So now our reshaped image is just one long list of 784 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "better-intranet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshaped_img[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plastic-works",
   "metadata": {},
   "source": [
    "So with the view function, we can specify the new dimensions we want for that image -- here, a list of length 784."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medical-status",
   "metadata": {},
   "source": [
    "Note that we can also go back into two dimensions.  And when we do, we specify the rows before the columns.  So if we want to convert our list to a tensor with 382 rows, with each row having two columns -- we use `view(382, 2)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "center-russell",
   "metadata": {},
   "outputs": [],
   "source": [
    "again_reshaped = reshaped_img.view(392, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "backed-continent",
   "metadata": {},
   "source": [
    "> So we can see that each row has two columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "associate-halloween",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "again_reshaped[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "french-mixer",
   "metadata": {},
   "source": [
    "### Pytorch Does the math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affiliated-playing",
   "metadata": {},
   "source": [
    "One thing to note is that Pytorch only allows us to convert into dimensions where the math adds up.  So, for example, a 28x28 tensor has 784 total elements.  If we try to convert this tensor to be of length 783, then Pytorch will throw an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "under-reserve",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 28])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "classical-breast",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[783]' is invalid for input of size 784",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-d6fc1a64737f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfirst_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m783\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[783]' is invalid for input of size 784"
     ]
    }
   ],
   "source": [
    "first_image.view(783)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blind-spring",
   "metadata": {},
   "source": [
    "So above, if were converting a 28x28 tensor to a single dimension, there's only one length that will work and it's 784.  It turns out that we can leave this number 784 unspecified and Pytorch will do the math for us.  Let's see this.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sublime-stanley",
   "metadata": {},
   "source": [
    "> We start with our original shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "royal-burden",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 28])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joint-picking",
   "metadata": {},
   "source": [
    "And then pass through a `-1` -- where the single number indicates we want to convert to one long list, and that we'll let Pytorch calculate how long the list should be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "blond-somerset",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([784])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshaped = first_image.view(-1)\n",
    "\n",
    "reshaped.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agricultural-sheffield",
   "metadata": {},
   "source": [
    "Note that Pytorch can also do this with two dimensional data.  So if we start with our tensor of one long list of 784, and then want to convert it to two rows, Pytorch can calculate the number of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "isolated-fields",
   "metadata": {},
   "outputs": [],
   "source": [
    "two_rows = reshaped.view(2, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "embedded-funeral",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 392])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_rows.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mobile-thanksgiving",
   "metadata": {},
   "source": [
    "### Putting it together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conceptual-darkness",
   "metadata": {},
   "source": [
    "Ok, so why are we learning this again?  Well because when we downto feed our image data into a neural network, we'll need to reshape it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "local-failing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serious-water",
   "metadata": {},
   "source": [
    "Above, we have a tensor that consists of 60,000 images, each a 28x28 grid.  And we we want to convert this to 60,000 elements, where each element is a list.  We do so with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "present-scholar",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reshaped = X_tensor.view(60000, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "modified-british",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 784])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_reshaped.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neither-consent",
   "metadata": {},
   "source": [
    "Or we can even replace the last number with a `-1`, and Pytorch will know how to convert our tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "pleasant-behalf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 784])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_reshaped = X_tensor.view(60000, -1)\n",
    "\n",
    "X_reshaped.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitting-inclusion",
   "metadata": {},
   "source": [
    "And now that each picture is represented as one long list, we'll be able to feed our images to our neural network, which we'll do in the next lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "requested-effect",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legal-voice",
   "metadata": {},
   "source": [
    "In this lesson, we learned how to work and reformat multidimensional data in Pytorch.  We saw that we can convert our data into a Pytorch tensor with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "smaller-sport",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "X_tensor = torch.tensor(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beautiful-better",
   "metadata": {},
   "source": [
    "And that we can change the datatype to be a float with the `float()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "synthetic-concentration",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tensor = X_tensor.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "mathematical-latino",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tensor.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blind-thesaurus",
   "metadata": {},
   "source": [
    "From there, we can change the dimensions of our data.  Our data starts off where each image is represented as a grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "informational-blues",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "buried-pepper",
   "metadata": {},
   "source": [
    "And we can reshape the data into a tensor of 60,000 elements, where each element is a long list with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "continued-bidding",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reshaped = X_tensor.view(60000, 784)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quick-alberta",
   "metadata": {},
   "source": [
    "And if we want, we can also replace one of the numbers with a -1, and Pytorch will calculate the size of the missing dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "arranged-cannon",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reshaped = X_tensor.view(60000, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "moral-stopping",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 784])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_reshaped.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instructional-emission",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "[Colab Deep Learning](https://colab.research.google.com/github/margaretmz/deep-learning/blob/master/fashion_mnist_keras.ipynb#scrollTo=d44TznbgZZgm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
