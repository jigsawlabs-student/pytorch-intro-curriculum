{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear functions with Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last lesson, we saw that our neural network is composed of various neurons.  And that these neurons take certain inputs like the size of a cell (cell area), and the number of bumps on a particular cell (cell concavities) and use these inputs to predict whether or not the cell is cancerous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./neuron_cancer.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call each example of a particular cell, an **observation**, which consists of both **features** (the characteristics of a cell), and the **target** that our neuron is trying to predict (here, is the cell cancerous).  In machine learning, we pass the features of our observation through a hypothesis function which outputs a prediction.  We saw that one component of a neuron's hypothesis function is the **linear function**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We represented the **linear function** as $z(x)$, like so:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$z(x) = w_1x_1 + w_2x_2 + bias$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Above, $w_1$ and $w_2$ are our two weights, which each multiply by the corresponding features $x_1$ and $x_2$, and the bias acts as a counterweight to the rest of the function.  When our **linear function** outputs a positive number, then our neuron fires.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_cancerous(cell_area, cell_concavities):\n",
    "    total = 2*cell_area + 1*cell_concavities - 4\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_cancerous(2, 3) # neuron takes in cell_area and cell_concavities features, and neuron fires"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reworking our Linear Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we saw that our linear function is respresented by the function:\n",
    "\n",
    "$z(x) = w_1x_1 + w_2x_2 + bias$\n",
    "\n",
    "and that the bias acts as a counterweight to the rest of the function.  Now *the rest of the function* $w_1x_1 + w_2x_2$ is referred to as our **weighted sum**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <img src=\"./full-weighted.png\" width=\"20%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, let's just focus on the weighted sum (that is, every term except for the bias), which we'll represent as $g(x)$.\n",
    "\n",
    "$g(x) = w_1x_1 + w_2x_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know in the weighted sum, each feature of an observation is multiplied by a corresponding weight to produce an output.  For example, let's return to our neuron that predicts whether or not cancer is present in a cell.  The linear function is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $z(x) = 2*cell\\_area + 1*cell\\_concavities - 4$\n",
    "\n",
    "And the weighted sum is:\n",
    "\n",
    "> $g(x) = 2*cell\\_area + 1*cell\\_concavities$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another view of our weighted sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so this is our weighted sum:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $g(x) = 2*cell\\_area + 1*cell\\_concavities$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's represent these weights above as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight cell_area 2, weight for cell_concavities 1\n",
    "w = [2, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can also represent the features for our first observation, which let's say has a cell area of $3$ and cell concavities of $4$, as another list, which we'll call $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell area is 3, and cell concavities is 4\n",
    "x = [3, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in math terminology, we would call these two lists **vectors**, $w$, and $x$.  And the *dot product* of these two vectors, is pairing the elements of these vectors together by their index, multiplying the pairs, and then adding.  It looks like this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{bmatrix}\n",
    "2 & 1 \n",
    "\\end{bmatrix} \\cdot  \\begin{bmatrix}\n",
    "3 \\\\ 4\n",
    "\\end{bmatrix} = 2*3 + 1*4 = 10$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The point is that if we have a vector of weights $w$ and features $x$, then we can calculate the weighted sum with the dot product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$g(x) = \\begin{bmatrix}\n",
    "w_1 & w_2 \\\\\n",
    "\\end{bmatrix}\\cdot \\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \n",
    "\\end{bmatrix} = w_1*x_1 + w_2*x_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> So with the dot product, we just pair up the entries of our vectors by their index, multiply each pair, and add.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the dot product above matches the formula for the weighted sum we saw earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's confirm this by converting these lists to Pytorch's equivalent of lists, called tensors, and then using the dot function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.tensor([2, 1])\n",
    "w = torch.tensor([3, 4])\n",
    "x.dot(w)\n",
    "# 2*3 + 1*4 = 6 + 4 = 10 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We can refer to the vector $x$ above as the **feature vector** of our observation.  This seems logical, as the vector contains an observation's features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now dot products are pretty essential to how neural networks function, so let's see it one more time before moving on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> First, we'll define our observation's features, `x_new` and the feature vector `w_new`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x_new = torch.tensor([4, 1]) # represents a new cell with cell area of 4 and concavities of 1\n",
    "w_new = torch.tensor([5, 4]) # and a new set of weights of w_1 = 5 and w_2 = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the **weighted sum** that we'll get through the dot product?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check your work with Pytorch's `dot` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check your work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating our Linear Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now, with our knowledge of the dot product, let's go back to our linear function and rewrite it.  Remember that our linear function currently looks like the following. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $z(x) = w_1x_1 + w_2x_2  + b $\n",
    "\n",
    "But with the dot product, we can write this as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$z(x) = w \\cdot x + b = \\begin{bmatrix}\n",
    "w_1 & w_2 \\\\\n",
    "\\end{bmatrix}\\cdot \\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \n",
    "\\end{bmatrix} + b = w_1*x_1 + w_2*x_2 + b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Where $b$ is our bias term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more thing to note is that while so far we have stuck to two features, we can really have as many features as we want.  So if we have $n$ features, then we would write our linear function like so:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$z(x) = w_1x_1 + w_2x_2 + ... w_nx_n + b $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Where $n$ is the number of features, and we have a weight $w_n$, associated with each one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we of course can represent our features and weight vectors, just by extending their size.  For example below, we have an observation with four features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([2, 1, 4, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we have a corresponding weight for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.tensor([3, 1, 1, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And can have a bias of $b = -20$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = -20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the dot product would simply pair the weight with the corresponding feature, multiply them and add."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$z(x) = w \\cdot x + b = \\begin{bmatrix}\n",
    "2 & 1 & 4 & 3\n",
    "\\end{bmatrix} \\cdot  \\begin{bmatrix}\n",
    "3 \\\\ 1 \\\\ 1 \\\\ 4\n",
    "\\end{bmatrix} - 20 = 2*3 + 1*1 + 4*1 + 3*4 -20 = 3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.dot(x) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we learned how to simplify our linear function a neuron with the use of the dot product.  As we saw our linear function consists of both a weighted sum, and a bias that acts as a counterweight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <img src=\"./full-weighted.png\" width=\"20%\">\n",
    "\n",
    "And when the output from our linear function is positive, then our neuron fires.  We then focused on rewriting that weighted sum and saw that we can do so with the dot product.\n",
    "\n",
    "With the dot product, we represent the weights as one vector, and the features as a second vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.tensor([2, 1])\n",
    "w = torch.tensor([3, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the dot products, means pairing the terms together, then multiplying, and adding each product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.dot(w)\n",
    "# 2*3 + 1*4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{bmatrix}\n",
    "2 & 3\n",
    "\\end{bmatrix} \\cdot  \\begin{bmatrix}\n",
    "1 \\\\ 4\n",
    "\\end{bmatrix} = 2*3 + 1*4  = 14$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$g(x) = \\begin{bmatrix}\n",
    "w_1 & w_2 \\\\\n",
    "\\end{bmatrix}\\cdot \\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \n",
    "\\end{bmatrix} = w_1*x_1 + w_2*x_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"https://www.jigsawlabs.io/free\" style=\"position: center\"><img src=\"./jigsaw-icon.png\" width=\"15%\" style=\"text-align: center\"></a>\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
