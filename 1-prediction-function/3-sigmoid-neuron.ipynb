{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Likelihoods with Sigmoid Neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have learned about the linear function of neuron, which is part of our neuron's hypothesis function.  We saw that when our linear functin returns a positive number our neuron fires, and with a negative number it does not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"neuron-general-2.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we'll learn about the other part the hypothesis function, the sigmoid function.  And we'll see how instead of an all or nothing response, we can use the activation function to change *how strongly* a neuron fires, based on how positive or negative our linear function's output is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding an Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we focused on the linear component of a neuron.  Let's see it again.  \n",
    "\n",
    "We represent a single observation -- like a potentially cancerous cell -- with a feature vector like so. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# cell area is 3, and cell concavities is 4\n",
    "x = torch.tensor([3, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we represent the neuron by a weight vector and the bias, like so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_function(x):\n",
    "    w = torch.tensor([2, 1])\n",
    "    b = -4\n",
    "    return w.dot(x) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = linear_function(x)\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./neuron_cancer.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now remember that with what we learned so far, our neuron will fire, or not fire, based on whether or not the linear component returns a positive number.  And we can represent this firing or not firing in Python with a simple `if else` statement like so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_function(z):\n",
    "    if z > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation_function(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that our neuron really has two layers to it -- the linear function which can output any positive or negative number.  And our activation function which outputs either a 1 or 0, to represent the neuron firing or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = linear_function(x)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation_function(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *activation function* is what direcetly determines thee output of a neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From all or nothing to probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our code is currently looking quite good, but there is one thing that we would like to change.  The activation function currently returns an all or nothing response.  But if we think about how we might use this neuron in practice, like to predict whether a cancer is cancerous, it is generally preferred to express a degree of confidence in the prediction.  \n",
    "\n",
    "> For example, we might want a prediction of a 95% chance of cancer, or 3% chance of cancer. \n",
    "\n",
    "So this time, if confident in a prediction of 1, we'll have the activation function return a number *close* to 1 (like .98).  And if confident in a prediction of 0, the neuron would return a number close to 0, like .02.  If we are completely uncertain in our prediction, our activation function would output a $.5$. \n",
    "\n",
    "> At this point, I like to think of a neuron lighting up like a lightbulb.  So previously when our activation function could just output a 1 or a 0, this was like an on off switch.  Here, we are changing this to a dimmer, with the brighter the light the stronger our prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch already a function that will take our positive or negative output from the linear function, and turn it into a percentage, where the lower the number the closer to 0, and the higher the number the closer to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9241)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_num = torch.tensor(2.5)\n",
    "torch.sigmoid(positive_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0759)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_num = torch.tensor(-2.5)\n",
    "torch.sigmoid(negative_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> So above we use the sigmoid function to translate our positive and negative numbers to an output between 0 and 1, where the more negative the input the closer the output is to 0.  And the more positive the input, the closer the output is to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So putting together our linear function with our sigmoid function above, we have the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_function(x):\n",
    "    w = torch.tensor([2, 1])\n",
    "    b = -4.\n",
    "    return w.dot(x) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And an updated activation function that uses the sigmoid, to return a number between 0 or 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_activation_function(z):\n",
    "    return torch.sigmoid(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = linear_function(x)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9975)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid_activation_function(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two layers together consist of our neuron's hypothesis function.  The linear layer weighs the different features together with the bias, to output either a positive or negative number, and the activation function translates this to a value between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the Sigmoid Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So above we saw that we can use the sigmoid function to have our activation function return a number between 0 and 1, instead of an all or nothing response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9526)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num = torch.tensor(3.)\n",
    "torch.sigmoid(num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's take a deeper look at the sigmoid function.  The sigmoid function is a pretty popular function within mathematics, and looks like the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\sigma(x) = \\frac{1}{1 + e^{-x}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the sigmoid function is represented by the Greek letter, $\\sigma$ (sigma), and it accomplishes what we want.  That is, it brings large positive numbers close to 1, and large negative numbers close to 0.  Now let's see why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When $z$ is a large positive number, say $1,000$, we have:\n",
    "\n",
    "* $\\sigma(1000) = \\frac{1}{1 + e^{-1000}} = \\frac{1}{1 + 1/e^{1000}}  = \\frac{1}{1 + small\\_num} \\approx 1$\n",
    "\n",
    "And when $z$ is a large negative number, we have: \n",
    "\n",
    "* $\\sigma(-1000) = \\frac{1}{1 + e^{1000}} = \\frac{1}{1 + e^{1000}}  = \\frac{1}{1 + big\\_num} \\approx 0$\n",
    "\n",
    "Finally, when $z = 0$ we have: \n",
    "\n",
    "* $\\sigma(0) = \\frac{1}{1 + e^{0}} = \\frac{1}{1 + 1} = \\frac{1}{2}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to experiment with this in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def sigmoid(value):\n",
    "    return 1/(1 + torch.exp(-value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5000)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_num = torch.tensor(-7.)\n",
    "pos_num = torch.tensor(7.)\n",
    "num_zero = torch.tensor(0.)\n",
    "\n",
    "sigmoid(neg_num)\n",
    "# 0.000911\n",
    "\n",
    "sigmoid(pos_num)\n",
    "# 0.9990\n",
    "\n",
    "sigmoid(num_zero)\n",
    "# 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our sigmoid function takes values between positive and negative infinity and maps those values to numbers between 0 and 1.  We've now learned all the components of a neuron's hypothesis function.  Let's review before moving on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So with our linear function and sigmoid activation function, we have just built the hypothesis function of neuron.  Mathematically, our hypothesis function looks like the following."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$z(x) = w_1x_1 + w_2x_2 + ... w_nx_n + b  = w \\cdot x + b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we pass this output to our activation function -- of the sigmoid function.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\sigma(z) = \\frac{1}{1 + e^{-z}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or in code, we represent our hypothesis function as the following: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_function(x):\n",
    "    w = torch.tensor([2, 1])\n",
    "    b = -4.\n",
    "    return w.dot(x) + b\n",
    "\n",
    "def sigmoid_activation_function(z):\n",
    "    return torch.sigmoid(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And make a prediction like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7311)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([2, 1])\n",
    "z = linear_function(x)\n",
    "\n",
    "sigmoid_activation_function(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we write it mathematically as: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $z(x) = w \\cdot x + b $\n",
    "* $\\sigma(z) =  \\frac{1}{1 + e^{-z(x)}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So above, even though we are describing a single neuron, we can think of the linear function $z(x)$ and the activation function as two different layers of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now made it to the final form of our neuron's hypothesis function.  Our artificial neuron takes in weighted inputs and now returns a value between 1 and 0.  A value of .5 means that the artificial neuron is not making a prediction one way or the other. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the hypothesis function in two steps: \n",
    "\n",
    "1. A linear component, which we represent with `z` \n",
    "2. Passing the output of that linear component to our activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for an sigmoid neuron that takes in two inputs, we calculate the output with the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$z = w_1x_1 + w_2x_2 + b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\sigma(z) = \\frac{1}{1 + e^{-z}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"https://www.jigsawlabs.io/free\" style=\"position: center\"><img src=\"jigsaw-icon.png\" width=\"15%\" style=\"text-align: center\"></a>\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
