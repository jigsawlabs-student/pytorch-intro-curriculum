{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "unknown-diabetes",
   "metadata": {},
   "source": [
    "# Pytorch Prediction Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varying-january",
   "metadata": {},
   "source": [
    "### Reviewing our Hypothesis Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blank-breathing",
   "metadata": {},
   "source": [
    "So far, we have learned how to the components of the prediction function of a single neuron in a neural network.  We saw that we represented our neuron as taking in inputs, and based on those inputs, the corresponding weights, and a bias terms -- firing or not firing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corrected-selling",
   "metadata": {},
   "source": [
    "<img src=\"neuron-general-2.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protective-extent",
   "metadata": {},
   "source": [
    "We saw that our neuron really consists of two components, the linear layer which can return any positive or negative or number, and our activation function which translates that output to a number between 1 and 0 (to represent firing or not).\n",
    "\n",
    "Mathematically, we represent our neuron's linear function and activation function as the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governing-product",
   "metadata": {},
   "source": [
    "$z(x) = w_1x_1 + w_2x_2 + b$\n",
    "\n",
    "$ \\sigma(z) = \\frac{1}{1 + e^{-z}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interior-mineral",
   "metadata": {},
   "source": [
    "And let's take a moment to review seeing this in code.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incoming-receipt",
   "metadata": {},
   "source": [
    "Remember that we can represent an observation as a vector like so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "pending-election",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# cell area is 3, and cell concavities is 4\n",
    "x = torch.tensor([2., 4.])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polyphonic-series",
   "metadata": {},
   "source": [
    "And the linear layer of our neuron can be represented as a weight vector and bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aquatic-township",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight cell_area 2, weight for cell_concavities 1\n",
    "w = torch.tensor([2, 1])\n",
    "b = torch.tensor(-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opened-default",
   "metadata": {},
   "source": [
    "So to get the outfrom the linear layer, we use the dot product, plus the bias: $z(x) = w \\cdot x + b $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "wound-blade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = w.dot(x) + b\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heard-hardwood",
   "metadata": {},
   "source": [
    "And then to translate this into a value between $1$ and $0$, we use our sigmoid activation function, $ \\sigma(z) = \\frac{1}{1 + e^{-z}} $, with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "lucky-stream",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_activation(z):\n",
    "    return 1/(1 + torch.exp(-z.float()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stylish-baltimore",
   "metadata": {},
   "source": [
    "> We need to convert our integers into floats, and do so with `z.float()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "colored-reader",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1192)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid_activation(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "together-implementation",
   "metadata": {},
   "source": [
    "### Making it Real"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "computational-macedonia",
   "metadata": {},
   "source": [
    "Ok, so above we reviewed our prediction function for our neuron.  Now below, let's we'll re-implement that same prediction function function -- the linear layer and the sigmoid layer, but this time we'll use all of the tools that a Pytorch professional would use.  To build a neural network in Pytorch with a single neuron, we generally would do so with the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dirty-center",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=2, out_features=1, bias=True)\n",
       "  (1): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(2, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "initial-bundle",
   "metadata": {},
   "source": [
    "And to have the neural network make a prediction, we just pass through a feature vector like we did above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "judicial-butler",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell area is 3, and cell concavities is 4\n",
    "x = torch.tensor([2., 4.])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impaired-failure",
   "metadata": {},
   "source": [
    "So $x$ represents the features of a single observation.  And we can see our neural network's predictions with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "scheduled-primary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9407], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authentic-plenty",
   "metadata": {},
   "source": [
    "> So our linear layer ouputs a positive or negative number, which our sigmoid activation translates to a number between 1 and 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressing-envelope",
   "metadata": {},
   "source": [
    "So, we'll break down the code above in a moment, but notice it largely consists of what we saw above: `nn.Linear` represents our linear layer and `nn.Sigmoid` represents our sigmoid activation function, and the `nn.Sequential` simply creates a neural network that passes the output from the linear layer to the sigmoid function, just like we saw above.  \n",
    "\n",
    "$z(x) = w \\cdot x + b$\n",
    "\n",
    "$ \\sigma(z) = \\frac{1}{1 + e^{-z}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acting-kansas",
   "metadata": {},
   "source": [
    "### Understanding the Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prospective-definition",
   "metadata": {},
   "source": [
    "Ok, so we just saw how we can create a neural network in Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "immune-bridges",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=2, out_features=1, bias=True)\n",
       "  (1): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(2, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mineral-country",
   "metadata": {},
   "source": [
    "Now let's understand these components a bit better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weird-civilization",
   "metadata": {},
   "source": [
    "The first, and main, component to understand is our `Linear` function -- for linear layer.  This remember, should represent the linear layer of a single neuron.  Notice that we pass through a (2, 1) to `Linear`.  This means that we want the linear function to take in two features -- and the `1` means also that we want it to consist of a single neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "noted-handling",
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = nn.Linear(2, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extreme-quilt",
   "metadata": {},
   "source": [
    "We can see what this looks like under the hood by calling the `_parameters` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "tested-moscow",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              Parameter containing:\n",
       "              tensor([[-0.5048, -0.6080]], requires_grad=True)),\n",
       "             ('bias',\n",
       "              Parameter containing:\n",
       "              tensor([-0.3516], requires_grad=True))])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll._parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frequent-stack",
   "metadata": {},
   "source": [
    "So just like the linear function we defined above, here we have a tensor, our weight vector, and a bias term.  Why is the weight vector of length 2?  Because when we created our linear layer, we said there would be 2 input features -- and we need a weight for each feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "packed-chart",
   "metadata": {},
   "source": [
    "$z(x) = w_1x_1 + w_2x_2 + b$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hindu-freight",
   "metadata": {},
   "source": [
    "So we understand the first number in `nn.Linear(2, 1)`, it creates a vector of length 2 for two features.  But what does it mean to create more than one neuron?  Well as we'll see later on, each neuron gets it's own feature vector and it's own bias.  So let's create 3 neurons of length 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dressed-treatment",
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_3 = nn.Linear(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "periodic-election",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              Parameter containing:\n",
       "              tensor([[ 0.4237,  0.4814],\n",
       "                      [ 0.5267, -0.1435],\n",
       "                      [ 0.0232,  0.5558]], requires_grad=True)),\n",
       "             ('bias',\n",
       "              Parameter containing:\n",
       "              tensor([-0.3506, -0.5314, -0.2793], requires_grad=True))])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll_3._parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disabled-hampton",
   "metadata": {},
   "source": [
    "Now we'll talk about working with multiple neurons later on.  For now let's just make sure we understand our layer with a single neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "taken-burke",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              Parameter containing:\n",
       "              tensor([[-0.0883, -0.0319]], requires_grad=True)),\n",
       "             ('bias',\n",
       "              Parameter containing:\n",
       "              tensor([0.4128], requires_grad=True))])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll = nn.Linear(2, 1)\n",
    "ll._parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faced-europe",
   "metadata": {},
   "source": [
    "It simply creates our weight vector and bias.  And if we pass through a feature vector, it will apply our linear function of $z(x) = w \\cdot x + b$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "sunset-payday",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 4.])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "instrumental-variation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1087], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twelve-editor",
   "metadata": {},
   "source": [
    "There we go."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loved-russia",
   "metadata": {},
   "source": [
    "Finally, the sigmoid function takes the output from our linear function and passes it through our sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "demanding-pakistan",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid = nn.Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "improving-consumption",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5272], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = ll(x)\n",
    "sigmoid(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quarterly-andorra",
   "metadata": {},
   "source": [
    "And then our `nn.Sequential` function packages up our two functions, and passes the output from one layer into the next layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "referenced-martial",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=2, out_features=1, bias=True)\n",
       "  (1): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(2, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "durable-retention",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chief-harmony",
   "metadata": {},
   "source": [
    "In this lesson, we saw how to create a neural network -- with a single neuron -- in Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "through-parish",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Linear(2, 1),\n",
    "    nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disturbed-filename",
   "metadata": {},
   "source": [
    "We saw that with the linear layer, we specify the number of input features, and the number of neurons -- where each neuron consists of a weight vector and a bias term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "median-airfare",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              Parameter containing:\n",
       "              tensor([[-0.2977,  0.1892]], requires_grad=True)),\n",
       "             ('bias',\n",
       "              Parameter containing:\n",
       "              tensor([-0.3397], requires_grad=True))])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = nn.Linear(2, 1)\n",
    "layer._parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joined-starter",
   "metadata": {},
   "source": [
    "And we saw that we can pass a feature vector to this layer, and it will apply the linear function $z(x) = w \\cdot x + b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "sweet-accountability",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 4.])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "extra-perception",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1087], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governing-associate",
   "metadata": {},
   "source": [
    "And finally that if we pass the feature vector to the neural net, that it will pass the feature vector through the linear layer, and that output to the sigmoid activation function to produce a prediction between 0 and 1 expressing the strength of the neuron firing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "differential-analysis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2172], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
