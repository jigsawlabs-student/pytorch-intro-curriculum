{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "compatible-caribbean",
   "metadata": {},
   "source": [
    "# Pytorch Prediction Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleased-bidder",
   "metadata": {},
   "source": [
    "### Reviewing our Hypothesis Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flying-superior",
   "metadata": {},
   "source": [
    "So far, we have learned about a neuron's hypothesis function.  Let's start by reviewing what we learned so far.\n",
    "\n",
    "We saw that we represented our neuron as taking in inputs, and based on those inputs, combined with the weights and bias term -- the neuron fired or not. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "global-chart",
   "metadata": {},
   "source": [
    "<img src=\"neuron-general-2.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norwegian-senior",
   "metadata": {},
   "source": [
    "We saw that this hypothesis function really consists of two components, the linear layer which can return any positive or negative or number, and our activation function which translates that output to a number between 1 and 0 (to represent firing or not).\n",
    "\n",
    "> Mathematically, we represented the neuron's linear function and activation function as the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cheap-drunk",
   "metadata": {},
   "source": [
    "$z(x) = w_1x_1 + w_2x_2 + b = w \\cdot x + b $\n",
    "\n",
    "$ \\sigma(z) = \\frac{1}{1 + e^{-z}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elegant-quest",
   "metadata": {},
   "source": [
    "And we can represent this as code like so:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "typical-dimension",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_layer(x):\n",
    "    return w.dot(x) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "economic-paste",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_activation(z):\n",
    "    return 1/(1 + torch.exp(-z.float()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geographic-asbestos",
   "metadata": {},
   "source": [
    "Then we can define some initial weights and a bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "processed-reach",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight cell_area 2, weight for cell_concavities 1\n",
    "w = torch.tensor([2., 1.])\n",
    "b = torch.tensor(-10.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "durable-spending",
   "metadata": {},
   "source": [
    "And pass through a vector $x$, which represents the efeeatures of an observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "organic-venue",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# cell area is 3, and cell concavities is 4\n",
    "x = torch.tensor([2., 4.])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "derived-accordance",
   "metadata": {},
   "source": [
    "So to get the out from the linear layer, we pass our data through the linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "global-separation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2.)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = w.dot(x) + b\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classified-bidding",
   "metadata": {},
   "source": [
    "And then pass that output to the activation layer, which returns a value between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "round-permission",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1192)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid_activation(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moved-second",
   "metadata": {},
   "source": [
    "### Making it Real"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parental-seeker",
   "metadata": {},
   "source": [
    "Ok, so above we reviewed our prediction function for our neuron.  Now below, let's re-implement that same prediction function function -- the linear layer and the sigmoid layer.  But this time we'll use all of the tools that a Pytorch professional would use.  \n",
    "\n",
    "Here's how we do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "celtic-prince",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=2, out_features=1, bias=True)\n",
       "  (1): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(2, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varied-merchandise",
   "metadata": {},
   "source": [
    "> So above, we created a neural network in Pytorch.  It's just a pretty simple neural network as it only consists of a single neuron that takes in two features.  We'll eventually, build a more complex neural network, but for now this is fine.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informative-chuck",
   "metadata": {},
   "source": [
    "To have the neural network above make a prediction, we just pass through a feature vector like we did above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cooked-wheat",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell area is 3, and cell concavities is 4\n",
    "x = torch.tensor([2., 4.])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "devoted-bullet",
   "metadata": {},
   "source": [
    "So $x$ represents the features of a single observation.  And we can see our neural network's predictions with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "macro-permit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0642], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respective-nelson",
   "metadata": {},
   "source": [
    "> So just like before, the linear layer ouputs a positive or negative number, which our sigmoid activation translates to a number between 1 and 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boxed-lawyer",
   "metadata": {},
   "source": [
    "We'll break down the code above in a moment, but for now notice it largely consists of what we saw above: `nn.Linear` represents our linear layer and `nn.Sigmoid` represents our sigmoid activation function, and the `nn.Sequential` simply creates a neural network that passes the output from the linear layer to the sigmoid function, just like we saw above.  \n",
    "\n",
    "$z(x) = w \\cdot x + b$\n",
    "\n",
    "$ \\sigma(z) = \\frac{1}{1 + e^{-z}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "known-exercise",
   "metadata": {},
   "source": [
    "### Understanding the Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amino-toyota",
   "metadata": {},
   "source": [
    "Ok, so we just saw how we can create a neural network in Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "typical-switch",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=2, out_features=1, bias=True)\n",
       "  (1): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(2, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threatened-packing",
   "metadata": {},
   "source": [
    "Now let's understand these components a bit better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "according-latex",
   "metadata": {},
   "source": [
    "The first, and main, component to understand is our `Linear` function -- which creates a linear layer.  This represents the linear layer of a single neuron.  \n",
    "\n",
    "In the linear layer above, we passed the argument `(2, 1)`.  This tells the linear to create a vector with two weights -- and the `1` specifies that we want to create a single neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "small-ethics",
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = nn.Linear(2, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faced-sport",
   "metadata": {},
   "source": [
    "We can see what our linear layer looks like under the hood by calling the `_parameters` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "successful-border",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              Parameter containing:\n",
       "              tensor([[-0.2562,  0.5871]], requires_grad=True)),\n",
       "             ('bias',\n",
       "              Parameter containing:\n",
       "              tensor([-0.5983], requires_grad=True))])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll._parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "built-swing",
   "metadata": {},
   "source": [
    "So just like the linear function we defined above, here we have a tensor, our weight vector, and a bias term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "broken-kinase",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.2562,  0.5871]], requires_grad=True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "limited-premium",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.5983], requires_grad=True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "double-creator",
   "metadata": {},
   "source": [
    "Why is the weight vector of length 2?  Because when we created our linear layer, we specified a `2` -- which created a weight vector of length 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attractive-disclosure",
   "metadata": {},
   "source": [
    "$z(x) = w_1x_1 + w_2x_2 + b$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raised-dover",
   "metadata": {},
   "source": [
    "### Creating Random Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "political-latex",
   "metadata": {},
   "source": [
    "Now where did those numbers come from?  Well they were randomly created.  So this means that each time we create the linear layer, new parameters would be created.  You can check this by repeatedly pressing `shift + return` on the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "marked-calculator",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.6047,  0.2253]], requires_grad=True)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll = nn.Linear(2, 1)\n",
    "ll.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cloudy-yukon",
   "metadata": {},
   "source": [
    "> Notice the weights change."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisite-bullet",
   "metadata": {},
   "source": [
    "Now this is largely ok, because we'll soon see that we can teach our neural network to learn the correct weights.  But, sometimes for teaching purpooses, it's easier if everyone is working with the same weights.  So to remove this randomness, we'll sometimes specify a `manual_seed` just before creating the neural network or layer.  As long as we pass the same number into that `manual_seed` this will ensure the same weights show up each time.  Let's see this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "negative-imperial",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.4670, -0.5288]], requires_grad=True)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(5)\n",
    "ll = nn.Linear(2, 1)\n",
    "\n",
    "ll.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "funded-salad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.4670, -0.5288]], requires_grad=True)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(5)\n",
    "ll = nn.Linear(2, 1)\n",
    "\n",
    "ll.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "female-belarus",
   "metadata": {},
   "source": [
    "See this time the we ensured the same initial weights show up each time. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dried-brooklyn",
   "metadata": {},
   "source": [
    "### Creating multiple neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experienced-berlin",
   "metadata": {},
   "source": [
    "So we just saw the different components to creating a single neuron of length 2.  We did so with `nn.Linear(2, 1)`.  But eventually, we'll want to create a neural network with more than one neuron.  So what does it even mean to create a neural network with more than one neuron?  Well as we'll see later, each neuron we create will get it's own feature vector and bias term.  So let's create a linear layer with 3 neurons, each of length 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "helpful-jacksonville",
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_3 = nn.Linear(2, 3) # 3 neurons of length 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "hearing-omega",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              Parameter containing:\n",
       "              tensor([[ 0.6720, -0.5793],\n",
       "                      [ 0.0386,  0.2537],\n",
       "                      [-0.3339, -0.1547]], requires_grad=True)),\n",
       "             ('bias',\n",
       "              Parameter containing:\n",
       "              tensor([-0.4722, -0.3343, -0.6446], requires_grad=True))])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll_3._parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weighted-pursuit",
   "metadata": {},
   "source": [
    "So it looks like we now have three weight vectors each of length 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "nasty-lyric",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.6047,  0.2253],\n",
       "        [ 0.3041,  0.1122],\n",
       "        [ 0.6801,  0.2124]], requires_grad=True)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll_3.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extensive-warrant",
   "metadata": {},
   "source": [
    "And three bias terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "orange-chinese",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.6270,  0.5941,  0.2401], requires_grad=True)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll_3.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "correct-transport",
   "metadata": {},
   "source": [
    "Now we'll talk about working with multiple neurons later on.  But hopefully the above allows us to understand what the `nn.Linear(2, 1)` -- it created a single vector of length 2 representing our single neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "solid-official",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              Parameter containing:\n",
       "              tensor([[-0.3373, -0.6495]], requires_grad=True)),\n",
       "             ('bias',\n",
       "              Parameter containing:\n",
       "              tensor([0.4031], requires_grad=True))])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll = nn.Linear(2, 1)\n",
    "ll._parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approximate-scott",
   "metadata": {},
   "source": [
    "In other words, the code above simply created our weight vector and our bias.  And if we pass through a feature vector, it will apply our linear function of $z(x) = w \\cdot x + b$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "exempt-joshua",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 4.])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "prescription-california",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.8697], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sized-forum",
   "metadata": {},
   "source": [
    "There we go."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classical-grain",
   "metadata": {},
   "source": [
    "Finally, the sigmoid function takes the output from our linear function and passes it through our sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "labeled-springer",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid = nn.Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "agreed-thought",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0537], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = ll(x)\n",
    "sigmoid(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solid-montreal",
   "metadata": {},
   "source": [
    "And then our `nn.Sequential` function packages up our two functions, and passes the output from one layer into the next layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cardiac-notice",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=2, out_features=1, bias=True)\n",
       "  (1): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(2, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dense-radiation",
   "metadata": {},
   "source": [
    "So that's what it looks like to create a neural network with one neuron that takes in an observation with two features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hispanic-shoulder",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structural-thesaurus",
   "metadata": {},
   "source": [
    "In this lesson, we saw how to create a neural network -- with a single neuron -- in Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "vertical-calgary",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Linear(2, 1),\n",
    "    nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embedded-secretary",
   "metadata": {},
   "source": [
    "We saw that with the linear layer, we specify the number of input features, and the number of neurons -- where each neuron consists of a weight vector and a bias term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ultimate-double",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              Parameter containing:\n",
       "              tensor([[-0.2977,  0.1892]], requires_grad=True)),\n",
       "             ('bias',\n",
       "              Parameter containing:\n",
       "              tensor([-0.3397], requires_grad=True))])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = nn.Linear(2, 1)\n",
    "layer._parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "going-communist",
   "metadata": {},
   "source": [
    "And we saw that we can pass a feature vector to this layer, and it will apply the linear function $z(x) = w \\cdot x + b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "differential-inside",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 4.])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "athletic-polymer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1087], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ignored-italic",
   "metadata": {},
   "source": [
    "And finally that if we pass the feature vector to the neural net, that it will pass the feature vector through the linear layer, and that output to the sigmoid activation function to produce a prediction between 0 and 1 expressing the strength of the neuron firing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cross-inquiry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2172], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
