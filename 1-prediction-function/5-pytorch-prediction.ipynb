{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alive-emission",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "initial-rally",
   "metadata": {},
   "source": [
    "### Why this matters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convinced-peeing",
   "metadata": {},
   "source": [
    "In a the neural network, we generally don't work with a single neuron individual, but rather work with a *layer* of multiple neurons.  In fact, we already saw a layer of neurons when we used Pytorch to predict our images.  You may have forgotten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "sudden-cuisine",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.W1 = nn.Linear(28*28, 64) # all of these are linear layers\n",
    "        self.W2 = nn.Linear(64, 64)\n",
    "        self.W3 = nn.Linear(64, 64)\n",
    "        self.W4 = nn.Linear(64, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "linear-diversity",
   "metadata": {},
   "source": [
    "Each of the `nn.Linear` is Pytorch's way of creating multiple neurons side by side in a linear layer.  The first argument specifies the number of features of all of the neurons in the layer, and the second argument specifies the number of neurons to create.  \n",
    "\n",
    "For example, if we want to create a layer that has only one neuron, with the neuron taking in four different features, we do so with the following. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "seven-practitioner",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = nn.Linear(4, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mature-shakespeare",
   "metadata": {},
   "source": [
    "Under the hood, this neuron looks just like the neuron we built when learning about the dot product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "absolute-steps",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'weight': Parameter containing:\n",
       " tensor([[-0.2594, -0.3593,  0.2731,  0.4234, -0.1417]], requires_grad=True),\n",
       " 'bias': Parameter containing:\n",
       " tensor([-0.2888], requires_grad=True)}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(W._parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dental-major",
   "metadata": {},
   "source": [
    "See that, this time there's a a tensor with four different weights, and then at the end there's a bias term of a single number.  Just like we saw before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simple-coordinate",
   "metadata": {},
   "source": [
    "And if we create two neurons, well this means there will be two neurons and two bias terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "promising-celebrity",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = nn.Linear(4, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "raised-aspect",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              Parameter containing:\n",
       "              tensor([[-0.1189,  0.1297, -0.0730,  0.3075],\n",
       "                      [-0.4594,  0.1429,  0.3236, -0.0862]], requires_grad=True)),\n",
       "             ('bias',\n",
       "              Parameter containing:\n",
       "              tensor([-0.1429, -0.1153], requires_grad=True))])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W._parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polar-greek",
   "metadata": {},
   "source": [
    "## Including the activation layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chief-defensive",
   "metadata": {},
   "source": [
    "And we can calculate hypothesis made by each neuron in a layer with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "amber-patrol",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.62245933, 0.81757448])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(x.dot(W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atlantic-truth",
   "metadata": {},
   "source": [
    "Or mathematically, we can write our layer as the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "angry-research",
   "metadata": {},
   "source": [
    "$ \\sigma(x \\cdot W + b) $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "given-edinburgh",
   "metadata": {},
   "source": [
    "Where sigma is applied to each entry of the vector resulting from $W\\cdot x + b$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "static-annual",
   "metadata": {},
   "source": [
    "$\\sigma (x \\cdot W + b) = \\begin{bmatrix} \\sigma(z_1) & \\sigma(z_2) \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dutch-secondary",
   "metadata": {},
   "source": [
    "Or expressing the above formula as two layers of a neural network, our linear layer and an activation layer.  We can express this as the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quantitative-zealand",
   "metadata": {},
   "source": [
    "$z = (x \\cdot W + b)$\n",
    "\n",
    "$a = \\sigma(z)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ahead-ground",
   "metadata": {},
   "source": [
    "Where $z$ is our linear layer and $a$ is our activation layer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
